-- MySQL dump 10.13  Distrib 5.7.12, for osx10.9 (x86_64)
--
-- Host: 127.0.0.1    Database: slr_publish
-- ------------------------------------------------------
-- Server version	5.6.20

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `approach`
--

DROP TABLE IF EXISTS `approach`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `approach` (
  `idapproach` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(90) DEFAULT NULL,
  PRIMARY KEY (`idapproach`)
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `approach`
--

LOCK TABLES `approach` WRITE;
/*!40000 ALTER TABLE `approach` DISABLE KEYS */;
INSERT INTO `approach` VALUES (1,'genetic based'),(2,'modification based'),(3,'coverage based'),(4,'history based'),(5,'fault based'),(6,'similarity based'),(7,'requirements based'),(8,'oracle based'),(9,'fault diagnosis based'),(10,'search based'),(11,'program structure based'),(12,'model based');
/*!40000 ALTER TABLE `approach` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `artifact`
--

DROP TABLE IF EXISTS `artifact`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `artifact` (
  `idartifact` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(45) DEFAULT NULL,
  `source` varchar(99) DEFAULT NULL,
  `link` varchar(99) DEFAULT NULL,
  PRIMARY KEY (`idartifact`)
) ENGINE=InnoDB AUTO_INCREMENT=58 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `artifact`
--

LOCK TABLES `artifact` WRITE;
/*!40000 ALTER TABLE `artifact` DISABLE KEYS */;
INSERT INTO `artifact` VALUES (1,'First Run','not clear','not clear'),(2,'XML-Security',NULL,'http://xml.apache.org/security'),(3,'JMock',NULL,NULL),(4,'EasyAccept',NULL,'http://easyaccept.sourceforge.net/'),(5,'26 java transformations',NULL,NULL),(6,'JMeter',NULL,'http://jmeter.apache.org/'),(7,'Siena',NULL,'http://www.inf.usi.ch/carzaniga/siena'),(8,'NanoXML',NULL,'http://nanoxml.sourceforge.net/'),(9,'Ant',NULL,'http://ant.apache.org'),(10,'Launch Interceptor Program (LIP)',NULL,NULL),(11,'Jasmine-maven',NULL,NULL),(12,'Java-apns',NULL,NULL),(13,'Jopt-simple',NULL,NULL),(14,'La4j',NULL,NULL),(15,'Scribe-java',NULL,NULL),(16,'Vraptor-core',NULL,NULL),(17,'Assertj-core',NULL,NULL),(18,'Metrics-core',NULL,NULL),(19,'print_tokens',NULL,NULL),(20,'print_tokens2',NULL,NULL),(21,'replace',NULL,NULL),(22,'schedule',NULL,NULL),(23,'schedule2',NULL,NULL),(24,'tcas',NULL,NULL),(25,'tot_info',NULL,NULL),(26,'space',NULL,NULL),(27,'jtopas',NULL,NULL),(28,'Gzip',NULL,NULL),(29,'grep',NULL,NULL),(30,'time&money',NULL,NULL),(31,'jgrapht',NULL,NULL),(32,'JDepend',NULL,NULL),(33,'Checkstyle',NULL,NULL),(34,'sed','SIR',NULL),(35,'University Students Monitoring System',NULL,NULL),(36,'Hospital Management System',NULL,NULL),(37,'Industrial Process Operation System',NULL,NULL),(38,'Automata Lift Controller',NULL,NULL),(39,'Traffic signal controlling',NULL,NULL),(40,'Stock index prediction',NULL,NULL),(41,'AveCalc',NULL,NULL),(42,'LaTazza',NULL,NULL),(43,'make','UNIX',NULL),(44,'Commons-CLI',NULL,NULL),(45,'Commons-Collections',NULL,NULL),(46,'Joda-Time',NULL,NULL),(47,'Bank',NULL,NULL),(48,'Identifier',NULL,NULL),(49,'Money',NULL,NULL),(50,'flex',NULL,NULL),(51,'QTB',NULL,NULL),(52,'Gradebook',NULL,NULL),(53,'Galileo',NULL,NULL),(54,'Altitude Switch',NULL,NULL),(55,'Wheel Brake System',NULL,NULL),(56,'Flight Guidance System',NULL,NULL),(57,'NoiseGen','Not real name. Industrial sofware designed for DSL technology manufacturers to test the performance',NULL);
/*!40000 ALTER TABLE `artifact` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `artifact_metric`
--

DROP TABLE IF EXISTS `artifact_metric`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `artifact_metric` (
  `idartifact_metric` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`idartifact_metric`)
) ENGINE=InnoDB AUTO_INCREMENT=19 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `artifact_metric`
--

LOCK TABLES `artifact_metric` WRITE;
/*!40000 ALTER TABLE `artifact_metric` DISABLE KEYS */;
INSERT INTO `artifact_metric` VALUES (1,'LOC'),(2,'Test Cases'),(3,'Classes'),(4,'Methods'),(5,'Coverage'),(6,'Fault'),(7,'Test classes'),(8,'Modules'),(9,'User requirements'),(10,'Test methods'),(11,'Branches'),(12,'Functions'),(13,'Blocks'),(14,'Decisions'),(15,'Simulink Nodes'),(16,'Conditions'),(17,'states'),(18,'transitions');
/*!40000 ALTER TABLE `artifact_metric` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `author`
--

DROP TABLE IF EXISTS `author`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `author` (
  `idauthor` int(11) NOT NULL AUTO_INCREMENT,
  `first_name` varchar(200) DEFAULT NULL,
  `last_name` varchar(200) DEFAULT NULL,
  PRIMARY KEY (`idauthor`)
) ENGINE=InnoDB AUTO_INCREMENT=219 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `author`
--

LOCK TABLES `author` WRITE;
/*!40000 ALTER TABLE `author` DISABLE KEYS */;
INSERT INTO `author` VALUES (1,'Angelin','Gladston'),(2,'Nehemiah','Khanna'),(3,'Palanisamy','Narayanasamy'),(4,'Kanna','Arputharaj'),(5,'Everton L. G.','Alves'),(6,'Patr√≠cia D. L.','Machado'),(7,'Tiago','Massoni'),(8,'Miryung','Kim'),(9,'Xiaobing','Sun'),(10,'Xin','Peng'),(11,'Hareton','Leung'),(12,'Bin','Li'),(13,'Geetanjali','Chaurasia'),(14,'Sonali','Agarwal'),(15,'Swarnima','Singh Gautam'),(16,'Ramzi','A. Haraty'),(17,'Nashat','Mansour'),(18,'Lama','Moukahal'),(19,'Iman','Khalil'),(20,'Yafeng','Lu'),(21,'Yiling','Lou'),(22,'Shiyang','Cheng'),(23,'Lingming','Zhang'),(24,'Dan','Hao'),(25,'Yangfan','Zhou'),(26,'Lu','Zhang'),(27,'Lei','Zang'),(28,'Yanbo','Wang'),(29,'Xingxia','Wu'),(30,'Tao','Xie'),(31,'Kun','Wu'),(32,'Chunrong','Fang'),(33,'Zhenyu','Chen'),(34,'Zhihong','Zhao'),(35,'Dongjiang','You'),(36,'Baowen','Xu'),(37,'Bin','Luo'),(38,'Chen','Zhang'),(39,'Yu-Chi','Huang'),(40,'Chin-Yu','Huang'),(41,'Jun-Ru','Chang'),(42,'Tsan-Yuan','Chen'),(43,'','Vedpal'),(44,'Naresh','Chauhan'),(45,'Harish','Kumar'),(46,'Mohapatra','Sudhir Kumar'),(47,'Prasad','Srinivas'),(48,'R. Uma','Maheswari'),(49,'D. Jeya','Mala'),(50,'N','Prakash'),(51,'T. R.','Rangaswamy'),(52,'Prem','Parashar'),(53,'Arvind','Kalia'),(54,'Rajesh','Bhatia'),(55,'Mahfuzul','Islam'),(56,'Alessandro','Marchetto'),(57,'Angelo','Susi'),(58,'Giuseppe','Scanniello'),(59,'Cristian','Simons'),(60,'Emerson Cabrera','Paraiso'),(61,'Xiaolin','Wang'),(62,'Hongwei','Zeng'),(63,'Rongcun','Wang'),(64,'Shujuan','Jiang'),(65,'Deng','Chen'),(66,'Patipat','Konsaard'),(67,'Lachana','Ramingwong'),(68,'Xiaobin','Zhao'),(69,'Zan','Wang'),(70,'Xiangyu','Fan'),(71,'Zhenhua','Wang'),(72,'Dongdong','Gao'),(73,'Xiangying','Guo'),(74,'Lei','Zhao'),(75,'Jung-Hyun','Kwon'),(76,'In-Young','Ko'),(77,'Gregg','Rothermel'),(78,'Matt','Staats'),(79,'Amitabh','Srivastava'),(80,'Jay','Thiagarajan'),(81,'Matthew J.','Rummel'),(82,'Gregory M.','Kapfhammer'),(83,'Andrew','Thall'),(84,'Y.','Fazlalizadeh'),(85,'A.','Khalilian'),(86,'M. Abdollahi','Azgomi'),(87,'S.','Parsa'),(88,'Samuel T. C.','Santos'),(89,'Soumen','Nayak'),(90,'Chiranjeev','Kumar'),(91,'Sachin','Tripathi'),(92,'Ji','Zhou'),(93,'Hong','Mei'),(94,'Thillaikarasi','Muthusamy'),(95,'K.','Seetharaman'),(96,'Arnaldo Marulitua','Sinaga'),(97,'Neha','Sharma'),(98,NULL,'Sujata'),(99,'G. N.','Purohit'),(100,'Yiting','Wang'),(101,'Xiaomin','Zhao'),(102,'Xiaoming','Ding'),(103,'Shweta','Singhal'),(104,'Shivangi','Gupta'),(105,'Bharti','Suri'),(106,'Supriya','Panda'),(107,'Kamna','Solanki'),(108,'Yudh Vir','Singh'),(109,'Sandeep','Dalal'),(110,'Roland H.','Untch'),(111,'Chengyun','Chu'),(112,'Mary Jean','Harrold'),(113,'Sebastian','Elbaum'),(114,'Alexey G.','Malishevsky'),(115,'Hyunsook','Do'),(116,'Alex','Kinneer'),(117,'Kristen R.','Walcott'),(118,'Mary Lou','Soffa'),(119,'Robert S.','Roos'),(120,'Siavash','Mirabab'),(121,'Ladan','Tahvildari'),(122,'Zheng','Li'),(123,'Mark','Harman'),(124,'Robert M.','Hierons'),(125,'Bo','Jiang'),(126,'Zhenyu','Zhang'),(127,'W. K.','Chan'),(128,'T. H.','Tse'),(129,'Pablo','Loyola'),(130,'Alberto','Gonzalez-Sanchez'),(131,'Rui','Abreu'),(132,'Hans-Gerhard','Gross'),(133,'Arjan J. C. van','Gemund'),(134,'Xu','Zhao'),(135,'Daniel Di','Nardo'),(136,'Nadia','Alshahwan'),(137,'Lionel','Briand'),(138,'Yvan','Labiche'),(139,'Michael G.','Epitropakis'),(140,'Shin','Yoo'),(141,'Edmund K.','Burke'),(142,'Samaila','Musa'),(143,'Abu Bakar Md','Sultan'),(144,'Abdul Azim Bin Abd','Ghani'),(145,'Salmi','Baharom'),(146,'Qi','Luo'),(147,'Kevin','Moran'),(148,'Denys','Poshyvanyk'),(149,'Sejun','Kim'),(150,'Jongmoon','Baik'),(151,'Tanzeem','Bin Noor'),(152,'Hadi','Hemmati'),(153,'Dario Di','Nucci'),(154,'Annibale','Panichella'),(155,'Andy','Zaidman'),(156,'Andrea De','Lucia'),(157,'Mohsen','Laali'),(158,'Huai','Liu'),(159,'Margaret','Hamilton'),(160,'Maria','Spichkova'),(161,'Heinz W.','Schmidt'),(162,'Yanmei','Zhang'),(163,'Yi','Bian'),(164,'Serkan','Kirbas'),(165,'Yue','Jia'),(166,'Fang','Yuan'),(167,'Ruilian','Zhao'),(168,'Satya','Kanduri'),(169,'Ryan','Carlson'),(170,'Anne','Denton'),(171,'John','Penix'),(172,'Ripon K.','Saha'),(173,'Sarfraz','Khursid'),(174,'Dewayne E.','Perry'),(175,'Matthias','Hirzel'),(176,'Jonathan Immanuel','Brachth√§user'),(177,'Herbert','Klaeren'),(178,'Zengkai','Ma'),(179,'Jianjun','Zhao'),(180,'Alexander P.','Conrad'),(181,'Zhi Quan','Zhou'),(182,'Willy','Susilo'),(183,'Kuan-Li','Peng'),(184,'Jun','Cheng'),(185,'Dusica','Marijan'),(186,'Yves','Ledru'),(187,'Alexandre','Petrenko'),(188,'Sergiy','Boroday'),(189,'Nadine','Mandran'),(190,'Waseem','Asghar'),(191,'Sepehr','Eghbali'),(192,'Dennis','Jeffrey'),(193,'Neelam','Gupta'),(194,'Yen-Ching','Hsu'),(195,'Stephen W.','Thomas'),(196,'Ahmed E.','Hassan'),(197,'Dorothea','Blostein'),(198,'Luay','Tahat'),(199,'Bogdan','Korel'),(200,'George','Koutsogiannakis'),(201,'Nada','Almasri'),(202,'Christopher','Henard'),(203,'Mike','Papadakis'),(204,'Yves Le','Traon'),(205,'Dmitry','Nurmuradov'),(206,'Ren√©e','Bryce'),(207,'Hema','Srikanth'),(208,'Sean','Banerjee'),(209,'Xiaofang','Zhang'),(210,'Xiaoyuan','Xie'),(211,'Tsong Yueh','Chen'),(212,'Laurie','Williams'),(213,'Jason','Osborne'),(214,'R.','Krishnamoorthi'),(215,'S. A. Sahaaya Arul','Mary'),(216,'Hasan','Ural'),(217,'Seyedeh Sepideh','Emam'),(218,'James','Miller');
/*!40000 ALTER TABLE `author` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `authorship`
--

DROP TABLE IF EXISTS `authorship`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `authorship` (
  `study_idStudy` int(11) NOT NULL AUTO_INCREMENT,
  `study_publication_vehicle_idpublication_vehicle` int(11) NOT NULL,
  `author_idauthor` int(11) NOT NULL,
  PRIMARY KEY (`study_idStudy`,`study_publication_vehicle_idpublication_vehicle`,`author_idauthor`),
  KEY `fk_study_has_author_author1_idx` (`author_idauthor`),
  KEY `fk_study_has_author_study1_idx` (`study_idStudy`,`study_publication_vehicle_idpublication_vehicle`),
  CONSTRAINT `fk_study_has_author_author1` FOREIGN KEY (`author_idauthor`) REFERENCES `author` (`idauthor`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `fk_study_has_author_study1` FOREIGN KEY (`study_idStudy`, `study_publication_vehicle_idpublication_vehicle`) REFERENCES `study` (`idStudy`, `publication_vehicle_idpublication_vehicle`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=110 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `authorship`
--

LOCK TABLES `authorship` WRITE;
/*!40000 ALTER TABLE `authorship` DISABLE KEYS */;
INSERT INTO `authorship` VALUES (1,1,1),(1,1,2),(1,1,3),(1,1,4),(2,2,5),(28,7,5),(2,2,6),(28,7,6),(2,2,7),(28,7,7),(2,2,8),(3,3,9),(3,3,10),(3,3,11),(3,3,12),(4,4,13),(4,4,14),(4,4,15),(5,3,16),(5,3,17),(5,3,18),(5,3,19),(6,5,20),(6,5,21),(6,5,22),(6,5,23),(30,25,23),(56,5,23),(57,36,23),(76,5,23),(85,6,23),(6,5,24),(7,6,24),(30,25,24),(54,9,24),(56,5,24),(57,36,24),(85,6,24),(6,5,25),(6,5,26),(7,6,26),(30,25,26),(54,9,26),(56,5,26),(57,36,26),(85,6,26),(7,6,27),(7,6,28),(7,6,29),(7,6,30),(8,7,31),(88,41,31),(8,7,32),(88,41,32),(93,49,32),(8,7,33),(9,8,33),(88,41,33),(93,49,33),(8,7,34),(20,18,34),(88,41,34),(9,8,35),(9,8,36),(93,49,36),(9,8,37),(9,8,38),(10,9,39),(81,45,39),(10,9,40),(81,45,40),(94,50,40),(10,9,41),(10,9,42),(11,10,43),(11,10,44),(11,10,45),(12,11,46),(15,13,46),(12,11,47),(15,13,47),(13,12,48),(35,13,48),(13,12,49),(35,13,49),(14,12,50),(14,12,51),(16,14,52),(16,14,53),(16,14,54),(17,15,55),(89,6,55),(17,15,56),(89,6,56),(17,15,57),(89,6,57),(17,15,58),(89,6,58),(18,16,59),(18,16,60),(19,17,61),(19,17,62),(20,18,63),(68,40,63),(68,40,64),(20,18,65),(68,40,65),(21,19,66),(21,19,67),(22,9,68),(22,9,69),(22,9,70),(22,9,71),(23,20,72),(23,20,73),(23,20,74),(24,21,75),(24,21,76),(24,21,77),(38,25,77),(39,22,77),(40,6,77),(41,6,77),(42,30,77),(43,25,77),(47,6,77),(48,32,77),(50,33,77),(52,30,77),(56,5,77),(57,36,77),(72,41,77),(75,33,77),(84,6,77),(85,6,77),(24,21,78),(52,30,78),(25,22,79),(25,22,80),(26,8,81),(26,8,82),(45,22,82),(26,8,83),(27,23,84),(73,31,84),(86,46,84),(27,23,85),(73,31,85),(86,46,85),(27,23,86),(73,31,86),(86,46,86),(27,23,87),(73,31,87),(28,7,88),(29,24,89),(29,24,90),(29,24,91),(30,25,92),(85,6,92),(30,25,93),(56,5,93),(57,36,93),(85,6,93),(31,26,94),(31,26,95),(32,27,96),(80,44,96),(33,28,97),(33,28,98),(33,28,99),(34,20,100),(34,20,101),(34,20,102),(36,29,103),(36,29,104),(36,29,105),(36,29,106),(37,13,107),(37,13,108),(37,13,109),(38,25,110),(40,6,110),(38,25,111),(40,6,111),(38,25,112),(40,6,112),(39,22,113),(41,6,113),(72,41,113),(75,33,113),(39,22,114),(41,6,114),(42,30,115),(43,25,115),(47,6,115),(48,32,115),(50,33,115),(74,25,115),(84,6,115),(98,51,115),(42,30,116),(48,32,116),(45,22,117),(45,22,118),(45,22,119),(79,43,119),(46,31,120),(50,33,120),(63,35,120),(84,6,120),(46,31,121),(50,33,121),(63,35,121),(84,6,121),(90,6,121),(49,6,122),(69,31,122),(71,31,122),(82,31,122),(49,6,123),(58,22,123),(69,31,123),(97,5,123),(108,2,123),(49,6,124),(51,34,125),(61,37,125),(70,9,125),(92,48,125),(51,34,126),(51,34,127),(61,37,127),(70,9,127),(92,48,127),(51,34,128),(61,37,128),(52,30,129),(53,34,130),(53,34,131),(53,34,132),(53,34,133),(54,9,134),(55,35,135),(59,2,135),(55,35,136),(59,2,136),(55,35,137),(59,2,137),(55,35,138),(59,2,138),(58,22,139),(58,22,140),(58,22,141),(60,26,142),(60,26,143),(60,26,144),(60,26,145),(62,33,146),(62,33,147),(62,33,148),(64,38,149),(64,38,150),(65,30,151),(65,30,152),(95,32,152),(66,39,153),(66,39,154),(66,39,155),(66,39,156),(67,31,157),(67,31,158),(67,31,159),(67,31,160),(67,31,161),(68,40,162),(69,31,163),(71,31,163),(82,31,163),(69,31,164),(69,31,165),(97,5,165),(71,31,166),(71,31,167),(82,31,167),(72,41,168),(74,25,169),(74,25,170),(75,33,171),(76,5,172),(76,5,173),(76,5,174),(77,42,175),(77,42,176),(77,42,177),(78,21,178),(78,21,179),(79,43,180),(80,44,181),(80,44,182),(81,45,183),(94,50,183),(82,31,184),(83,37,185),(87,47,186),(87,47,187),(87,47,188),(87,47,189),(89,6,190),(90,6,191),(91,45,192),(91,45,193),(94,50,194),(95,32,195),(95,32,196),(95,32,197),(96,41,198),(105,55,198),(106,25,198),(108,2,198),(96,41,199),(105,55,199),(106,25,199),(107,35,199),(108,2,199),(96,41,200),(105,55,200),(106,25,200),(107,35,200),(96,41,201),(97,5,202),(97,5,203),(97,5,204),(98,51,205),(98,51,206),(100,45,207),(102,53,207),(100,45,208),(101,37,209),(101,37,210),(101,37,211),(102,53,212),(102,53,213),(103,3,214),(104,54,214),(103,3,215),(104,54,215),(108,2,216),(109,36,217),(109,36,218);
/*!40000 ALTER TABLE `authorship` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `comparison_technique`
--

DROP TABLE IF EXISTS `comparison_technique`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `comparison_technique` (
  `idcomparison_technique` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(100) DEFAULT NULL,
  `short_name` varchar(45) DEFAULT NULL,
  `granularity_id` int(11) DEFAULT '11',
  `approach` varchar(45) DEFAULT NULL,
  `reference` varchar(200) DEFAULT NULL,
  `feedback` varchar(1) DEFAULT NULL,
  PRIMARY KEY (`idcomparison_technique`),
  KEY `granularity_id_fk_idx` (`granularity_id`),
  CONSTRAINT `granularity_id_fk` FOREIGN KEY (`granularity_id`) REFERENCES `granularity` (`idgranularity`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=154 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `comparison_technique`
--

LOCK TABLES `comparison_technique` WRITE;
/*!40000 ALTER TABLE `comparison_technique` DISABLE KEYS */;
INSERT INTO `comparison_technique` VALUES (1,'refactoring based approach','RBA',11,'refactoring','',''),(2,'total statement coverage','',2,'coverage','','0'),(3,'total method coverage','',1,'coverage','','0'),(4,'additional method coverage','',1,'coverage','','1'),(5,'random order','',11,'','',''),(6,'change blocks','CB',8,'','Srivastava A, Thiagarajan J. Effectively prioritizing tests in development environment. ACM SIGSOFT Software Engineering Notes, Vol. 27, ACM: New York, 2002; 97‚Äì106',''),(7,'total coverage (generic)','',11,'','',''),(8,'additional coverage (generic)','',11,'','',''),(9,'adaptive random test case prioritization','ART',11,'','',''),(10,'total time-aware','',11,'','','0'),(11,'additional time-aware','',11,'','','1'),(12,'total integer linear programming','',11,'','','0'),(13,'additional integer linear programming','',11,'','','1'),(14,'coverage-based ART','',11,'coverage / search','',''),(15,'ordered sequence prioritization','',11,'','',''),(16,'additional statement coverage','',2,'coverage','','1'),(17,'time-aware total statement coverage using integer linear programming','',11,'','',''),(18,'time-aware additional statement coverage using integer linear programming','',11,'','','1'),(19,'Clustering Bayesian Network','CBN',11,'','',''),(20,'Bayesian Network','BN',11,'','','0'),(21,'Bayesian Network with feedback','BNA',11,'','','1'),(22,'coverage-based method-level clustering approach','MCC',1,'coverage','',''),(23,'total branch coverage','',7,'coverage','','0'),(24,'total line coverage','',10,'coverage','','0'),(25,'IRCOV total branch coverage','',7,'coverage','','0'),(26,'IRCOV total line coverage','',10,'coverage','','0'),(27,'IRCOV total method coverage','',1,'coverage','','0'),(28,'additional line coverage','',10,'coverage','','1'),(29,'additional branch coverage','',7,'coverage','','1'),(30,'IRCOV additional branch coverage','',7,'coverage','','1'),(31,'IRCOV additional line coverage','',10,'coverage','','1'),(32,'IRCOV additional method coverage','',1,'coverage','','1'),(33,'Manhattan distance-based ART','',11,'similarity','',''),(34,'Additional branch coverage with ART','Add ART',7,'coverage','','1'),(35,'ART with additional branch coverage','ART Add',7,'coverage','','1'),(36,'total fault-exposing potential','FEP-total',11,'','','0'),(37,'additional fault-exposing potential','FEP-addtl',11,'','','1'),(38,'statement total fault-exposing potential','st-fep-total',2,'','','0'),(39,'statement additional fault-exposing potential','st-fep-addtl',2,'','','1'),(40,'total function coverage','fn-total',3,'coverage','','0'),(41,'additional function coverage','fn-addtl',3,'coverage','','1'),(42,'function total fault-exposing potential','',3,'','','0'),(43,'function additional fault-exposing potential','',3,'','','1'),(44,'function total fault index','fn-fi-total',3,'','','0'),(45,'function additional fault-index','fn-fi-addtl',3,'','','1'),(46,'function total fault-index with FEP','fn-fi-fep-total',3,'','','0'),(47,'function additional fault-index with FEP','fn-fi-fep-addtl',3,'','',''),(48,'total DIFF prioritization','',11,'modification','','0'),(49,'additional DIFF prioritization','',11,'modification','','1'),(50,'total function diff','fn-diff-total',3,'modification','','0'),(51,'additional function diff','fn-diff-addtl',3,'modification','','1'),(52,'total block coverage','',8,'coverage','','0'),(53,'additional block coverage','',8,'coverage','','1'),(54,'total method DIFF prioritization','',1,'modification','','0'),(55,'additional method DIFF prioritization','',1,'modification','','1'),(56,'2-optimal algorithm','',11,'search','',''),(57,'hill climbing','',11,'search','',''),(58,'genetic algorithm','',11,'genetic','',''),(59,'total coverage bayesian network class level','',4,'coverage','','0'),(60,'additional coverage bayesian network class level','',4,'coverage','','1'),(61,'ART statement max-min','',2,'','',''),(62,'ART statement max-average','',2,'','',''),(63,'ART statement max-max','',2,'','',''),(64,'ART function max-min','',3,'','',''),(65,'ART function max-average','',3,'','',''),(66,'ART function max-max','',3,'','',''),(67,'ART branch max-min','',7,'','',''),(68,'ART branch max-average','',7,'','',''),(69,'ART branch max-max','',7,'','',''),(70,'fault-exposing potential','FEP',11,'','',''),(71,'SEQUOIA','',11,'','',''),(72,'adaptive test-case prioritization','',11,'','',''),(73,'total block diff','',8,'modification','','0'),(74,'additional block diff','',8,'modification','','1'),(75,'extended method coverage','',1,'coverage','',''),(76,'extended statement coverage','',2,'coverage','',''),(77,'Non-dominated Sorting Genetic Algorithm','NSGA-II',11,'','',''),(78,'Two Archive Evolutionary Algorithm','TAEA',11,'','',''),(79,'additional statement diff coverage','',2,'coverage / modification','',''),(80,'additional statement fault history coverage','',2,'coverage / history','',''),(81,'additional branch diff coverage','',7,'coverage / modification','',''),(82,'total branch diff coverage','',7,'coverage / modification','',''),(83,'statement ART','',2,'','',''),(84,'function ART','',3,'','',''),(85,'branch ART','',7,'','',''),(86,'PORA random','',11,'','',''),(87,'PORA distance','',11,'','',''),(88,'total statement call-graph-based','',2,'coverage','','0'),(89,'additional statement call-graph-based','',2,'coverage','','1'),(90,'string-distance-based','',2,'','',''),(91,'topic-model with R-Ida','',2,'','',''),(92,'topic-model with Mallet','',2,'','',''),(93,'statement search based','',2,'search','',''),(94,'statement FATCP','',2,'','',''),(95,'branch FATCP','',7,'','',''),(96,'online statement 0','',2,'coverage','','1'),(97,'online statement 1','',2,'coverage','','1'),(98,'online branch 0','',7,'coverage','','1'),(99,'online branch 1','',7,'coverage','','1'),(100,'global similarity-based','',11,'','',''),(101,'LBS max-min','',11,'','',''),(102,'LBS max-avg','',11,'','',''),(103,'LBS max-max','',11,'','',''),(104,'Total Jupta method level method test','',1,'','',''),(105,'Additional Jupta method level method test','',1,'','','1'),(106,'Total Jupta statement level method test','',2,'','',''),(107,'Additional Jupta statement level method test','',2,'','','1'),(108,'Total Jupta method level class test','',1,'','',''),(109,'Additional Jupta method level class test','',1,'','','1'),(110,'Total Jupta statement level class test','',2,'','',''),(111,'Additional Jupta statement level class test','',2,'','','1'),(112,'history-based','',11,'history','J.M. Kim, A. Porter, A history-based test prioritization technique for regression testing in resource constrained environment, in: Proc. of the 24th Int‚Äôl Conf. Soft. Eng., 2002, pp. 119‚Äì129.',''),(113,'Variable coefficient','',11,'','',''),(114,'Constant coefficient','',11,'','',''),(115,'geTLO','',11,'','',''),(116,'additional MC/DC coverage','',11,'coverage','','1'),(117,'additional {SA0, SA1} coverage','',11,'coverage','','1'),(118,'additional {CCF,CDF} coverage','',11,'coverage','','1'),(119,'additional {ENF, ASF, ORG} coverage','',11,'coverage','','1'),(120,'additional {CCF, CDF, ENF, ASF, ORF} coverage','',11,'coverage','','1'),(121,'enhanced additional branch coverage','EAGA',7,'coverage','','1'),(122,'string-based','',11,'similarity','Ledru Y, Petrenko A, Boroday S, Mandran N (2011) Prioritizing test cases with string distances.',''),(123,'black-box call-graph-based','',11,'','',''),(124,'topic-model','',11,'','',''),(125,'selective test prioritization','S',11,'model','',''),(126,'count-based test prioritization','',11,'model','',''),(127,'frequency-based test prioritization','',11,'model','',''),(128,'model dependence-based test prioritzation','IP',11,'model','',''),(129,'additional spanning statement','',2,'','','1'),(130,'additional spanning branches','',7,'','','1'),(131,'statement diversity','',2,'','',''),(132,'branch diversity','',7,'','',''),(133,'t-wise','',11,'','',''),(134,'input model diversity','',11,'','',''),(135,'total input model mutation','',11,'','','0'),(136,'additional input model mutation','',11,'','','1'),(137,'min. output diversity','',11,'','',''),(138,'max output diversity','',11,'','',''),(139,'input diversity with NCD','',11,'','',''),(140,'input diversity with Levenshtein','',11,'','',''),(141,'input test set diameter','',11,'','',''),(142,'output test set diameter','',11,'','',''),(143,'combinatorial pair of windows','Win',11,'','',''),(144,'combinatorial inter-windows pair parameter','Par',11,'','',''),(145,'combinatorial inter-windows pair parameter/value','PV',11,'','',''),(146,'hybrid multilevel coarse-to-fine-grained method','ML',11,'','',''),(147,'dynamic history based','',11,'history','',''),(148,'RL-based HMM','',11,'','',''),(149,'acumulated q-value','',11,'','',''),(150,'Bayesian Network (method coverage)','BN',1,'coverage',NULL,'0'),(151,'Bayesian Network with feedback (method coverage)',NULL,1,'coverage',NULL,'1'),(152,'Bayesian Network (block coverage)',NULL,8,'coverage',NULL,'0'),(153,'Bayesian Network wiith feedback (block coverage)',NULL,8,'coverage',NULL,'1');
/*!40000 ALTER TABLE `comparison_technique` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `evaluation`
--

DROP TABLE IF EXISTS `evaluation`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `evaluation` (
  `idevaluation` int(11) NOT NULL AUTO_INCREMENT,
  `results` varchar(999) DEFAULT NULL,
  `tools` varchar(999) DEFAULT NULL,
  `comparisons` varchar(999) DEFAULT NULL,
  `type` varchar(45) DEFAULT NULL,
  `study_id` int(11) DEFAULT NULL,
  PRIMARY KEY (`idevaluation`),
  KEY `fk_study_id` (`study_id`),
  CONSTRAINT `fk_study_id` FOREIGN KEY (`study_id`) REFERENCES `study` (`idStudy`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=112 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `evaluation`
--

LOCK TABLES `evaluation` WRITE;
/*!40000 ALTER TABLE `evaluation` DISABLE KEYS */;
INSERT INTO `evaluation` VALUES (1,'Performs better than the compared algorithm (Genetic Algorithm) for all measures used','not clear','Compared with Zheng genetic algorithm. (Zheng., Harman M., and Hierons R., ‚ÄúSearch\nAlgorithms for Regression Test Case\nPrioritization,‚Äù IEEE Transaction on Software\nEngineering, vol. 33, pp. 225-237, 2007)','case study',1),(2,'In 53% of the cases, RBA enabled fault detection after running a single test case (F measure D 1), which is the best scenario for a prioritized suite. This rate was at least 2.6 times higher than all the other techniques (CB = 20%; RD = 0%; TSC = 13%; TMC = 13%; ASC = 13%; AMC = 13%). Moreover, RBA placed at least one fault-revealing test case among the six first test cases in 93% of the cases, a satisfactory position, given the size of the suites‚Äîranging from 65 to 504 tests. Another result was the high stability of RBA. With exception of one case, APFD for RBA varied over a very tight range [0.925; 0.999], that is, RBA‚Äôs orderings detected all behavioural changes early, and in similar positions.','ref-finder (identify from two consecutive versions of a java program, which refactoring edits were applied), priorJ (prioritization tool)','RBA, TSC (total statement coverage), TMC (total method coverage), ASC (additional statement coverage), AMC (additional method coverage), RD (random), CB (change blocks (Srivastava A, Thiagarajan J. Effectively prioritizing tests in development environment. ACM SIGSOFT Software\nEngineering Notes, Vol. 27, ACM: New York, 2002; 97‚Äì106.))','case study',2),(3,'Both null hypothesis were rejected. Accepting the alternative hypothesis that  there is significative difference between the prioritization techniques under investigation.',NULL,'RBA, TSC (total statement coverage), TMC (total method coverage), ASC (additional statement coverage), AMC (additional method coverage), RD (random), CB (change blocks (Srivastava A, Thiagarajan J. Effectively prioritizing tests in development environment. ACM SIGSOFT Software\nEngineering Notes, Vol. 27, ACM: New York, 2002; 97‚Äì106.))','experiment',2),(4,'The range of APFD is between 57% and 80% for ComboRT, 46%‚Äì75% for AMC, and 43%‚Äì75% for AMC',NULL,'TMC (total method coverage), AMC (additional method coverage)','experiment',3),(5,'The proposed technique performed better than all other compared techniques',NULL,'normal, random, function-total, function-additional, agglomerative hierarchical clustering based, k-means clustering based','case study',4),(6,'The evaluation showed that executing test cases from the first cluster, contains 67,85% of the test cases affected by a change. Precision reached an average of 73%',NULL,'','case study',5),(7,'when using the most up-to-date coverage information, for the majority of the subjects, the additional and the search-based techniques are among the best  prioritization technique, while the ART technique is also competitive. This finding confirms the previous work on search-based test prioritization [8] and adaptive random test prioritization [7], respectively.The search-based test prioritization technique is superior to the ART (adaptive random test) technique','FaultTracer tool (ASM byte-code manipulation and analysis), GUROBI optimization (Linear integer programming solver), PIT mutation testing tool (seeding faults)','Total coverage, Additional coverage, genetic-algorithm-based, adaptive random test, total time-aware, additional time-aware, total with integer linear programming, additional with integer linear programming','experiment',6),(8,'First, the coverage-based technique is ignorably better than the additional coverage-based technique for coverage (i.e., in terms of APxC). Second, the optimal coverage-based technique is significantly worse than the additional coverage-based technique for fault detection (i.e., in terms of APFD). Third, the optimal coverage-based technique is much less time-efficient than the additional coverage-based technique.','SIR (software infrastructure repository), MuJava(java mutation generation), Proteum (c mutation generation), ILOG CPLEX (IBM‚ÄôS integer linear programming tool)','optimal coverage-based test case prioritization and additional coverage-based test-case prioritization','experiment',7),(9,'the comparison between the optimal technique and the ideal technique. That is, the additional technique significantly outperforms the ideal technique in terms of APxC results, but the latter significantly outperforms the former in terms of APFD results. \nFrom this study, the ideal optimal test-case prioritization achieves the best APFD results but the worst APxC results. This observation also consolidates our findings in the first study. That is, it is not worthwhile to purse optimality by taking the coverage as an intermediate goal in test-case prioritization. ','MutGen(C mutation generation), Javalanche (java mutation generation), GUROBI (integer linear programming tool)','optimal coverage-based test-case prioritization, additional coverage-based test-case prioritization, ideal optimal test-case prioritization','experiment',7),(10,'The ordered-sequence technique achieved higher APFD values than the other two techniques, which means a faster rate of fault detection.','CodeCover (source code coverage information), Jumble (source code mutation generation)','Random test case prioritization, coverage-based ART technique, ordered-sequence prioritization','experiment',8),(11,'prioritizing test cases both on their coverage and time cost (rt-add, ILP-add) (rt-tot, ILP-tot) cannot significantly outperform that only on their coverage (st-add) (st-tot) in most cases both for the purpose of statement coverage and fault detection.\n','gcov (coverage information), time (collect test execution time)','random ordering, total statement coverage, additional statement coverage, ratio of total statement coverage to time cost, ratio of additional statement coverage to time coverage, time-aware total statement coverage using integer linear programming, time-aware additional statement coverage using integer linear programming','experiment',9),(12,'The proposed technique (genetic algorithm considering historical information of faults) performed better than the compared ones in terms of APFDc. Also, when adjusting the parameter of APFDc in order to be equivalent to APFD, the proposed technique also performed better than the others. (except from the optimal, which is a control technique)','SIR (software infrastructure repository)','random, optimal, historical fault detected, GA historical faults (proposed), historical value cost severity, genetic algorithm using function coverage, total function coverage, cost severity total function coverage','experiment',10),(13,'The results indicates that the proposed technique performed better than random prioritization approach.',NULL,'random prioritization, hierarchical prioritization','case study',11),(14,'the authors found that the proposed approach performs better than the compared ones in terms of APFD.','Eclipse Emma (code coverage), MatLab (implementation of the proposed technique)','random, optimal, FEP (not explained)','case study',12),(15,'The proposed technique achieved better APFD than the compared ones. ','','no order, random order, reverse order, total fault coverage, proposed order','case study',13),(16,'the proposed approach performs better than the compared approaches in terms of APFD. ',NULL,'greedy prioritization, additional greedy prioritization, proposed approach','case study',14),(17,'according to the author, the presented technique achieve 100% fault coverage after execution of 50% of the test suite. Compared to the random technique, the presented technique performs better.','Emma (coverage information), ant (execution information)','random order prioritization','case study',15),(18,'the proposed technique performed better than the techniques APFD (average percentage of faults detection) and OTCP (optimal test case prioritization).',NULL,'APFD, OTCP   (didn\'t describe them)','case study',16),(19,'regarding effectiveness, the proposed approach MOTCP performed better than the compared approaches for the LaTazza application and performed equally well as Additional code coverage for the AveCalc application. Regarding sensitivity, the proposed approach outperforms other approaches for measure APFD sub1 and sub2 for application AveCalc, while the random approach outperforms the other techniques for APFD sub1. ','Traceclipse (traceability), JMetal (genetic algorithm NSGA-II)','random prioritization, code coverage, additional code coverage','case study',17),(20,'the proposed approach performed better than the compared one for 4 out of 6 versions of the tested software. The other two versions achieved better results after a ‚Äútunning‚Äù in the approach parameters.','SIR (repository), ','failure pursuit sampling, adapted failure pursuit sampling ','case study',18),(21,'The approach was demonstrated with an example and APBC metric was calculated. For the example, the proposed approach achieved the best results together with the additional coverage method.','','additional coverage, fault exposure potential, execution time, requirement property relevance, multi-objective prioritization','case study',19),(22,' Particularly, Euclidean distance outperformed other similarity measures for all samples\nover the program print_tokens and print_tokens2. The application of Euclidean distance reduces the risk of missing faults in practice. Compared to coverage-based approaches, it performed better than random prioritization and additional functional coverage in terms of NAPFD.  Performed equal to Additional statement coverage and was comparable to the better coverage-based approach (additional branch coverage).\n','gcov (instrumentation), SLOCCount (loc metric)','GSTCP (proposed) , random, additional function coverage, additional statement coverage, additional branch coverage','experiment',20),(23,' the proposed approach performed better than the compared ones in terms of APCC measure, that is, coverage of conditions of a program. Also, achieved faster execution time.',NULL,'no order, reverse order, random order, optimum order (rate of fault detection), genetic order and bee colony optimization order','case study',21),(24,': results show that the proposed approach performed better than the compared approaches in terms of APFD.','SIR (objects), Major Mutation Framework (mutation tests generation), Cobertura (coverage information), sandmark (change analysis information), ckjm (quality metrics gathering)','Clustering ‚Äì BN based approach (CBN, proposed), Additional greedy approach (ADD), BN based approach (BN), BN based approach with feedback (BNA), method-level coverage based clustering approach (MCC)','experiment',22),(25,'the proposed prioritization approach performed better in terms of APFD than the compared approaches and equally well with the optimal order.',NULL,'original order, optimal order, random order, ACO algorithm (proposed)','case study',23),(26,'The IRCOV approaches generally performed more effectively than random and coverage-based techniques. There are 48 comparisons between the proposed techniques and baseline approaches for each object. In 43 of these 48 cases IRCOV techniques were better than baseline techniques, with 40 of the 43 improvements being statistically significant.','SIR, JaCoCo (coverage information), Ubuntu diff (change information)','random order, total branch coverage, total line coverage, total method coverage, IRCOV total branch coverage, IRCOV total line coverage, IRCOV total method coverage, additional branch coverage, additional line coverage, additional method coverage, IRCOV additional branch coverage, IRCOV additional line coverage, IRCOV additional method coverage','experiment',24),(27,'the proposed approach was able to operate on large binaries built from millions of lines of source code and produce results within a few minutes.','Echelon (prioritization framework that implements the proposed algorithm), Magellan test effectiveness tool, Vulcan (framework for analysis and optimization of dynamic binary code)',NULL,'case study',25),(28,'The results show that the approach does not introduce significant overheads into test suites execution. For effectiveness, it was found that not always the proposed approach is better than random ordering. ',NULL,'random ordering','case study',26),(29,'according to the author, the proposed approach presented ‚Äúconsiderable improvements in fault detection and stability of the results, for various test suites‚Äù. However, these results are not clear in the presented graphs.','SAS (statistical analysis)','random ordering','case study',27),(30,'the proposed approach outperformed the compared ones in almost every case, except for two. This indicates that, for the applied context (refactoring faults), the proposed approach is more likely to find refactoring faults faster.','PriorJ (implemented tool)','random ordering, Srivastava and thiagarajan approach, total statement coverage, total method coverage, additional statement coverage, additional method coverage','case study',28),(31,'the proposed work achieved better AFPD than the traditional compared approaches. Comparing to the kavitha et al. approach, it slightly improves the APFD .',NULL,'kavitha et al, reverse order, random order\nR. Kavitha and N. Sureshkumar, ‚ÄúTest case prioritization for\nregression testing based on severity of fault,‚Äù Int‚Äôl J. Computer Sc.\nand Eng. (IJCSE), vol. 02, no. 05, pp. 1462-1466, 2010','case study',29),(32,'The experimental results indicate that, on average, Jupta can be as effective as coverage information based techniques.','SIR, ASM (bytecode manipulation framework, used to implement compared techniques)','random ordering, total method coverage, additional method coverage, JuptaT, JuptaA','experiment',30),(33,'the results show that using the proposed approach, a larger amount of defects can be found in less time, when compared to random ordering.',NULL,': random ordering, proposed technique','case study',31),(34,'The experiments with the two programs under tests, Replace and Space program show that there is different performance of the studied methods when applied to different types of program. The results with Space show that the studied methods performed better when applied to larger program with larger test suite. All four compared methods outperformed random order for the Space (larger) program in terms of F-Measure. The best was additional branch coverage, followed by Add ART, ART and ART Add. For the smaller program, not every approach outperformed random ordering. In fact, only ART, which achieved the best f-measure, outperformed random ordering significantly.','SIR (repository infrastructure), gcov (coverage information)','random ordering, additional branch coverage (Add), Manhattan distance-based ART (ART), additional branch coverage with ART (Add ART), ART with additional branch coverage (ART Add).','experiment',32),(35,'it is found that the additional FEP achieves higher APFD than the compared techniques.',NULL,'total statement coverage, additional statement coverage, total branch coverage, additional branch coverage, total fault exposing potential (FEP), additional fault exposing potential (FEP)','case study',33),(36,'the APFDc achived by the proposal is higher than the additional statement coverage',NULL,'additional statement coverage','case study',34),(37,'compared to the original genetic algorithm and simulated annealing, the proposed approach executes in less time. Also, the proposed approach achieves results nearly to optimum time.','','genetic algorithm, simulated annealing, GASA (proposed)','case study',35),(38,'the m-ACO technique performs equally good or better than the compared techniques in terms of APFD and PTR.',NULL,'m-ACO (proposed), BCO (bee colony optimization), GA (genetic algorithm), ACO (ant colony optimization)','case study',37),(39,'the results indicate that all the techniques compared provide significant improvement in rate of fault detection. However, different techniques performed differently on each subject program, indicating that for different characteristics of program different techniques may be more suitable. Overall, averaging the results for all subjects, the FEP-based heuristics perform better than the others.','Aristotle (test coverage and control-flow graph information), Proteum (mutation system)','Random order, optimum order, total branch coverage (branch-total), additional branch coverage (branch-addtl), total statement coverage (stmt-total), additional statement coverage (stmt-addtl), total fault-exposing-potential (FEP-total), additional fault-exposing-potential (FEP-addtl)','experiment',38),(40,'concerning granularity effects on prioritization, it was found that statement level techniques are more effective than function level techniques. As for the fault indexes (fault proneness) it was found that adding those information to prioritization techniques do not improve their effectiveness.','Aristotle (test coverage and control-flow graph information), Proteum (mutation system)','Random order, optimum order, statement level (total statement coverage (st-total), additional statement coverage (st-addtl), total fault-exposing-potential (st-fep-total), additional fault-exposing-potential (st-fep-addtl)), function level  ( total function coverage (fn-total), additional function coverage (fn-addtl), total FEP (fn-fep-total), additional FEP (fn-fep-addtl), total fault index (fn-fi-total), additional fault-index (fn-fi-addtl), total fault-index with FEP (fn-fi-fep-total), additional fault-index with FEP (fn-fi-fep-addtl))','experiment',39),(41,'4 studies were performed. The first study, using real faults, reveal that FEP based approaches performed better than the compared ones on average. On the second study, mutant faults are used, and as result, FEP based approaches performed better than the compared ones. For the third study, where real faults were used, additional FEP outperformed the other techniques, but there was no significant difference between total FEP and the coverage based techniques. For the fourth study, mutant faults were used. Again, additional FEP outperformed the other techniques.','Aristotle (test coverage and control-flow graph information), Proteum (mutation system)','Random order, optimum order, total statement coverage (stmt-total), additional statement coverage (stmt-addtl), total branch coverage (branch-total), additional branch coverage (branch-addtl), total fault-exposing-potential (st-fep-total), additional fault-exposing-potential (st-fep-addtl), ','experiment',40),(42,'concerning granularity effects on prioritization, it was found that statement level techniques are more effective than function level techniques. As for the fault indexes (fault proneness) it was found that adding those information to prioritization techniques did improve their effectiveness statistically significantly.','Aristotle (test coverage and control-flow graph information), Proteum (mutation system), diff (unix syntatic difference tool)','Random order, optimum order, statement level (total statement coverage (st-total), additional statement coverage (st-addtl), total fault-exposing-potential (st-fep-total), additional fault-exposing-potential (st-fep-addtl)), function level  ( total function coverage (fn-total), additional function coverage (fn-addtl), total FEP (fn-fep-total), additional FEP (fn-fep-addtl), total fault index (fn-fi-total), additional fault-index (fn-fi-addtl), total fault-index with FEP (fn-fi-fep-total), additional fault-index with FEP (fn-fi-fep-addtl), total DIFF prioritization, additional DIFF prioritization, total DIFF with FEP prioritization, additional DIFF with FEP prioritization))','experiment',41),(43,'On both grep and flex, in terms of mean APFD, optimal ranks first, fn-addtl ranks second, and fn-fi-addtl ranks third. On both programs, techniques using feedback (addtl) produce APFDs closer to   optimal than do techniques not using feedback (total). On QTB, in contrast, the average APFD for\ntechniques using feedback exceeds the average APFD for techniques not using feedback. Further, on grep and flex, techniques using feedback exhibited less variance in APFD than those not using feedback, whereas, on QTB, this relationship was reversed. Another surprise was the high mean APFD value exhibited by the random technique on grep and flex. On QTB, the random technique outperforms the other techniques in some cases (evident in the extents of the tails of the distributions), but, in terms of\nmean APFD, it is the worst performing technique overall\n','TSL (test scripts)','random, optimal, fn-total, fn-addtl, fn-fi-total, fn-fi-addtl, fn-diff-total, and fn-diff-addtl','experiment',41),(44,'it was found that the use of prioritization methods can improve the rate of fault detection in the JUnit environment when compared to traditional approaches (untreated). However, when compared to a random ordering, it was found that at the class testing level, none of the approaches improves (statistically significantly) the random ordering effectiveness and at the method testing level, block-addtl and method-addtl differ from the random ordering significantly. \nRegarding coverage information, different level of coverage information did not impact the effectiveness of prioritization. As for the use of feedback information, the results indicate that there is a significant difference between its use and non-use at test-method level but no difference at test-class level.\nFor modification information, there were no significant difference between techniques that use and don‚Äôt use modification information at either test suite level.\nFor the granularity of the test suite, the results indicat','Galielo (coverage information)','random ordering, optimal ordering, total block coverage, additional block coverage, total method coverage, additional method coverage, total diff method, additional diff method','experiment',42),(45,'the results show that the compared techniques outperformed untreated and random orderings in all but a few cases. The results also show that test case prioritization can improve the rate of fault detection of JUnit test suite, assessed relative to mutation faults, but the results vary with the numbers of mutations faults and with the test suites‚Äô fault detection ability.','Galielo (coverage information)',' random ordering, optimal ordering, total block coverage, additional block coverage, total method coverage, additional method coverage','experiment',43),(46,'on average, the prioritizations created with fitnesses based on block coverage outperformed those developed with fitnesses based on method coverage. Overall, the GA-produced prioritizations performed extremely well in comparison to randomly generated prioritizations. Nearly all results were more than one standard deviation from the mean APFD values calculated for prioritizations that were produced randomly.','mma (coverage information), Jester (seeding mutation faults)','random ordering, fault-aware prioritization','case study',45),(47,'As far as the level of granularity for coverage information is concerned, there is no meaningful difference between class level and method level techniques. Also, techniques employing the feedback mechanism (or ‚Äúadditional techniques‚Äù) bring about better results. As for BN technique, the median of its AFPD values among all versions is better than all other techniques (however not significantly). Furthermore, feedback employing techniques perform better when a very small number of faults are\navailable, but as the potential number of faults grows BN is the most promising technique.\n','ckjm (software metrics information), Emma (coverage information), Sandmark (program change information), Smile Library (Bayesian network framework), SIR (software repository)','optimal order, random order, class coverage, method coverage, class additional coverage, method additional coverage, change coverage, change additional coverage, BN (proposed)','case study',46),(48,'for the ant object, noncontrol techniques improved the rate of fault detection compared to\nboth randomly ordered and untreated test suites.  Regarding the effects of information types on prioritization, the level of coverage information utilized (block versus method) had no effect on the techniques‚Äô rate of fault detection at the test-method and test-class levels. In contrast, comparing the results of block-total to block-addtl and method-total to method-addtl at the test-method level shows that techniques using feedback did yield improvement over those not using feedback\nFor Jmeter, noncontrol techniques significantly improved the rate of fault detection compared to random\nand untreated orderings in all cases other than the one involving random orderings at the test-method level. Regarding the effects of information types and feedback, in the box plots, no visible between techniques was found.\nFor xml-security, noncontrol techniques improved the rate of fault detection compared to\nbo','Sofya (bytecode analysis coverage information), SIR (repository)','random order, optimal order, total block coverage, additional block coverage, total method coverage, additional method coverage','experiment',47),(49,'For galileo, noncontrol techniques improved the rate of fault detection compared to untreated test suites.\nNo noncontrol techniques produced results better than random orderings; however, random orderings\noutperformed both total techniques overall. The level of coverage information utilized (block versus method) had an effect on techniques‚Äô rate of fault detection for total coverage techniques, but not for\nadditional coverage techniques. Also, techniques using feedback do yield improvements over those not using feedback\nFor nanoxml, all noncontrol techniques significantly improved the rate of fault detection compared to untreated test suites, whereas the only significant difference involving randomly ordered test suites was an improvement associated with block-addtl. Regarding the effects of information types and feedback\nand their use in prioritization, the results are the same as those seen on galileo.\n','Sofya (bytecode analysis coverage information), SIR (repository), ','random order, optimal order, total block coverage, additional block coverage, total method coverage, additional method coverage','experiment',47),(50,'Considering results from ant, at both test suite levels, there were prioritization techniques that outperformed both untreated and randomly ordered test suites. Random test case orderings outperformed untreated test case orderings at the test-method level.\nBlock-level techniques were not significantly different overall from method-level techniques.','Galileo (analysis of bytecode to gather coverage information)','random order, optimal order, total block coverage, additional block coverage, total method coverage, additional method coverage, total diff method coverage, additional diff method coverage','experiment',48),(51,'the results for small test suites suggests that where applicable, the cheaper-to-implement-and-execute Additional Greedy and 2-Optimal Algorithms should be used.  For large test suites, the results produced by Hill Climbing and Genetic Algorithms vary with each experiment, while the results for the Greedy, Additional Greedy, and 2-Optimal Algorithms do not, since these algorithms are\ndeterministic.\n','Cantata++ (coverage information), SPSS (statistical analysis), SIR (repository)','2-optimal algorithm, hill climbing, greedy algorithm, additional greedy algorithm, genetic algorithm','experiment',49),(52,'Overall, cost-benefit values decreased as time constraint levels increase. In the case of non-feedback techniques negative cost-benefit values were observed in all cases in which no time constraints applied. When time constraints applied, techniques produced positive cost-benefit values on the three object programs that have JUnit test cases in all but one case with values usually trending upwards as time constraints increase. In the case of feedback techniques, the positive effects of prioritization, together with upward trends as time constraint levels increase. \nThe results of this study suggest, in fact, that using simple rate of fault detection measures to assess prioritization techniques may be misleading, and that more comprehensive economic models can lead to quite different conclusions about the cost-effectiveness of heuristics','sandmark (change information), ckjm (quality merics), sofya (coverage information)','random order, total block coverage, total coverage Bayesian network (class level coverage), additional block coverage, additional Bayesian network (class level coverage)','experiment',50),(53,'for all Siemens programs except schedule2, the means of APFD values for ART prioritization techniques are higher than that of random ordering. For UNIX programs, all the techniques using ‚Äúmaxmin‚Äù (equation (1) for f2) have higher mean APFD values than random ordering while other ART\ntechniques are comparable to random.  Every ART technique can improve over random by 5 to 10%.\nAt the same time, the best greedy coverage-based technique (namely, additional branch) can improve over random by up to 11%. Thus, the best ART technique (namely, ART-br-maxmin) and the best coverage-based technique can achieve almost the same improvements over random ordering.\nDifferent levels of coverage information do have impact on the ART prioritization techniques. In general,\nbranch-level techniques are comparable to statement-level techniques and both of them are more effective than function-level techniques.\nART-br-maxmin is comparable to the best coverage-based prioritization technique in terms of APFD r','SIR (repository), gcov (coverage information)','random order, total statement coverage, total function coverage, total branch coverage, additional statement coverage, additional function coverage, additional branch coverage, ART statement maxmin, ART statement maxaverage, ART statement maxmax, ART function maxmin, ART function maxaverage, ART function maxmax, ART branch maxmin, ART branch maxaverage, ART branch maxmax.','experiment',51),(54,'oracle-centric prioritization outperforms both random and additional-block-coverage prioritization for each case example','CLOC (lines of code metric), Sofya (mutantfaults generation)',' random ordering, additional block coverage','case study',52),(55,'ADDST and FEP are the best performing techniques, especially in the Siemens programs. For the larger SIR programs, however, RAPTOR has the best performance, tied with ADDST.','Zoltar (spectrum-based fault localization tool, used for generating mutation faults)','random order, ART, Additional statement coverage, FEP, SEQUOIA.','experiment',53),(56,'the adaptive approach is usually significantly more effective than the total approach and is competitive to the additional approach. Moreover, the adaptive approach is significantly better than\nthe additional approach on some objects (2 of them).\n',NULL,'total statement coverage, additional statement coverage, adaptive test-case prioritization (proposed)','experiment',54),(57,'although the granularity level of coverage affects results in all approaches, the effect is\nmuch more pronounced in techniques that are based on additional coverage.\nIn both additional coverage and additional coverage of modified code techniques, the finer grained coverage criteria (block, basic block and decision) perform similarly overall and significantly\noutperform the coarser grained coverage criteria (function entry and function return).\nThe two approaches based on additional coverage using finer grained coverage criteria yield the\nhighest APFD scores of all applied prioritisation approaches\n','xSuds (Visualization and test analysis tool, provides the implementations of the prioritization techniques and code coverage analysis)','total coverage, additional coverage, total coverage of modified code, additional coverage of modified code. All techniques with different coverage granularities: function entry, function return, block, basic block and decision.','experiment',55),(58,'The proposed approach statistically significantly outperform the additional strategies using method coverage. Some of the proposed strategies using differentiated p values with method coverage even statistically significantly outperform the additional strategies using statement coverage.','ASM (byte-code analysis framework)','total coverage, additional coverage, extended greedy coverage (proposed); all at method and statement coverage granularity.','experiment',56),(59,'The proposed test case prioritization approach generated a spectrum of techniques that\nwere more effective than those associated with the total strategy and competitive with those associated with the additional strategy. Some techniques generated by our unified approach even outperformed it. Also, the proposed approach was more effective for Java programs than C programs in incorporating the benefits of the additional strategy.\n','MutGen, Javalanche, MuJava (mutation faults generation for C and Java)','total and additional coverage, extended greedy coverage (proposed). All of them at different coverage granularities levels (method and statement)','experiment',57),(60,'MOEAs and their variants tend to outperform the cost-cognisant additional greedy algorithms in general','valgrind (profiling tool)',' Non-dominated Sorting Genetic Algorihtm (NSGA-II), Two Archive Evolutionary Algorithm (TAEA), additional statement coverage, additional statement diff coverage, additional statement fault history coverage','experiment',58),(61,'Using prioritization techniques based on additional coverage does, in fact, significantly improve the fault detection rate of a regression test suite over random ordering and the original test suite. However, the APFD scores of these prioritization techniques are significantly lower than optimal. This suggests that better prioritization techniques still need to be investigated, probably based on entirely different principles than coverage and code modification analysis','xSuds (regression framework that support the compared prioritization techniques)','total coverage, additional coverage, total coverage of modified code, additional coverage of modified code. At different coverage granularities: function, statement, branches.','case study',59),(62,'ARW (replacing worst individual) and ARP (replacing the parent with the child) are significantly better than ARR (replacing at random) in terms of APFD. No significant difference was found for ARO (replacing oldest individual)','Mujava (mutation tool)','proposed method (genetic) with 4 different replacement strategies (replacing with the worst individual; replacing at random; replacing the oldest individual or replace the parent with the child)','experiment',60),(63,'both pora-distance and pora-random are either more effective than or as effective as nonPORA techniques. In terms of time to execute, PORA techniques are only slightly slower than random ordering and some greedy techniques, but are much more efficient than existing code-coverage-based additional greedy techniques, branch-level techniques, and statement-level ART techniques.','gcov (coverage information)','random order;  [total coverage, additional coverage and ART] at different coverage granularities: statement, function and branch; PORA random and PORA distance.','experiment',61),(64,'On average, static technique call-graph additional is the most effective technique at test-class level, whereas dynamic technique greedy additional statement coverage is the most effective technique at test-method level. Overall, the static techniques outperform the dynamic ones at test-class level, but the dynamic techniques outperform the static ones at test-method level. \nThe test granularity significantly impacts the effectiveness of TCP techniques. All the studied techniques perform better at test-method level compared to test-class level.\n','PIT (mutation tool)',' static (call-graph-based [total strategy; additional strategy], string-distance-based, topic-model [using R-Ida package; using Mallet] vs dynamic techniques (greedy total, greedy additional, adaptive random and search based, all at statement level.','experiment',62),(65,'BNA results in better values of APFD for nano and galileo, the two objects with the TSL test suite. However, for the rest of objects (the ones with Junit testcases), the differences are not statistically significant.','Sandmark (change information)','Bayesian network (BN), bayesian network with feedback (BNA)','experiment',63),(66,'the proposed approach outperformed the compared techniques in terms of APFD within the same programs and corresponding test suites.','LDRA Testbed (coverage information, fault detection history)',' total statement coverage, total branch coverage, statement FATCP (proposed), branch FATCP (proposed)','experiment',64),(67,'the similarity metric IBC (improved basic counting) outperforms the others metrics, indicating that it‚Äôs the best choice when using a similarity-based prioritization approach.','daikon (execution traces generation), AspectJ (execution trace generation)','prioritization using different similarity metrics','experiment',65),(68,'for the two-objective formulation there is an improvement in terms of APFDc ranging between 5% and 11%, while in the three-objective formulation the improvement ranges between 3 %\nand 12 %. HGA is statistically better than the additional greedy in 4 cases out of 6 for both two- or three-objective TCP problems\n','gcov (coverage information, execution frequency of code instructions), JMetal (java-based framework for multi-objective optimization with metaheuristics)','HGA with two criteria (execution cost and statement coverage), cost cognizant additional greedy), HGA with three criteria, additional greedy (code coverage, execution cost and past coverage)','experiment',66),(69,'using the proposed technique can outperform the baseline techniques. The average APFD score of the proposed new techniques are always among the leading group, and mostly they outperform the baselines significantly (53 out of 56 cases).\nOnline branch-based techniques outperform branch-based baselines. online statement-based techniques outperform statement-based baselines; and online branch-based techniques outperform both branch and statement-based baselines.',NULL,' additional statement coverage, additional branch coverage, total statement coverage, total branch coverage, optimal order, low bound (random) order, online statement 0 (behaving initially like additional), online statement 1 (behaving initially like total), online branch 0 and online branch 1.','experiment',67),(70,'for the ART-based technique and for the global similarity technique, Euclidian distance performed better as a similarity measure than the compared ones. When comparing the two prioritization techniques using the Euclidian distance similarity measure, global similarity performed better than ART-based. When compared to the coverage-based prioritization techniques, the global similarity performed equally well as the best coverage-based technique, additional branch coverage.','gcov (execution profiles gathering), SLOCCount (LOC metric)','ART-based, global similarity-based. Both with similarity measures: Cosine similarity, jaccard index and variants, proportional distance, Euclidean distance. Additional function coverage, additional statement coverage, additional branch coverage, random order','experiment',68),(71,'both 100 % APCC and APSC can be achieved with 0.2 % of test cases and 0.45 % of total execution time for the entire suite.',NULL,'APSC (Average Percentage of Statements Coverage), APCC (Average Percentage of Change Coverage) and EET (Effective Execution Time) objectives','case study',69),(72,'LBS techniques performed better than total and random approaches and equally well as additional statement and additional branch and better than additional function coverage. Across the LBS approaches, the LBS maxmin performed better than maxavg and maxmax.',NULL,'random ordering, total statement coverage, total function coverage, total branch coverage, additional statement coverage, additional function coverage, additional branch coverage, [LBS maxmin, LBS maxavg, LBS maxmax] proposed','experiment',70),(73,'the proposed crossover operators outperform original crossovers in terms of APSC.',NULL,'genetic algorithm with and without epistasis, with single and two point crossover, and a two-point crossover Partially-Mapped.','experiment',71),(74,'techniques using feedback ( addtl and addtl-diff) usually produced better prioritization results than random, and in some cases approximated optimal ordering. In contrast, the  implest prioritization technique, total, produced an average APFD lower than or equal to that produced by random. The use of modification information (indicated by the -diff suffix) sometimes improved the total technique, but often caused the addtl technique to behave more poorly',NULL,'total function coverage, additional function coverage, total binary-diff function coverage, additional binary-diff function coverage, random order','experiment',72),(75,'the proposed approach improved the APFD considerably over random approach.','S.A.S (statistics)','random ordering, and proposed approach (historical test case performance)','experiment',73),(76,'all heuristic techniques (proposed) outperformed their corresponding control technique. Regarding time constraint, overall,the code complexity with clustering (Tcc-clst) technique produced\nthe best results in terms of improvement over the control technique\n','Matlab (clustering)',' total coverage, total code complexity, fault history based, combined (code complexity and fault history) and those same techniques using clustering.','case study',74),(77,': Overall, all instantiations of the prioritization technique perform better than no-prioritization. The best window time was 0.1.',NULL,'different time windows are used','case study',75),(78,'REPiR overall outperforms all the JUPTA and coverage-based approaches (total or additional) regardless of test-case granularities','Indri toolkit (IR framework)','random order, additional method coverage, total method coverage, additional statement coverage, total statement coverage, JUPTA (static-analysis based, with 4 different variants: method additional, method total, statement additional, statement total)','experiment',76),(79,'the results show that the dynamic variants of the proposed technique achieve better APFD than its static counterpart, however, they add considerably overhead to the prioritization process. LFP and (D)CFP outperforms GFP in most of the cases. Compared to other techniques, the proposed one tend to achieve higher APFD values.','SIR (repository)','Global frequency-based (GFP), local frequency-based (LFP), change frequency-based (CFP) and those with feedback mechanism (DGFP, DLFP, DCFP), random ordering, total block coverage, additional block coverage, total method coverage, additional method coverage, total-diff method coverage, additional-diff method coverage, Bayesian network, similarity-measures based','experiment',77),(80,'The proposed approach performs better than two widely used techniques, 8.5 % better than MT,\n3.4 % than MA on average in NDS (newly developed tests) testing and 15 % better than DMT, 10 % than DMA on average in regression testing\n','Apros (tool implementing the proposed approach), Celadon (change information)','random order (Ran), total method coverage (MT), additional method coverage (MA), total-diff method coverage (DMT), additional-diff methods coverage (DMA)','experiment',78),(81,'The selection operator TRU40 has the best CE score across all applications.',NULL,'random order, hill climbing based approach, genetic algorithm with different operators','case study',79),(82,'experimental results with the replace program show that the coverage-based measure (namely, CMD) was better than the frequency-based measures (namely, FMD and FHD) as well as RT, and branch CMD was better than statement CMD. It is also revealed that frequency information is useful and can complement the coverage based measure.\nFor space program, ART based on CMD was the best algorithm, and branch CMD was better than statement CMD.\n','gcov (execution trace and coverage information)',' fault-detection capabilities of Frequency Manhattan distance x Frequency Hamming Distance used on adaptive random technique with different granularities (branch, statement). Coverage manhattan distance','experiment',80),(83,'for the replace program, out of the 6 investigated methods for the replace program, cmd-10 was the best in terms of both F-measure and APFD. \nFor space program, the results are consistent with the replace results: cmd-10 was the best among the 6\nmethods in terms of F-measure and APFD.\n','gcov (execution trace and coverage information)','Coverage Manhattan Distance x Jaccard Distance on ART at branch level. [ART algorithms: FSCS-ART, Flex-ART]; random order; additional coverage','experiment',80),(84,'on average, the proposed technique can significantly improve the effectiveness in comparison with three coverage-based techniques and two other history-based techniques.',NULL,'andom order, historical-fault-detected ((Kim and Porter, 2002), GA with historical records, historical value cost and severity, GA using function coverage, total function coverage, cost cognizant total function coverage\nKim, J.M., Porter, A., 2002. A history-based test prioritization technique for regression testing in resource constrained environments. In: Proceedings of the International Conference on Software Engineering (ICSE 2002), Orlando, FL, USA, pp.\n119‚Äì129.\n','experiment',81),(85,'GPU execution speed-up effect is very different between different programs. About V8 program, the speed-up rate of parallel execution is more than 100x\nthan serial execution results\n',' SIR (repository)',' NSGA-II CPU x GPU version','case study',82),(86,'proposed prioritization framework has higher rate of regression fault detection per unit of test\ncase cost than manual approach.\n','','manual testing vs proposed approach (multi-perspective)','case study',83),(87,'Tccf always produced greater cost-benefits as TCL increased (15 increases, 9 of them statistically significant).\n. Tbnf often produced greater cost-benefits as TCL increased (12 increases, 8 of them statistically significant).\n. Tbn was less stable in producing cost-benefits as TCL increased (9 increases, 5 of them statistically\nsignificant).\n. Tcc was least stable (8 increases, 3 statistically significant).\nRegarding faultiness level: \nAt faultiness levels FL1 and FL2, in all cases but one, heuristics failed to outperform random orderings (the single exception occurring for Tbnf on galileo at FL2). At faultiness level FL3, however, several techniques outperformed random orderings. Moreover, the ranking between techniques changes as faultiness level moves from FL2 to FL3 in more than half of the cases, with the performance of heuristics improving. In particular, techniques using feedback performed better than the control techniques in several cases: Tccf on ant, nanoxml, and galileo, ','Sofya (coverage information), ckjm (quality metrics), sandmark (change information)','Different time constraints (no time constraint, 25%, 50%, 75%) applied to techniques: random order, total block coverage (tcc), total Bayesian network (class level) (tbn), additional block coverage (tccf), additional Bayesian network (class level) (tbnf).   Different faultiness levels: FL1: 1 to 5 faults; FL2: 6 to 10 faults; FL3: 11 to 15 faults.','experiment',84),(88,'Among the eight JUPTA techniques, techniques using feedback tend to be more effective than their corresponding techniques that do not use feedback, and techniques using feedback at the test-method\nlevel (AJ-MM and AJ-SM) tend to be the most effective. Techniques using feedback retain the\nadvantage over techniques without feedback because the latter may postpone the detection of faults\nin rarely covered statements.\nJUPTA is more effective than an approach that reuses untreated test suites. JUPTA is also more effective than the random approach at the test-method level, and usually more effective than the random approach at the test-class level. As JUPTA schedules test cases based on their TA (testing ability) values, the test suites generated by JUPTA are intuitively better than the test suites in random or untreated orders on fault-detection effectiveness.\nAs dynamic coverage-based techniques use actual coverage information while JUPTA uses estimated\ncoverage information, the former is','SIR (repository), muJava (mutation faults)','random order, all 8 variations of JUPTA (proposed) (with/without feedback mechanism [TJ/AJ]; at method/statement level [M/S]; using class or method test granularity [C/M]), dynamic techniques with the same 8 variations as JUPTA','experiment',85),(89,'cases the VC approach leads to an improvement over KP is significantly more than those where results are degraded.\nthe overall results imply a greater likelihood of exposing faults earlier by the VC approach and show that the VC has the best performance among the three introduced history-based approaches.\n','ATAC (coverage information), SAS (statistics)','proposed approach (VC, Variable Coefficient), a previous proposed approach (CC, constant coefficient), Kim and Porter approach (KP)\nJ.M. Kim, A. Porter, A history-based test prioritization technique for regression testing in resource constrained environment, in: Proc. of the 24th Int‚Äôl Conf. Soft. Eng., 2002, pp. 119‚Äì129.\n','experiment',86),(90,'On average, prioritized test suites have a higher APFD than randomly ordered ones.\nTotal statement coverage is the best method in two of seven cases; it is not statistically different\nthan most methods in one case and is statistically worse than all distance-based prioritization in four cases. \nAmong the distance measures used, Manhattan distance provides the best results.\n','SIR (repository)','random order, string distance (proposed, with different distance measures [Cartersian, Edit, Hamming, Manhattan]), total statement coverage','experiment',87),(91,'FOS techniques based on the ordered sequences of program entities, especially statement entities, may have only minor actual differences from the most effective group and outperform most of the other techniques. In addition, GOS techniques can statistically outperform other ICP techniques.\nTechniques based on the ordered sequence of program entities, especially ones that adopt FOS utilizing statements and GOS utilizing branches, can find more bugs in loops than the other techniques when the same number of test cases are executed.\n','SIR (repository), Jumble (mutation), CodeCover (coverage information)','additional statement coverage, additional branch coverage, proposed technique variations (FOS-statement, FOS-branch, GOS statement, GOS-branch; all using edit distance), similarity-technique using execution profile and proportional binary distance variants (ART-branch, ART-statement, ICP-branch, ICP-statement).','experiment',88),(92,'MOTCP+ (proposed) outperforms: (i) MOTCP 80% of the applications, (ii) AddCodeCov 66.6% of the applications, and (iii) NSGAIIdim2 62% of the applications','MOTCP+ (tool implementing the proposed approach), MOTCP (tool implementing previous approach, extended), SWT-Metrics (automatic weighting), Traceclipse (recover traceability links), JMetal (NSGA-II solver)','random order, code coverage, additional code coverage, NSGA-II (dimensions: [code coverage and execution cost], [code coverage, requirements coverage, execution time], [requirements coverage, code coverage and execution cost; proposed])','experiment',89),(93,'In all programs, GeTLO-6 and GeTLO-0 are ranked as the highest, although they may share this rank\nwith other alternatives.\n‚óè On Ant, almost all techniques exhibit similar performance. \n‚óè On Jtopas and Nano, all techniques are outperformed by the GeTLO. \n‚óè On Galileo, the first rank is shared among GeTLO-0, GeTLO-6 and BNA.\n','SIR (repository), Mujava (mutation), Sofya (coverage information), Fault Tracer (coverage information), Matlab (implementation of the proposed technique)','additional coverage, total coverage, Bayesian network, additional Bayesian network and geTLO (with different depth parameters, 0 to 6)','experiment',90),(94,'M OD * (REG + OI + POI) approach has been shown to be an improvement over the REG\napproach in terms of rate of fault detection.\n',NULL,'total statement coverage (REG), REG+OI+POI, GRP_REG + OI + POI and MOD * (REG + OI + POI)','experiment',91),(95,'We observe that the LBS family of techniques can achieve higher mean effectiveness than, if not as effective as,\nthe additional greedy, 2-Optimal, Hill Climbing, and Genetic Algorithm. Among all techniques studied in the controlled experiment,\nwe observe that the most effective technique is LBS(min-avg). However, the differences between LBS techniques and other techniques are not statistically significant.\nThe input-based LBS techniques can be as effective as the best search-based techniques using code coverage information and yet\nare much more efficient.\n','SIR (repository)','LBS technique family(min;ag;max;min-avg;min-max;avg-max;min-avg-max), total statement coverage, additional statement coverage, 2-optimal, hill climbing, genetic algorithm','experiment',92),(96,'Although there is no significant difference between the fine-grained criteria, M7 performs more effectively and with greater stability. Moreover, M6 and M7 are complementary criteria for achieving stable effectiveness.','CodeCover (coverage information)','random order (m2), additional branch coverage (m3), additional MC/DC (m4), additional {SA0, SA1} coverage (m5), additional {CCF,CDF} coverage (m6), additional {ENF, ASF, ORF} coverage (m7), additional {CCF, CDF, ENF, ASF, ORF} coverage (m8)','experiment',93),(97,'EAHA and AGA outperforms GA in terms of APDC, except for one case from 8 cases, where the software under test was the biggest one. In terms of APFD, AGA outperformed EAGA and GA. In terms of APFDc, EAGA outperformed APFD with EAGA. However, APFDc with AGA and GA did not outperform APFD with AGA and GA.\r',NULL,'Enhanced Additional Greedy Algorithm - EAGA (proposed), Additional Greedy Algorihtm, Greedy Algorithm, all at branch coverage','experiment',94),(98,'the proposed topic-based technique is more effective than the comapred techniques when applied to the studied systems.',NULL,'black-box call graph based, string-based (Ledru et al. 2011), topic-based (proposed), random order.\rLedru Y, Petrenko A, Boroday S, Mandran N (2011) Prioritizing test cases with string distances.\rAutom Softw Eng 19(1):65‚Äì95\r','experiment',95),(99,'All of the proposed techniques performs better than random ordering in terms of RP (relative position) metric. For single marked transitions (changes in the code) the ranking of techniques effectiveness is H4, (S, H1, H2, H3), H5. For multiple marked transitions, the ranking is H3, IP, H1, S, (H2 and H5), H4. Overall, IP, H3,H1,H2,(S and H5), H4.',NULL,'random ordering, (selective test prioritization (S), count ‚Äìbased test prioritization (h1,h2,h3 variants), frequency-based test prioritzation (h4,h5 variants), model dependence-based test prioritization (IP)) proposed','experiment',96),(100,'Regarding white-box techniques, AB, ASS and ASB produce the best fault detection rates. For black-box techniques, I-TSD, t-W (with t=4) and IMD produce the best fault detection rates. When comparing white-box vs black-box, the differences between the APFDs of the white- and black-box techniques range from 2% to 4% and\rwhite-box approaches perform better than black-box ones in 56 to 60% of the cases. Regarding exeuction time, the black-box approaches, overall, take more time than the white-box ones to prioritize the test suite.\r','cloc (loc metric gathering)','TS (total statement coverage), AS (additional statement coverage), TB (total branch coverage), AB (additional branch coverage), TM (total method coverage), AM (additional method coverage), ASS (additional spanning statement), ASB (additional spanning branches), SD (statement diversity), BD (branch diversity), t-W (t-wise), IMD (Input Model diversity), TIMM (total input model mutation) AIMM (additional input model mutation), MiOD (min. Output diversity), MaOD (max output diversity), ID-NCD (input diversity with NCD), ID-Lev (Input diversity with Levenshtein), I-TSD (input test set diameter), O-TSD (output test set diameter)','experiment',97),(101,'ML outperforms all single-criterion methods in two out of four applications. The comparison to random ordering reveals ML produces better results in all four applications. The comparison to the previously studied two-way combinatorial prioritization using parameter/values (PV) reveals that hybrid multilevel prioritization (ML) yields higher mean APFD values in three out of four applications.',NULL,'random ordering, combinatorial prioritization that uses pair of windows as criterion (Win), combinatorial prioritization that uses inter-windows pair of parameter as criterion (Par), combinatorial prioritization that uses inter-windows pairs of parameter/values as criterion (two-way inter-window, PV), Hybrid multilevel coarse-to-fine-grained method (ML)','experiment',98),(102,'History-based performs better than rp-based in terms of APFD and random ordering.',NULL,'tcp initialization (based on importance of requirements), random ordering, rp-based (based on priority of requirements), history-based (dynamic proposed method)','experiment',99),(103,'PORT achieves 10 to 30% better results in terms of WPFD than random ordering',NULL,'random ordering, PORT (proposed)','case study',100),(104,'the proposed approach outperforms random ordering. Regarding the distance measure used, although sometimes Manhattan distance has its favorable scenarios to achieve higher APFD value, techniques with CP distance have better APFD values than those with Manhattan distance in most cases. ARS-pass and ARS-all algorithms deliver comparable performance with Ledru algorithm in terms of\rAPFD.\r',NULL,'random ordering, [CP-ARS-all, CP-ARS-pass] proposed, Ledru algorithm using CP metric, [Mht-ARS-all, Mht-ARS-pass]  proposed algorithms using Manhattan Distance measure, Mht-Ledru.','experiment',101),(105,'The results indicate that the proposed prioritization scheme leads to improved rate of failure detection for both the projects over random ordering',NULL,'random ordering, requirements based technique (proposed)','case study',103),(106,'The proposed approach achieves better results in terms of TPFD than the compared approaches.','Emma (coverage information)','random ordering, requirements based approach (proposed), total statement coverage, total method coverage','case study',104),(107,'some model-based test prioritization methods may improve on average the effectiveness of early fault detection as comparedto random prioritization',NULL,'random prioritization, selective test prioritization, model dependence-based test prioritization, count-based test prioritization, frequency-based test prioritization','experiment',105),(108,'The best performance is shown by the model dependence-based test prioritization (IP) and count-based test prioritization.',NULL,'random prioritization, selective test prioritization, model dependence-based test prioritization, count-based test prioritization','experiment',106),(109,'The model-based technique, count-based test priorization, achieved better results in terms of RP than the code-based technique, additional statement coverage.',NULL,'additional statement coverage, count-based test prioritization, random ordering','experiment',107),(110,'results indicate that the model-based test prioritization approach may improve the\reffectiveness of test prioritization for Version II of selective test prioritization and the model\rdependence-based test prioritization over random ordering. \r',NULL,'random ordering, selective test priorization, model dependence-based test priorization ','experiment',108),(111,'RL-basedHMM achieve significantly better results than the compared approaches','AutoBlackTest (generation of tests and models)','RL-based HMM, acumulated q-value] proposed, additional statement coverage, random ordering','experiment',109);
/*!40000 ALTER TABLE `evaluation` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `evaluation_comparison_technique`
--

DROP TABLE IF EXISTS `evaluation_comparison_technique`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `evaluation_comparison_technique` (
  `id_evaluation` int(11) NOT NULL,
  `id_comparison_technique` int(11) NOT NULL,
  KEY `id_evaluation_fk_idx` (`id_evaluation`),
  KEY `id_comparison_technique_fk_idx` (`id_comparison_technique`),
  CONSTRAINT `id_comparison_technique_fk` FOREIGN KEY (`id_comparison_technique`) REFERENCES `comparison_technique` (`idcomparison_technique`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `id_evaluation1_fk` FOREIGN KEY (`id_evaluation`) REFERENCES `evaluation` (`idevaluation`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `evaluation_comparison_technique`
--

LOCK TABLES `evaluation_comparison_technique` WRITE;
/*!40000 ALTER TABLE `evaluation_comparison_technique` DISABLE KEYS */;
INSERT INTO `evaluation_comparison_technique` VALUES (2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(4,3),(4,4),(7,7),(7,8),(7,9),(7,10),(7,11),(7,12),(7,13),(8,7),(9,8),(8,8),(10,5),(10,14),(10,15),(11,5),(11,2),(11,16),(11,17),(11,18),(24,19),(24,8),(24,20),(24,21),(24,22),(26,5),(26,23),(26,24),(26,25),(26,26),(26,27),(26,28),(26,29),(26,30),(26,31),(26,32),(34,5),(34,7),(34,33),(34,34),(34,35),(39,5),(39,23),(39,29),(39,2),(39,16),(39,36),(39,37),(40,5),(40,2),(40,16),(40,38),(40,39),(40,40),(40,41),(40,42),(40,43),(40,44),(40,45),(40,46),(40,47),(41,5),(41,2),(41,16),(41,23),(41,29),(41,36),(41,37),(42,5),(42,2),(42,16),(42,38),(42,39),(42,40),(42,41),(42,42),(42,43),(42,44),(42,45),(42,46),(42,47),(42,48),(42,49),(43,5),(43,40),(43,41),(43,44),(43,45),(43,50),(43,51),(44,5),(44,52),(44,53),(44,3),(44,4),(44,48),(44,49),(45,5),(45,52),(45,53),(45,3),(45,4),(48,5),(48,52),(48,53),(48,3),(48,4),(49,5),(49,52),(49,53),(49,3),(49,4),(50,5),(50,52),(50,53),(50,3),(50,4),(50,54),(50,55),(51,56),(51,57),(51,58),(51,7),(51,8),(52,5),(52,52),(52,59),(52,53),(52,60),(53,5),(53,2),(53,40),(53,23),(53,16),(53,41),(53,29),(53,61),(53,62),(53,63),(53,64),(53,65),(53,66),(53,67),(53,68),(53,69),(54,5),(54,53),(55,5),(55,9),(55,16),(55,70),(55,71),(56,2),(56,16),(56,72),(57,40),(57,52),(57,41),(57,53),(57,50),(57,73),(57,51),(57,74),(58,3),(58,2),(58,4),(58,16),(58,75),(58,76),(59,3),(59,2),(59,4),(59,16),(59,75),(59,76),(60,77),(60,78),(60,16),(60,79),(60,80),(61,40),(61,2),(61,23),(61,41),(61,16),(61,29),(61,50),(61,79),(61,82),(61,51),(61,79),(61,81),(62,58),(63,5),(63,2),(63,40),(63,23),(63,16),(63,41),(63,29),(63,83),(63,84),(63,85),(63,86),(63,87),(64,88),(64,89),(64,90),(64,91),(64,92),(64,2),(64,16),(64,83),(64,93),(65,20),(65,21),(66,2),(66,23),(66,94),(66,95),(69,16),(69,29),(69,2),(69,23),(69,5),(69,96),(69,97),(69,98),(69,99),(70,9),(70,100),(70,41),(70,16),(70,29),(70,5),(72,5),(72,2),(72,40),(72,23),(72,16),(72,41),(72,29),(72,101),(72,102),(72,103),(82,83),(82,85),(83,85),(83,5),(83,8),(87,5),(87,52),(87,59),(87,53),(87,60),(88,5),(88,104),(88,105),(88,106),(88,107),(88,108),(88,109),(88,110),(88,111),(89,112),(89,113),(89,114),(90,5),(90,90),(90,2),(91,16),(91,29),(91,85),(91,83),(92,5),(92,7),(92,8),(92,77),(93,7),(93,8),(93,20),(93,21),(93,115),(96,5),(96,29),(96,116),(96,117),(96,118),(96,119),(96,120),(97,121),(97,29),(97,23),(98,122),(98,123),(98,124),(98,5),(99,5),(99,125),(99,126),(99,127),(99,128),(100,2),(100,16),(100,23),(100,29),(100,3),(100,4),(100,129),(100,130),(100,131),(100,132),(100,133),(100,134),(100,135),(100,136),(100,137),(100,138),(100,139),(100,140),(100,141),(100,142),(101,5),(101,143),(101,144),(101,145),(101,146),(102,5),(102,147),(111,16),(111,5),(111,148),(111,149);
/*!40000 ALTER TABLE `evaluation_comparison_technique` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `evaluation_faults_type`
--

DROP TABLE IF EXISTS `evaluation_faults_type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `evaluation_faults_type` (
  `id_evaluation` int(11) NOT NULL,
  `id_faults_type` int(11) NOT NULL,
  KEY `id_evaluation_fk_idx` (`id_evaluation`),
  KEY `id_fautls_type_fk_idx` (`id_faults_type`),
  CONSTRAINT `id_evaluation_fk` FOREIGN KEY (`id_evaluation`) REFERENCES `evaluation` (`idevaluation`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `id_fautls_type_fk` FOREIGN KEY (`id_faults_type`) REFERENCES `faults_type` (`idfaults_type`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `evaluation_faults_type`
--

LOCK TABLES `evaluation_faults_type` WRITE;
/*!40000 ALTER TABLE `evaluation_faults_type` DISABLE KEYS */;
INSERT INTO `evaluation_faults_type` VALUES (50,1),(74,1),(84,1),(48,2),(49,2),(54,2),(55,2),(59,2),(62,2),(64,2),(87,2),(88,2),(91,2),(93,2),(53,3),(75,4),(79,4),(80,4),(88,4),(93,4),(52,5),(58,5),(60,5),(65,5),(90,5),(56,6),(66,6),(69,6),(72,6),(78,6),(82,6),(83,6),(94,6),(95,6),(68,6),(70,6),(89,6),(53,7),(57,7),(53,7),(57,7),(60,7),(61,7),(63,7),(67,7),(70,7),(75,7),(76,7),(77,7),(79,7),(86,7),(92,7),(96,4),(97,6),(98,6),(99,2),(100,2),(101,7),(102,7),(103,7),(104,5),(105,6),(106,6),(107,1),(108,1),(109,1),(110,2),(111,7);
/*!40000 ALTER TABLE `evaluation_faults_type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `evaluation_metric`
--

DROP TABLE IF EXISTS `evaluation_metric`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `evaluation_metric` (
  `evaluation_idevaluation` int(11) NOT NULL,
  `prioritization_metric_idprioritization_metric` int(11) NOT NULL,
  PRIMARY KEY (`evaluation_idevaluation`,`prioritization_metric_idprioritization_metric`),
  KEY `fk_evaluation_has_prioritization_metric_prioritization_metr_idx` (`prioritization_metric_idprioritization_metric`),
  KEY `fk_evaluation_has_prioritization_metric_evaluation1_idx` (`evaluation_idevaluation`),
  CONSTRAINT `fk_evaluation_has_prioritization_metric_evaluation1` FOREIGN KEY (`evaluation_idevaluation`) REFERENCES `evaluation` (`idevaluation`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `fk_evaluation_has_prioritization_metric_prioritization_metric1` FOREIGN KEY (`prioritization_metric_idprioritization_metric`) REFERENCES `prioritization_metric` (`idprioritization_metric`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `evaluation_metric`
--

LOCK TABLES `evaluation_metric` WRITE;
/*!40000 ALTER TABLE `evaluation_metric` DISABLE KEYS */;
INSERT INTO `evaluation_metric` VALUES (1,1),(51,1),(71,1),(73,1),(85,1),(1,2),(51,2),(97,2),(1,3),(21,3),(51,3),(2,4),(4,4),(5,4),(7,4),(8,4),(9,4),(10,4),(13,4),(14,4),(15,4),(16,4),(17,4),(19,4),(20,4),(25,4),(26,4),(28,4),(29,4),(30,4),(31,4),(32,4),(35,4),(38,4),(39,4),(40,4),(41,4),(42,4),(43,4),(44,4),(45,4),(46,4),(47,4),(48,4),(49,4),(50,4),(53,4),(54,4),(56,4),(57,4),(58,4),(59,4),(61,4),(62,4),(63,4),(64,4),(65,4),(66,4),(69,4),(72,4),(74,4),(75,4),(76,4),(77,4),(78,4),(79,4),(83,4),(84,4),(88,4),(89,4),(90,4),(91,4),(92,4),(93,4),(94,4),(95,4),(96,4),(97,4),(98,4),(100,4),(101,4),(102,4),(104,4),(111,4),(2,5),(3,5),(30,5),(34,5),(82,5),(83,5),(3,6),(6,7),(6,8),(8,9),(9,9),(11,10),(11,11),(12,12),(36,12),(55,12),(60,12),(68,12),(80,12),(84,12),(86,12),(97,12),(18,13),(18,14),(18,15),(18,16),(18,17),(18,18),(19,19),(19,20),(19,21),(22,22),(70,22),(23,23),(38,24),(52,25),(87,25),(71,26),(71,27),(85,27),(81,28),(96,29),(99,30),(107,30),(108,30),(109,30),(110,30),(103,31),(105,32),(106,32);
/*!40000 ALTER TABLE `evaluation_metric` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `evaluation_results`
--

DROP TABLE IF EXISTS `evaluation_results`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `evaluation_results` (
  `idevaluation_results` int(11) NOT NULL AUTO_INCREMENT,
  `id_settings` int(11) DEFAULT NULL,
  `id_metric` int(11) DEFAULT NULL,
  `result` float DEFAULT NULL,
  `type` varchar(45) DEFAULT NULL,
  `additional_info` varchar(45) DEFAULT NULL,
  `experiment_metric_id` int(11) DEFAULT NULL,
  PRIMARY KEY (`idevaluation_results`),
  KEY `id_settings_idx` (`id_settings`),
  KEY `id_metric_fk_idx` (`id_metric`),
  CONSTRAINT `id_metric_fk` FOREIGN KEY (`id_metric`) REFERENCES `comparison_technique` (`idcomparison_technique`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `id_settings_fk` FOREIGN KEY (`id_settings`) REFERENCES `evaluation_settings` (`idevaluation_settings`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=397 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `evaluation_results`
--

LOCK TABLES `evaluation_results` WRITE;
/*!40000 ALTER TABLE `evaluation_results` DISABLE KEYS */;
INSERT INTO `evaluation_results` VALUES (1,1,5,0.894,'mean','',1),(2,1,83,0.897,'mean','',2),(3,2,5,0.866,'mean','',1),(4,2,83,0.902,'mean','',2),(5,3,5,0.644,'mean','',1),(6,3,83,0.647,'mean','',2),(7,8,29,0.808,'mean','',3),(8,8,23,0.768,'mean','',4),(9,8,2,0.764,'mean','',5),(10,8,16,0.748,'mean','',6),(11,8,5,0.572,'mean','',1),(12,5,29,0.702,'mean','',3),(13,5,23,0.706,'mean','',4),(14,5,2,0.675,'mean','',5),(15,5,16,0.637,'mean','',6),(16,5,5,0.513,'mean','',1),(17,9,29,0.761,'mean','',3),(18,9,23,0.748,'mean','',4),(19,9,2,0.758,'mean','',5),(20,9,16,0.755,'mean','',6),(21,9,5,0.559,'mean','',1),(22,4,29,0.733,'mean','',3),(23,4,23,0.785,'mean','',4),(24,4,2,0.789,'mean','',5),(25,4,16,0.685,'mean','',6),(26,4,5,0.501,'mean','',1),(27,10,29,0.665,'mean','',3),(28,10,23,0.768,'mean','',4),(29,10,2,0.771,'mean','',5),(30,10,16,0.623,'mean','',6),(31,10,5,0.54,'mean','',1),(32,7,29,0.758,'mean','',3),(33,7,23,0.748,'mean','',4),(34,7,2,0.739,'mean','',5),(35,7,16,0.714,'mean','',6),(36,7,5,0.6,'mean','',1),(37,6,29,0.606,'mean','',3),(38,6,23,0.596,'mean','',4),(39,6,2,0.598,'mean','',5),(40,6,16,0.504,'mean','',6),(41,6,5,0.514,'mean','',1),(42,11,29,0.808,'mean','',3),(43,11,23,0.768,'mean','',4),(44,11,2,0.764,'mean','',5),(45,11,16,0.748,'mean','',6),(46,11,5,0.572,'mean','',1),(47,12,29,0.761,'mean','',3),(48,12,23,0.748,'mean','',4),(49,12,2,0.758,'mean','',5),(50,12,16,0.755,'mean','',6),(51,12,5,0.559,'mean','',1),(52,13,29,0.665,'mean','',3),(53,13,23,0.768,'mean','',4),(54,13,2,0.771,'mean','',5),(55,13,16,0.623,'mean','',6),(56,13,5,0.54,'mean','',1),(57,14,29,0.606,'mean','',3),(58,14,23,0.596,'mean','',4),(59,14,2,0.598,'mean','',5),(60,14,16,0.504,'mean','',6),(61,14,5,0.514,'mean','',1),(62,15,29,0.702,'mean','',3),(63,15,23,0.706,'mean','',4),(64,15,2,0.675,'mean','',5),(65,15,16,0.637,'mean','',6),(66,15,5,0.513,'mean','',1),(67,16,29,0.733,'mean','',3),(68,16,23,0.785,'mean','',4),(69,16,2,0.789,'mean','',5),(70,16,16,0.685,'mean','',6),(71,16,5,0.501,'mean','',1),(72,17,29,0.758,'mean','',3),(73,17,23,0.748,'mean','',4),(74,17,2,0.739,'mean','',5),(75,17,16,0.714,'mean','',6),(76,17,5,0.6,'mean','',1),(77,18,29,0.92,'mean','',3),(78,18,23,0.922,'mean','',4),(79,18,2,0.923,'mean','',5),(80,18,16,0.918,'mean','',6),(81,18,5,0.834,'mean','',1),(82,19,40,0.771,'mean','',7),(83,20,40,0.752,'mean','',7),(84,21,40,0.72,'mean','',7),(85,22,40,0.72,'mean','',7),(86,23,40,0.808,'mean','',7),(87,24,40,0.84,'mean','',7),(88,25,40,0.771,'mean','',7),(89,26,40,0.931,'mean','',7),(90,27,40,0.749,'mean','',7),(91,28,40,0.658,'mean','',7),(92,29,40,0.382,'mean','',7),(93,30,5,0.59,'mean','class level tests',1),(94,30,52,0.57,'mean','class level tests',8),(95,30,53,0.83,'mean','class level tests',9),(96,30,3,0.57,'mean','class level tests',10),(97,30,4,0.79,'mean','class level tests',11),(98,30,5,0.64,'mean','method level tests',1),(99,30,52,0.52,'mean','method level tests',8),(100,30,53,0.87,'mean','method level tests',9),(101,30,3,0.51,'mean','method level tests',10),(102,30,4,0.84,'mean','method level tests',11),(103,31,5,0.64,'mean','class level tests',1),(104,31,52,0.57,'mean','class level tests',8),(105,31,53,0.65,'mean','class level tests',9),(106,31,3,0.57,'mean','class level tests',10),(107,31,4,0.65,'mean','class level tests',11),(108,31,5,0.6,'mean','method level tests',1),(109,31,52,0.34,'mean','method level tests',8),(110,31,53,0.74,'mean','method level tests',9),(111,31,3,0.34,'mean','method level tests',10),(112,31,4,0.77,'mean','method level tests',11),(113,32,5,0.67,'mean','class level tests',1),(114,32,52,0.92,'mean','class level tests',8),(115,32,53,0.86,'mean','class level tests',9),(116,32,3,0.95,'mean','class level tests',10),(117,32,4,0.95,'mean','class level tests',11),(118,32,5,0.71,'mean','method level tests',1),(119,32,52,0.96,'mean','method level tests',8),(120,32,53,0.96,'mean','method level tests',9),(121,32,3,0.97,'mean','method level tests',10),(122,32,4,0.87,'mean','method level tests',11),(123,33,5,0.56,'mean','class level tests',1),(124,33,52,0.41,'mean','class level tests',8),(125,33,53,0.4,'mean','class level tests',9),(126,33,3,0.36,'mean','class level tests',10),(127,33,4,0.37,'mean','class level tests',11),(128,33,5,0.61,'mean','method level tests',1),(129,33,52,0.68,'mean','method level tests',8),(130,33,53,0.97,'mean','method level tests',9),(131,33,3,0.68,'mean','method level tests',10),(132,33,4,0.97,'mean','method level tests',11),(133,34,53,0.744,'mean','',9),(134,34,5,0.766,'mean','',1),(135,35,53,0.821,'mean','',9),(136,35,5,0.821,'mean','',1),(137,36,53,0.801,'mean','',9),(138,36,5,0.769,'mean','',1),(139,37,2,0.179,'mean','',5),(140,37,16,0.184,'mean','',6),(141,38,2,0.318,'mean','',5),(142,38,16,0.328,'mean','',6),(143,39,2,0.106,'mean','',5),(144,39,16,0.102,'mean','',6),(145,40,2,0.195,'mean','',5),(146,40,16,0.178,'mean','',6),(147,41,2,0.118,'mean','',5),(148,41,16,0.141,'mean','',6),(149,42,2,0.182,'mean','',5),(150,42,16,0.174,'mean','',6),(151,43,2,0.5,'mean','',5),(152,43,16,0.523,'mean','',6),(153,44,2,0.364,'mean','',5),(154,44,16,0.371,'mean','',6),(155,45,2,0.061,'mean','',5),(156,45,16,0.066,'mean','',6),(157,46,2,0.076,'mean','',5),(158,46,16,0.095,'mean','',6),(159,47,2,0.101,'mean','',5),(160,47,16,0.245,'mean','',6),(161,48,2,0.179,'mean','',5),(162,48,16,0.187,'mean','',6),(163,49,41,0.609,'mean','',12),(164,49,41,0.611,'median','',12),(165,49,53,0.742,'mean','',9),(166,49,53,0.741,'median','',9),(167,49,52,0.597,'mean','',8),(168,49,52,0.597,'median','',8),(169,49,5,0.708,'mean','',1),(170,49,5,0.706,'median','',1),(171,50,5,0.96,'mean','',1),(172,50,8,0.966,'mean','',13),(173,51,5,0.993,'mean','',1),(174,51,8,0.997,'mean','',13),(175,52,4,0.845,'mean','method level tests',11),(176,52,4,0.793,'mean','class level tests',11),(177,52,3,0.73,'mean','method level tests',10),(178,52,3,0.708,'mean','class level tests',10),(179,52,2,0.719,'mean','method level tests',5),(180,52,2,0.695,'mean','class level tests',5),(181,52,16,0.885,'mean','method level tests',6),(182,52,16,0.806,'mean','class level tests',6),(183,52,5,0.784,'mean','method level tests',1),(184,52,5,0.7,'mean','class level tests',1),(185,53,4,0.947,'mean','method level tests',11),(186,53,4,0.893,'mean','class level tests',11),(187,53,3,0.761,'mean','method level tests',10),(188,53,3,0.663,'mean','class level tests',10),(189,53,2,0.763,'mean','method level tests',5),(190,53,2,0.71,'mean','class level tests',5),(191,53,16,0.953,'mean','method level tests',6),(192,53,16,0.866,'mean','class level tests',6),(193,53,5,0.723,'mean','method level tests',1),(194,53,5,0.569,'mean','class level tests',1),(195,54,4,0.883,'mean','method level tests',11),(196,54,4,0.754,'mean','class level tests',11),(197,54,3,0.608,'mean','method level tests',10),(198,54,3,0.603,'mean','class level tests',10),(199,54,2,0.617,'mean','method level tests',5),(200,54,2,0.612,'mean','class level tests',5),(201,54,16,0.869,'mean','method level tests',6),(202,54,16,0.734,'mean','class level tests',6),(203,54,5,0.605,'mean','method level tests',1),(204,54,5,0.565,'mean','class level tests',1),(205,55,4,0.921,'mean','method level tests',11),(206,55,4,0.79,'mean','class level tests',11),(207,55,3,0.786,'mean','method level tests',10),(208,55,3,0.79,'mean','class level tests',10),(209,55,2,0.791,'mean','method level tests',5),(210,55,2,0.793,'mean','class level tests',5),(211,55,16,0.954,'mean','method level tests',6),(212,55,16,0.884,'mean','class level tests',6),(213,55,5,0.761,'mean','method level tests',1),(214,55,5,0.651,'mean','class level tests',1),(215,56,4,0.83,'mean','method level tests',11),(216,56,4,0.75,'mean','class level tests',11),(217,56,3,0.48,'mean','method level tests',10),(218,56,3,0.508,'mean','class level tests',10),(219,56,2,0.483,'mean','method level tests',5),(220,56,2,0.493,'mean','class level tests',5),(221,56,16,0.861,'mean','method level tests',6),(222,56,16,0.694,'mean','class level tests',6),(223,56,5,0.664,'mean','method level tests',1),(224,56,5,0.588,'mean','class level tests',1),(225,57,4,0.872,'mean','method level tests',11),(226,57,4,0.776,'mean','class level tests',11),(227,57,3,0.573,'mean','method level tests',10),(228,57,3,0.638,'mean','class level tests',10),(229,57,2,0.588,'mean','method level tests',5),(230,57,2,0.637,'mean','class level tests',5),(231,57,16,0.843,'mean','method level tests',6),(232,57,16,0.764,'mean','class level tests',6),(233,57,5,0.696,'mean','method level tests',1),(234,57,5,0.552,'mean','class level tests',1),(235,58,4,0.815,'mean','method level tests',11),(236,58,4,0.656,'mean','class level tests',11),(237,58,3,0.579,'mean','method level tests',10),(238,58,3,0.578,'mean','class level tests',10),(239,58,2,0.587,'mean','method level tests',5),(240,58,2,0.59,'mean','class level tests',5),(241,58,16,0.845,'mean','method level tests',6),(242,58,16,0.664,'mean','class level tests',6),(243,58,5,0.708,'mean','method level tests',1),(244,58,5,0.601,'mean','class level tests',1),(245,59,16,0.776,'mean','',6),(246,59,29,0.704,'mean','',3),(247,59,83,0.788,'mean','',2),(248,59,85,0.789,'mean','',14),(249,60,16,0.407,'mean','',6),(250,60,29,0.699,'mean','',3),(251,60,83,0.599,'mean','',2),(252,60,85,0.591,'mean','',14),(253,61,16,0.86,'mean','',6),(254,61,29,0.844,'mean','',3),(255,61,83,0.822,'mean','',2),(256,61,85,0.813,'mean','',14),(257,62,16,0.665,'mean','',6),(258,62,29,0.684,'mean','',3),(259,62,83,0.556,'mean','',2),(260,62,85,0.57,'mean','',14),(261,63,16,0.4484,'mean','',6),(262,63,29,0.416,'mean','',3),(263,63,83,0.52,'mean','',2),(264,63,85,0.504,'mean','',14),(265,65,5,0.73,'mean','',1),(266,65,5,0.74,'median','',1),(267,65,2,0.77,'raw','',5),(268,65,16,0.79,'raw','',6),(269,64,5,0.75,'mean','',1),(270,64,5,0.75,'median','',1),(271,64,2,0.6,'raw','',5),(272,64,16,0.6,'raw','',6),(273,67,5,0.52,'mean','',1),(274,67,5,0.51,'median','',1),(275,67,2,0.4,'raw','',5),(276,67,16,0.55,'raw','',6),(277,66,5,0.53,'mean','',1),(278,66,5,0.53,'median','',1),(279,66,2,0.56,'raw','',5),(280,66,16,0.59,'raw','',6),(281,68,5,0.63,'mean','',1),(282,68,5,0.63,'median','',1),(283,68,2,0.56,'raw','',5),(284,68,16,0.7,'raw','',6),(285,69,5,0.57,'mean','',1),(286,69,5,0.58,'median','',1),(287,69,2,0.58,'raw','',5),(288,69,16,0.53,'raw','',6),(289,70,5,0.57,'mean','',1),(290,70,5,0.57,'median','',1),(291,70,2,0.46,'raw','',5),(292,70,16,0.5,'raw','',6),(293,71,5,0.56,'mean','',1),(294,71,5,0.57,'median','',1),(295,71,2,0.5,'raw','',5),(296,71,16,0.54,'raw','',6),(297,72,5,0.63,'mean','',1),(298,72,5,0.63,'median','',1),(299,72,2,0.48,'raw','',5),(300,72,16,0.76,'raw','',6),(301,73,5,0.58,'mean','',1),(302,73,5,0.58,'median','',1),(303,73,2,0.52,'raw','',5),(304,73,16,0.58,'raw','',6),(305,74,5,0.53,'mean','',1),(306,74,5,0.53,'median','',1),(307,74,2,0.54,'raw','',5),(308,74,16,0.55,'raw','',6),(309,75,5,0.52,'mean','',1),(310,75,5,0.52,'median','',1),(311,75,2,0.59,'raw','',5),(312,75,16,0.52,'raw','',6),(313,76,5,0.51,'mean','',1),(314,76,5,0.51,'median','',1),(315,76,2,0.48,'raw','',5),(316,76,16,0.49,'raw','',6),(317,77,5,0.56,'mean','',1),(318,77,5,0.56,'median','',1),(319,77,2,0.52,'raw','',5),(320,77,16,0.75,'raw','',6),(321,78,5,0.49,'mean','',1),(322,78,5,0.5,'median','',1),(323,78,2,0.55,'raw','',5),(324,78,16,0.49,'raw','',6),(325,79,5,0.5,'mean','',1),(326,79,5,0.5,'median','',1),(327,79,2,0.52,'raw','',5),(328,79,16,0.51,'raw','',6),(329,80,5,0.54,'mean','',1),(330,80,5,0.55,'median','',1),(331,80,2,0.66,'raw','',5),(332,80,16,0.52,'raw','',6),(333,81,5,0.52,'mean','',1),(334,81,5,0.52,'median','',1),(335,81,2,0.54,'raw','',5),(336,81,16,0.63,'raw','',6),(337,82,5,0.58,'mean','',1),(338,82,5,0.58,'median','',1),(339,82,2,0.54,'raw','',5),(340,82,16,0.54,'raw','',6),(341,83,5,0.57,'mean','',1),(342,83,5,0.56,'median','',1),(343,83,2,0.43,'raw','',5),(344,83,16,0.43,'raw','',6),(345,84,5,0.55,'mean','',1),(346,84,5,0.56,'median','',1),(347,84,2,0.52,'raw','',5),(348,84,16,0.63,'raw','',6),(349,85,4,0.842,'mean','',11),(350,85,3,0.792,'mean','',10),(351,85,150,0.818,'mean','',15),(352,85,151,0.833,'mean','',16),(353,86,4,0.624,'mean','',11),(354,86,3,0.6,'mean','',10),(355,86,150,0.763,'mean','',15),(356,86,151,0.868,'mean','',16),(357,87,4,0.687,'mean','',11),(358,87,3,0.601,'mean','',10),(359,87,150,0.633,'mean','',15),(360,87,151,0.68,'mean','',16),(361,88,4,0.914,'mean','',11),(362,88,3,0.729,'mean','',10),(363,88,150,0.613,'mean','',15),(364,88,151,0.636,'mean','',16),(365,89,4,0.806,'mean','',11),(366,89,3,0.8,'mean','',10),(367,89,150,0.711,'mean','',15),(368,89,151,0.845,'mean','',16),(369,90,4,0.916,'mean','',11),(370,90,3,0.896,'mean','',10),(371,90,150,0.517,'mean','',15),(372,90,151,0.57,'mean','',16),(373,85,52,0.792,'mean','',8),(374,85,53,0.853,'mean','',9),(375,85,152,0.819,'mean','',17),(376,85,153,0.834,'mean','',18),(377,86,52,0.465,'mean','',8),(378,86,53,0.755,'mean','',9),(379,86,152,0.768,'mean','',17),(380,86,153,0.94,'mean','',18),(381,87,52,0.605,'mean','',8),(382,87,53,0.688,'mean','',9),(383,87,152,0.635,'mean','',17),(384,87,153,0.685,'mean','',18),(385,88,52,0.724,'mean','',8),(386,88,53,0.912,'mean','',9),(387,88,152,0.619,'mean','',17),(388,88,153,0.639,'mean','',18),(389,89,52,0.802,'mean','',8),(390,89,53,0.945,'mean','',9),(391,89,152,0.705,'mean','',17),(392,89,153,0.84,'mean','',18),(393,90,52,0.896,'mean','',8),(394,90,53,0.927,'mean','',9),(395,90,152,0.504,'mean','',17),(396,90,153,0.565,'mean','',18);
/*!40000 ALTER TABLE `evaluation_results` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `evaluation_settings`
--

DROP TABLE IF EXISTS `evaluation_settings`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `evaluation_settings` (
  `idevaluation_settings` int(11) NOT NULL AUTO_INCREMENT,
  `study_id` int(11) DEFAULT NULL,
  `evaluation_id` int(11) DEFAULT NULL,
  `object_name` varchar(45) DEFAULT NULL,
  `object_source` varchar(150) DEFAULT NULL,
  `object_language` varchar(45) DEFAULT NULL,
  `object_versions` int(11) DEFAULT NULL,
  `object_loc` int(11) DEFAULT NULL,
  `object_suite_size` int(11) DEFAULT NULL,
  `object_suite_type` varchar(45) DEFAULT NULL,
  `object_faults` int(11) DEFAULT NULL,
  `object_faults_type` varchar(45) DEFAULT NULL,
  `object_suite_source` varchar(45) DEFAULT NULL,
  `additional_info` varchar(600) DEFAULT NULL,
  PRIMARY KEY (`idevaluation_settings`),
  KEY `study_id_fk_idx` (`study_id`),
  KEY `evaluation_id_fk_idx` (`evaluation_id`),
  CONSTRAINT `evaluation_id_fk` FOREIGN KEY (`evaluation_id`) REFERENCES `evaluation` (`idevaluation`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `study_id_fk` FOREIGN KEY (`study_id`) REFERENCES `study` (`idStudy`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=91 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `evaluation_settings`
--

LOCK TABLES `evaluation_settings` WRITE;
/*!40000 ALTER TABLE `evaluation_settings` DISABLE KEYS */;
INSERT INTO `evaluation_settings` VALUES (1,8,10,'XML-Security','SIR','Java',1,18237,95,'',912,'seeded (mutation)','provided',''),(2,8,10,'Jdepend','Official website','Java',1,2459,47,'',462,'seeded (mutation)','provided',''),(3,8,10,'Checkstyle','Official website','Java',1,43407,164,'',6874,'seeded (mutation)','provided',''),(4,38,39,'tcas','Siemens','C',41,138,6,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(5,38,39,'schedule2','Siemens','C',10,297,8,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(6,38,39,'schedule','Siemens','C',9,299,8,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(7,38,39,'tot_info','Siemens','C',23,346,7,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(8,38,39,'print_tokens','Siemens','C',7,402,16,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(9,38,39,'print_tokens2','Siemens','C',10,483,12,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(10,38,39,'replace','Siemens','C',32,516,19,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(11,40,41,'print_tokens','Siemens','C',7,402,16,'TSL',1,'provided (manual seeded)','generated','Journal extension of the previous study (39)'),(12,40,41,'print_tokens2','Siemens','C',10,483,12,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(13,40,41,'replace','Siemens','C',32,516,19,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(14,40,41,'schedule','Siemens','C',9,299,8,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(15,40,41,'schedule2','Siemens','C',10,297,8,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(16,40,41,'tcas','Siemens','C',41,138,6,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(17,40,41,'tot_info','Siemens','C',23,346,7,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(18,40,41,'space','European Space Agency','C',35,6218,155,'TSL',1,'provided (real)','generated','33 versions have only 1 fault each, 2 additional versions have 5 faults each'),(19,41,43,'QTB','Industrial Partner','C',6,300000,135,'',4,'provided (real)','provided','different number of faults per version'),(20,41,43,'print_tokens2','Siemens','C',10,483,12,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(21,41,43,'schedule2','Siemens','C',10,297,8,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(22,41,43,'schedule','Siemens','C',9,299,8,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(23,41,43,'replace','Siemens','C',32,516,19,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(24,41,43,'tcas','Siemens','C',41,138,6,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(25,41,43,'print_tokens','Siemens','C',7,402,16,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(26,41,43,'space','European Space Agency','C',35,6218,155,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(27,41,43,'tot_info','Siemens','C',23,346,7,'TSL',1,'provided (manual seeded)','generated','1 fault per version'),(28,41,43,'flex','Unix','C',5,9153,525,'TSL',18,'provided (manual seeded)','generated',''),(29,41,43,'grep','Unix','C',5,7451,613,'TSL',11,'provided (manual seeded)','generated',''),(30,48,50,'ant','SIR','Java',9,80400,877,'JUnit',21,'seeded (manual)','generated','Authors didn\'t state the source of test cases (whether it was generated by researchers or provided by the developers of the application) , so we assume that they were generated by researchers in the SIR infrastructure project.'),(31,48,50,'jmeter','SIR','Java',6,43400,78,'JUnit',9,'seeded (manual)','generated',''),(32,48,50,'xml-security','SIR','Java',4,16300,83,'JUnit',6,'seeded (manual)','generated',''),(33,48,50,'jtopas','SIR','Java',4,5400,128,'JUnit',5,'seeded (manual)','generated',''),(34,52,54,'Altitude Switch (ASW)','University of Minnesota','Java',1,336,30,'JUnit',15,'seeded (mutation)','generated',''),(35,52,54,'Wheel Brake System (WBS)','University of Minnesota','Java',1,206,30,'JUnit',15,'seeded (mutation)','generated',''),(36,52,54,'Flight Guidance System (FGS)','University of Minnesota','Java',1,1237,30,'JUnit',15,'seeded (mutation)','generated',''),(37,54,56,'print_tokens','Siemens','C',10,402,16,'TSL',0,'seeded (manual, random)','generated','Authors only state that they use multiple faults versions, but doesn\'t specify the exact amount.'),(38,54,56,'print_tokens2','Siemens','C',10,483,12,'TSL',0,'seeded (manual, random)','generated',''),(39,54,56,'replace','Siemens','C',10,516,19,'TSL',0,'seeded (manual, random)','generated',''),(40,54,56,'schedule','Siemens','C',10,299,8,'TSL',0,'seeded (manual, random)','generated',''),(41,54,56,'schedule2','Siemens','C',10,297,8,'TSL',0,'seeded (manual, random)','generated',''),(42,54,56,'tcas','Siemens','C',10,138,6,'TSL',0,'seeded (manual, random)','generated',''),(43,54,56,'tot_info','Siemens','C',10,346,7,'TSL',0,'seeded (manual, random)','generated',''),(44,54,56,'space','European Space Agency','C',10,6218,155,'TSL',0,'seeded (manual, random)','generated',''),(45,54,56,'ant','SIR','Java',10,80400,877,'JUnit',0,'seeded (manual, random)','generated',''),(46,54,56,'jmeter','SIR','Java',10,43400,78,'JUnit',0,'seeded (manual, random)','generated',''),(47,54,56,'jtopas','SIR','Java',10,5400,128,'JUnit',0,'seeded (manual, random)','generated',''),(48,54,56,'xml-security','SIR','Java',10,16300,83,'JUnit',0,'seeded (manual, random)','generated',''),(49,55,57,'NoiseGen','Industrial Partner','C/C++',4,70000,600,'TSL',9,'provided (real)','generated',''),(50,80,83,'replace','Siemens','C',28,512,5542,'TSL',0,'provided (real)','generated',''),(51,80,83,'space','European Space Agency','C',34,6199,13551,'TSL',0,'provided (real)','generated',''),(52,85,88,'jtopas','SIR','Java',3,5000,150,'JUnit',1800,'seeded (mutation)','generated',''),(53,85,88,'XML-Security','SIR','Java',3,18000,90,'JUnit',2600,'seeded (mutation)','generated',''),(54,85,88,'jmeter','SIR','Java',5,36000,80,'JUnit',3800,'seeded (mutation)','generated',''),(55,85,88,'ant','SIR','Java',8,60000,550,'JUnit',26000,'seeded (mutation)','generated',''),(56,85,88,'jtopas','SIR','Java',3,5000,150,'JUnit',12,'provided (manual seeded)','generated',''),(57,85,88,'jmeter','SIR','Java',5,36000,80,'JUnit',16,'provided (manual seeded)','generated',''),(58,85,88,'ant','SIR','Java',8,60000,550,'JUnit',14,'provided (manual seeded)','generated',''),(59,88,91,'Jdepend','Official website','Java',1,2459,47,'JUnit',1,'seeded (mutation)','provided','They used multiple single-fault versions of the softwares'),(60,88,91,'jtopas','SIR','Java',1,5297,47,'JUnit',1,'seeded (mutation)','generated','They used multiple single-fault versions of the softwares'),(61,88,91,'XML-Security','SIR','Java',1,18237,95,'JUnit',1,'seeded (mutation)','generated','They used multiple single-fault versions of the softwares'),(62,88,91,'Checkstyle','Official website','Java',1,43407,164,'JUnit',1,'seeded (mutation)','provided','They used multiple single-fault versions of the softwares'),(63,88,91,'Ant','SIR','Java',1,75429,101,'JUnit',1,'seeded (mutation)','generated','They used multiple single-fault versions of the softwares'),(64,89,92,'LaTazza','Official website','Java',1,2000,33,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(65,89,92,'AveCalc','Official website','Java',1,2000,47,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(66,89,92,'CommonsProxy','Official website','Java',1,5000,179,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(67,89,92,'DBUtils','Official website','Java',1,5000,225,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(68,89,92,'iTrust','Official website','Java',1,15000,919,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(69,89,92,'CommonsCodec','Official website','Java',1,17000,608,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(70,89,92,'Jtidy','Official website','Java',1,20000,289,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(71,89,92,'Woden','Official website','Java',1,22000,263,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(72,89,92,'Log4J','Official website','Java',1,25000,1029,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(73,89,92,'Betwixt','Official website','Java',1,25000,325,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(74,89,92,'JXPath','Official website','Java',1,25000,386,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(75,89,92,'CommonsIO','Official website','Java',1,25000,859,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(76,89,92,'CommonsBcel','Official website','Java',1,30000,75,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(77,89,92,'CommonsBeanUtils','Official website','Java',1,32000,1556,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(78,89,92,'XMLGraphics','Official website','Java',1,34000,196,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(79,89,92,'XML-Security','Official website','Java',1,40000,92,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(80,89,92,'CommonsCollections','Official website','Java',1,50000,798,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(81,89,92,'Pmd','Official website','Java',1,55000,698,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(82,89,92,'CommonsLang','Official website','Java',1,60000,2307,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(83,89,92,'Jabref','Official website','Java',1,70000,213,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(84,89,92,'Xerces','Official website','Java',1,138000,376,'JUnit',1,'seeded (manual)','provided','They used multiple single-fault versions of the softwares'),(85,90,93,'Ant','SIR','Java',9,80400,877,'JUnit',412,'seeded (mutation)','generated',''),(86,90,93,'Galileo','SIR','Java',16,15200,912,'TSL',2494,'seeded (mutation)','generated',''),(87,90,93,'Jmeter','SIR','Java',6,43300,78,'JUnit',386,'seeded (mutation)','generated',''),(88,90,93,'Jtopas','SIR','Java',4,5400,128,'JUnit',1104,'seeded (mutation)','generated',''),(89,90,93,'NanoXML','SIR','Java',6,7600,216,'TSL',204,'seeded (mutation)','generated',''),(90,90,93,'xml-security','SIR','Java',4,16300,83,'JUnit',246,'seeded (mutation)','generated','');
/*!40000 ALTER TABLE `evaluation_settings` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `faults_type`
--

DROP TABLE IF EXISTS `faults_type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `faults_type` (
  `idfaults_type` int(11) NOT NULL AUTO_INCREMENT,
  `details` varchar(150) DEFAULT NULL,
  PRIMARY KEY (`idfaults_type`)
) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `faults_type`
--

LOCK TABLES `faults_type` WRITE;
/*!40000 ALTER TABLE `faults_type` DISABLE KEYS */;
INSERT INTO `faults_type` VALUES (1,'Authors manually seeded faults into the software'),(2,'Authors seeded faults through mutation into the software'),(3,'Authors seeded faults into the software, didn\'t provide further details'),(4,'Faults provided with the dataset, they were manually seeded into the software'),(5,'Faults provided with the dataset, they were seeded into the software using mutation testing'),(6,'Faults provided with the dataset, didn\'t include further details'),(7,'Real faults of the software');
/*!40000 ALTER TABLE `faults_type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `granularity`
--

DROP TABLE IF EXISTS `granularity`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `granularity` (
  `idgranularity` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(90) DEFAULT NULL,
  PRIMARY KEY (`idgranularity`)
) ENGINE=InnoDB AUTO_INCREMENT=14 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `granularity`
--

LOCK TABLES `granularity` WRITE;
/*!40000 ALTER TABLE `granularity` DISABLE KEYS */;
INSERT INTO `granularity` VALUES (1,'method'),(2,'statement'),(3,'function'),(4,'class'),(5,'any'),(6,'test case'),(7,'branch'),(8,'block'),(9,'block of binary form'),(10,'line'),(11,'not stated'),(12,'requirement property'),(13,'requirement');
/*!40000 ALTER TABLE `granularity` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `input_method`
--

DROP TABLE IF EXISTS `input_method`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `input_method` (
  `idinput_method` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`idinput_method`)
) ENGINE=InnoDB AUTO_INCREMENT=16 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `input_method`
--

LOCK TABLES `input_method` WRITE;
/*!40000 ALTER TABLE `input_method` DISABLE KEYS */;
INSERT INTO `input_method` VALUES (1,'source code'),(2,'binary form'),(3,'call graph for program structure'),(4,'requirements/specifications'),(5,'system'),(6,'fault matrix'),(7,'time budget'),(8,'test suite'),(9,'coverage information'),(10,'source code change information'),(11,'test input data'),(12,'test execution history'),(13,'execution trace'),(14,'software metrics'),(15,'system model');
/*!40000 ALTER TABLE `input_method` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `prioritization_metric`
--

DROP TABLE IF EXISTS `prioritization_metric`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `prioritization_metric` (
  `idprioritization_metric` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(100) DEFAULT NULL,
  `description` varchar(500) DEFAULT NULL,
  PRIMARY KEY (`idprioritization_metric`)
) ENGINE=InnoDB AUTO_INCREMENT=33 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `prioritization_metric`
--

LOCK TABLES `prioritization_metric` WRITE;
/*!40000 ALTER TABLE `prioritization_metric` DISABLE KEYS */;
INSERT INTO `prioritization_metric` VALUES (1,'APSC','Average Percentage of Statement Coverage'),(2,'APDC','Average Percentage of Decision Coverage'),(3,'APBC','Average Percentage of Block Coverage'),(4,'APFD','Average Percentage of Faults Detected'),(5,'f-measure','Number of distinc test cases to be run for causing the first failure'),(6,'f-spreading','measures how the failing test cases are spread in a prioritized test suite'),(7,'Inclusiveness','Measures the degree to which an algorithm chooses test cases that can\ncause the modified program to produce different output'),(8,'Precision','measures the ability of a technique to avoid choosing a test that is not modification-revealing test'),(9,'APxC','Average Percentage of X Coverage, where X is the granularity. Eg: method or statement'),(10,'APSCta','average percentage statement coverage, time-aware: measures the rate at which a prioritized test suite cover statements'),(11,'APFDta','average percentage of faults detected, time-aware: measures the rate at which a prioritized test suite detect faults'),(12,'APFDc','Average percentage of faults detected considering test case cost information'),(13,'Total faults exposed (TFE)','total number of faults exposed by prioritized test suite.'),(14,'Distinct faults detected (DFD)','represents the distinct faults detected by prioritized test suite'),(15,'Effective time for execution of test cases (ET)','total time taken to execute all test cases of prioritized test suite'),(16,'Percentage of test suite failure (TSF)','percentage of faults not detected by the prioritized test suite'),(17,'Percentage of time wastage (TW)','represents the percentage of total time wasted in detecting repetitive faults.'),(18,'Percentage of time saving (TS)','detection of all faults before given time limit has been considered as time saving'),(19,'APFD sub1','considers only severe faults'),(20,'APFD sub2','considers faults related to relevant requirements'),(21,'TOD (Test Ordering Diversity)','measures the amount of different between two strings (two orderings)'),(22,'NAPFD','Normalized APFD. Considers only executed test cases'),(23,'APCC','(Average percentage code coverage) measures the degree at which a prioritized test suite covers the conditions of a program'),(24,'PTR','percentage of test suite required for complete fault coverage. Measures how much of the test suite is executed before detecting all its faults. '),(25,'Relative cost-benefit value ','positive values indicate that the technique is beneficial compared to another technique, negative indicate otherwise'),(26,'APCC','Average Percentage of Change Coverage. Measures how fast the test suite covers changed code.'),(27,'EET','Effective Execution Time. Measures execution time required for the test sequence to achieve 100% of the test objectives.'),(28,'CE','Coverage Effectiveness (Determine the cumulative coverage of tests over time)'),(29,'CI (Convergence index)','Is the ratio between the percentage of faults detected over the percentage of tests executed'),(30,'Relative Position','measures the average effectiveness of early fault detection'),(31,'WPFD','weighted percentage of failures detected'),(32,'TPFD','Total percentage of failures detected');
/*!40000 ALTER TABLE `prioritization_metric` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `proposed_technique`
--

DROP TABLE IF EXISTS `proposed_technique`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `proposed_technique` (
  `idtechnique` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(100) DEFAULT NULL,
  `description` varchar(999) DEFAULT NULL,
  `language_type` varchar(99) DEFAULT NULL,
  PRIMARY KEY (`idtechnique`)
) ENGINE=InnoDB AUTO_INCREMENT=81 DEFAULT CHARSET=utf8 COMMENT='proposed techniques';
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `proposed_technique`
--

LOCK TABLES `proposed_technique` WRITE;
/*!40000 ALTER TABLE `proposed_technique` DISABLE KEYS */;
INSERT INTO `proposed_technique` VALUES (1,'Immune Genetic Algorithm','Uses Immune Genetic Algorithm (IGA) with an Immune Prioritization Algorithm (IPA) which employs a vaccine selection function to select vaccines for every chromosome. The purpose of using immune concepts in genetic algorithms is to make use of local characteristic to arrive at an optimal solution. In this case, test cases which have best coverage (local characteristic) are prioritized','language independent'),(2,'Refactor-based approach (RBA)','Refactor-based approach (RBA) as the name states, is based on code-refactoring. It works by identifying refactoring edits between two versions of a software, discovering impacted elements by these changes, calculating impact values and prioritizing the test cases according to these values. The impact values are calculated from the call graph of each test case.','object oriented'),(3,'ComboRT','The approach combines test case selection and prioritization to directly generate a ranked list of test cases for regression testing.\nTests are ordered according to an impact factor of the changes, calculated by a method named FCA-CIA.','object oriented'),(4,'Clustering-based approach','Test cases are clustered based on their line, method and branch coverage. The algorithm used for clustering is Fuzzy C-Means. After clustering the test cases, an intra-cluster procedure is performed. Each cluster is subdivided considering code coverage, test-case failure rate, fault detection ratio, execution time and code complexity.','language independent'),(5,'Clustering and change based approach','The proposed technique is based on clustering and specific change location. It clusters test cases based on their similarities. Then, clusters are prioritized based on specific change location. Finally, each cluster has its test cases ranked according to code complexity, code coverage and requirements complexity.','language independent'),(6,'optimal coverage-based test-case prioritization with integer linear programming','This technique is based on maximizing an intermediary goal (coverage) instead of an ultimate goal (fault coverage). It is used an integer linear programming model to achieve this.','language independent'),(7,'Ordered-sequence prioritization','This technique is based on similarity measures to order the test cases. For example, the first selected test is the one that covers most part of the source code. Then, the next one is the farthest away from the first selected, using a distance measure called edit distance. ','language independent'),(8,'Cost-cognizant test case prioritization','Uses historical information combined with genetic algorithm to prioritize test cases. Historical information includes test cost, detected faults and fault severities. After each execution of the test cases, their information are stored in a Historical Information Repository. The goal of this technique is to consider the cost of each test case when creating the ordered sequence to be executed.','language independent'),(9,'Hierarchical test case prioritization technique','The authors propose a prioritization approach based on hierarchical characteristics of an object-oriented program. The prioritization is executed in two levels. First, classes of the project are prioritized according to their number of descendants, number of inherited attributes/methods and level of class in inheritance hierarchy. On the second level, test cases related to the prioritized classes are ordered taking into account fault criticality and coverage. The problem is that the description is not clear on how the technique works on the second level.','object oriented'),(10,'genetic prioritization approach','The technique is based on testing time and potential code coverage applied on a genetic algorithm. Also, the technique is applied on test case identified as ‚Äúsevere‚Äù by customer requirements. The suite is then ordered by prioritizing those tests that takes less time to execute and cover more code.','language independent'),(11,'Hamming distance based','The proposed approach aims at achieving maximum fault coverage at the earliest as well as to reduce time and cost of executing the test suite. To ensure that all faults are covered by the test cases selected in the ordering (faults needs to be known), mutation test is used. Faults exposed by each test case are represented as a binary string, where each position represents each fault. The similarity between two test cases is measured by the distance between two binary strings using Hamming distance. A test case with high hamming distance is ordered first until all faults are covered.','language independent'),(12,'Modular based test case prioritization','In the proposed approach, test cases are prioritized in two levels. First at module level, then at global level. At first level, test cases are ordered such as they cover the maximum amount of faults (faults need to be known). This is repeated until all faults are covered. In the second level, two set of test cases are considered, 1. The modular list obtained in the first level, 2. Common test cases (test cases common to all modules). Then the approach is applied again, to order such as the maximum amount of faults are covered. ','object oriented'),(13,'Ordersequence (ant colony optimization)','The algorithm starts with a complete graph representing all the test cases as vertices. An ant is positioned at each vertex. At first interaction, each ant searches the best path by adding new edges to its path. The best path is selected according to the amount of pheromone it contains. If all the edges have the same amount of pheromone value, then a random edge is selected. After each ant selects a test case, a path is selected which have the minimum cost, whereas the cost is the execution time of the test cases contained in it. Then all the edges cost are updated. 1 is added to all the edges and 10% is taken from the value of the best path. This process continue until no update in path is possible.','object oriented'),(14,'PTCP (pair-wise test case prioritization)','The technique is based on number of faults detected, time of execution and number of repetitive faults detected. It has as input a test suite, time budget, set of faults not yet detected and set of faults detected after execution of each test case and has as output a prioritized list of test cases and set of faults detected. ','language independent'),(15,'Multi-objective Test case Prioritization (MOTCP)','The proposed technique rely on information about code coverage, requirement coverage and execution cost. These three information are plotted on a Cartesian plan for different orderings of test cases. The area of the curves obtained are used to define the order of the test cases. The greater the area, the better an execution order is. As the evaluation of all possible orderings is expensive, a genetic algorithm is used to select the best ones.','object oriented'),(16,'adapted failure pursuit sampling','The technique is based on clustering of similar test cases during regression testing execution. Test cases are clustered based on tests profile and execution behavior extracted from previous runs. A special cluster is created, which contains modified and adapted test cases, identified during functional test phase. An amount of test case are selected from this special cluster and executed. If any of them fail, the technique select again k nearest neighbors to them, otherwise, it select tests from others clusters. This is repeated until all test cases are run.','language independent'),(17,'multi-objective test case prioritization','Considers five dimensions to give a value to each test case and then sort the test suite according to this value. The dimensions are coverage, ratio of fault exposing, requirement property relevance, history information and time spending.\nFor coverage, it uses statement coverage amount. \nFor Fault exposure potential, mutation analysis is performed. \nRequirement property relevance is calculated by checking how many requirements properties each test case satisfies.\nFor history information, a formula is used to calculate a value for each of the previous runs.\nFinally, execution time is also taken into account.\nEach test case has its value calculated for each of the five dimensions and then has its value normalized for one dimension.\n','language independent'),(18,'Global similarity-based test case prioritization algorithm (GSTCP)','Branches covered by a test case can be represented as a binary branch coverage vector, where Vi is 0 if the ith branch is covered, otherwise 1. Similarly, the vector can be implemented using numeric entries, representing the number of times that ith branch is executed. In this way, similarity measures can be used to order the test cases. Tested measures are cosine similarity, Euclidean distance, proportional distance, jaccard index and its variants. The prioritization algorithm seeks test cases that has the maximum distance from the last test added to the prioritized set.','language independent'),(19,'Genetic (total coverage) algorithm','The approach is a mix of test case generation and prioritization. It uses genetic algorithm to generate a test suite to achieve total conditions coverage. Then a fitness function using number of branches, statement and methods covered by each test is used to rank the test cases and order them.','language independent'),(20,'Clustering ‚Äì BN based approach (CBN)','The technique is based on previous work on prioritization using Bayesian Networks (BN). In this new approach, a clustering of the test cases is performed before using the BN, based on a method-level coverage matrix. The BN uses information about software quality metrics (afferent coupling and coupling between object classes), source code change information and class-level coverage-matrix to give a failure probability for each test case. This probability is used to order each the tests cases within each cluster, which are then sorted in descending order of failure probability. Using a round robin method, a test case from each cluster is selected to be added to the prioritized set of test cases. This is repeated until all test cases are ordered.','object oriented'),(21,'Ant Colony Optimization based','The technique is based on Ant Colony Optimization algorithm. It uses as input data number of faults detected by a test case, test case execution time and fault severity.','language independent'),(22,'IRCOV prioritization','The technique is based on information retrieval concepts applied to coverage data of test cases. The concepts used are term-frequency (TF) and Inverted document frequency (IDF). TF represents how often a term appears in a document and IDF represents how unique a term is in a document. In the TCP context, a document is viewed as a test case and the ‚Äúwords‚Äù of a document as elements covered by each test case. Each covered element has a TF and IDF scores, indicating how important the element is to the test case. A similarity measure is used based on TF-IDF scores and indicates how much a test case is related to modifications in the program (SUT). A linear regression model is used to automatically set weights on the coverage and similarity scores of test cases. It adjusts the weights of each feature such that a feature having little correlation with faults in a program has little weight. The linear model produces an estimated number of faults that each test case can detect. This number is','language independent'),(23,'Echelon modification based algorithm','The proposed approach is based on modification and coverage at binary level. In the first step, it uses a matching block algorithm in the old binary (previous version) for each block in the new binary (current version) to check for new and modified source code. In the second step, it uses coverage information to check if the modified and new blocks are covered by existing tests. This gives the set of new and modified blocks covered by each test, using this information for the third step that will find a sequence of tests that cover as many blocks as possible from the new and modified blocks set. A weight is given for each test, according to its coverage. Thus, tests are ordered according to their number of covered blocks. The coverage is recalculated after every reordering, to ensure that a feedback system is used.','binary code'),(24,'All-DU criterion based prioritization','The approach uses a metric called adequacy, which is denoted by the ratio between the number of covered test requirements and total number of test requirements. These requirements are derived from the all-DU (def-use) criterion, which is related to the data-flow of a program. The test cases are ordered according to the adequacy metric.','not defined'),(25,'Historical performance data based prioritization','The technique is based on historical execution of the test cases. A proportion of number of times a test case has revealed a fault over the total amount of times it has been executed is used as a prioritization factor. Coverage is also taken into account when calculating the prioritization factor. ','language independent'),(26,'Fault based prioritization','The technique uses two parameters to calculate the ordering of test case. One is Fault Rate, which is given by total number of faults revealed by a test case over the total execution time of that same test case. The other is Fault Severity, which values the severity of each value in a scale from 2 to 32. The weighted final priority is given by Fault rate * Fault severity for each test case.','language independent'),(27,'Total TA based prioritization (JuptaT)','This technique is similar to the total coverage prioritization approach, except that it doesn‚Äôt use coverage information. Instead, it statically analyze the source code to infer the test ability (TA) of each test case. This test ability is based on the amount of methods in the source code that a test case invokes and is derived from the program static call graph. Each iteration, the test cases with the highest TA are selected to compose the prioritized set of test cases, until all test cases are selected.','object oriented'),(28,'Additional TA based prioritization (JuptaA)','This technique is similar to the additional coverage prioritization approach, except that it doesn‚Äôt use coverage information. Instead, it statically analyze the source code to infer the test ability (TA) of each test case. This test ability is based on the amount of methods in the source code that a test case invokes and is derived from the program static call graph. Differently from the Total TA based prioritization, each iteration the test case with highest TA is selected to compose the prioritized set of test cases and all TA values of the remaining test cases are updated. The process is repeated until all test cases are selected.','object oriented'),(29,'Practical Priority Factors based prioritization','The technique is based on 10 different factors based on time, defect, requirement and complexity. \nTime: execution time ‚Äì it can used a scale of high, medium, low ranges to the execution time of each test case; Validation time ‚Äì is the total time required for validating the expected result and actual result.\nDefect: defect occurrence ‚Äì as the name imply, means the occurrence of a defect on the execution of a test case; defect impact ‚Äì classify seriousness of defects using historical information about defects.\nRequirement: customer assigned priority ‚Äì measures the importance of a specific requirement; Implementation complexity ‚Äì measures the complexity of implementing the requirement; Requirement change ‚Äì based on the total number of times that a requirement has been changed; Requirement coverage ‚Äì measures the number of requirements covered by each test case.\nComplexity: test case complexity ‚Äì measures the difficulty and complexity while running a test case; test impact ‚Äì uses a weigh','language independent'),(30,'Additional branch coverage ART (Add ART)','Hybrid between additional branch coverage and ART with Manhattan distance measure. The first iteration of the additional branch coverage is executed and in the next iterations ART is used instead.','not defined'),(31,'ART and Additional Branch Coverage (ART Add)','Hybrid between ART and additional branch coverage. It executes the ART normally, but instead of using Manhattan distance as distance measure, it uses the additional branch coverage.','not defined'),(32,'Additional-statement-on-fault-severity approach','The technique is similar to additional statement coverage technique. Instead of using coverage as criteria, it uses fault severity that each test case can detect. Severities are divided into fatal fault, serious fault, general fault and minor fault. Each degree has an associated value which is calculated for each test case. ','language independent'),(33,'GASA (Genetic Algorithm and Simulated Annealing)','The technique combines genetic algorithm and simulated annealing. It uses the total execution time to cover all faults as fitness function.','language independent'),(34,'Time-aware genetic algorithm','The technique uses genetic algorithm to prioritize the test cases using time constraints. The genetic algorithm has as input the execution time of each test case, the program under test and some tunning parameters and the time-constraint. The fitness function is based on the percentage of code covered in P by a given set of test cases and the time for its execution. The code coverage used are at block or method level.','object oriented'),(35,'Bayesian network based','The technique uses Bayesian network to estimate the probability of a test case to be ordered first. The approach extract evidences from source code and feed this information into a Bayesian network model. As evidences, it uses software quality metrics, test coverage measures and change analysis data.','object oriented'),(36,'2-Optimal algorithm','This technique is an instantiation of the K-Optimal greedy approach. For each iteration, it selects the 2 test cases that ‚Äúconsumes‚Äù the largest part of the problem. In this case, coverage information can be used. Thus, it selects a pair of test cases that cover the most of the program each iteration and then update the coverage information.','language independent'),(37,'Hill Climbing','Hill Climbing with steepest ascent is used. It picks a random solution (order of test cases) and evaluate its neighbors (different orders when changing one test case of position). Then it moves to the solution with the best fitness (can be coverage). This is repeated until no move can be made.','language independent'),(38,'ART coverage based prioritization','The technique select test cases in a controlled random way, meaning that at each iteration, a test case that is the farthest away from all the already selected test case is selected. The ‚Äúfarthest away‚Äù candidate is selected by using a function that calculates distance between a pair of test cases, in this case, the function used is Jaccard distance. The information used in the function is the test case coverage, which can be at different granularity levels (statement, function and branch).','language independent'),(39,'Oracle-centric prioritization','The technique is based on dataflow analysis in which the definitions and uses of variables are tracked during system execution. They developed a metric that measures how well each variable in a program is checked by a test oracle. This metric is used to order the test cases. Thus, a new version of the program uses information from previous version.','language independent'),(40,'Raptor (gReedy diAgnostic Prioritization by ambiguiTy grOup Reduction)','The technique is based on a metric of ambiguity reduction with the aim of facilitating fault diagnosis. In that way, for each iteration, it selects the test case produces the highest improvement appended to the result. ','language independent'),(41,'Adaptive test-case prioritization','The technique adapts the ordering of test cases based on execution information. In this way, the approach order the test cases while running them. Each test case executed gives information that is used to select the next test case to be executed. Initially, the first test case to be run is selected by using statement coverage information (the highest one is selected). Then, this first test case is executed and the result (passed or failed) is used to update the priority of the remaining test cases. The highest priority one is selected and then the process is repeated until all test cases have been executed.','language independent'),(42,'Extended greedy coverage','The technique is based on the probability that a unit of the program can reveal a fault. This probability is measured by the p-value. It is meant to be a combination of the total and additional coverage strategies. The p-value for a test case increases as it covers a unit of the program. It also increases according to the metrics Lines of Code and McCabe complexity of the unit (method) of a program.','language independent'),(43,'Genetic Algorithm with different replacement strategies','it‚Äôs a genetic algorithm where four different strategies can be used for the replacement phase: replacing with the worst individual; replacing at random; replacing the oldest individual or replace the parent with the child.','not defined'),(44,'Proportion-Oriented Randomized Algorithm (PORA)','It‚Äôs a search-based TCP algorithm. It uses clusters and distances between test cases to converge to a prioritized order. The distances are calculated using test cases input data.','language independent'),(45,'Fault Aware Test Case Prioritization (FATCP)','The technique aims at introducing a fault localization mechanism together with test case prioritization. It considers fault detection history from previous test suite executions and the coverage of each test case. It uses the TARANTULA method to calculate the suspiciousness of a test case, which indicates the possibility of a coverage criterion to contain faults.','language independent'),(46,'Similarity-based with historical failure information','The technique uses a similarity-based test quality metric. It considers a test as effective if it is similar to any of the failed test cases from the previous versions of the program under test. The similarity if extracted from the sequences of method calls, extracted from execution traces. The similarity can be calculated with different functions, like basic counting, hamming distance of edit distance.','object oriented'),(47,'Hypervolume-based GA (HGA)','The technique consists in a genetic algorithm which uses Hypervolume attributes to solve the prioritization problem given multiple test criteria. In this case, it is used three criteria: execution cost (to minimize), statement coverage (to maximize) and past fault coverage (to maximize).','language independent'),(48,'Online TCP','The technique takes advantage of the strengths of both additional and total greedy strategies. It initially works similar to additional technique, until full coverage of code is reached, then the total strategy is used, to try to detect those faults that could not be detected by the test cases selected by the additional strategy. A weight is assigned to the test cases according to their covered units. Two coverage metrics (statement and branch) are abstracted as a single coverage unit. As this weight approaches the 0 value, the technique behaves more like additional strategy, and as it approaches 1 value, it behaves like the total one.','language independent'),(49,'Global similarity-based TCP algorithm','This technique is similar to the ART-based algorithm. But instead of selecting a subset of the not yet prioritized test cases, it selects only one.  \nIt first selects the test case that has the maximum average distance from the set of test cases and put it into the end of the prioritized order. Then, it iteratively selects the test case from the not yet selected list that is the farthest from the last test case in the ordered sequence and then it is added at the tail of the ordered sequence. This is repeated until all test cases have been put into the ordered sequence.\n','language independent'),(50,'Randomized Local Beam Search (LBS)','The algorithm tries to enhance ART-based technique. It uses discrepancy measure to select the best k successors from the successor pool of test cases at each iteration. Using this measure ensures that the selected successors are far away from each other and distributed within the input domain with equal density.\nThe algorithm uses a candidate set of test cases at each round which is sorted according to their distance from the set of already prioritized test case and the first defined amount (beam) of these candidates are selected to form a successor‚Äôs pool. When all successors are selected, it selects the best k successors to compose the prioritized set. The process is repeated until all test cases have been included in the set.\n','language independent'),(51,'Genetic Algorithm with Epistasis','The technique introduces the epistasis theory into genetic algorithm to solve test case prioritization problem. It is used to analyze the different genes with the interaction of other genes.','language independent'),(52,'Historical test case performance','The technique incorporates historical fault detection to the prioritization of test cases. It uses the ratio of faults revealed by each test case historically over the number of times that this test case has been executed historically. Also, it uses the period of time that a test case hasn‚Äôt been executed, in the sense that if a test case hasn‚Äôt been executed in a long time, it should have higher priority. The third factor used is the priority of each test case during past executions and its coverage.','language independent'),(53,'Clustering prioritization techniques','The technique first clusters the test cases using code coverage information and Euclidian distance measure for distance calculation. Then, the test cases at each cluster are ordered using software metrics.  Four variants of the technique can be used:\n1)	Using code coverage information\n2)	Using code complexity, which is calculated using dependency relationships and LOC.\n3)	Using fault history information. That is, test cases that have detected faults in previous versions are given more priority.\n4)	Using the arithmetic mean of code complexity metric and fault detection rate.\nAfter reordering each cluster, a test case from each one is picked and put into the ordered list, using round robin method, until all test cases have been ordered.\n','language independent'),(54,'Time window based','The technique addresses prioritization at continuous integration environments, where different test suites need to be executed as resources become available, instead of only one test suite. A prioritization time window is used where test suites can have 2 different priorities, high or low. If a test suite hasn‚Äôt been executed in a given time or the test suite has failed within the given time or the test suite is new then it is given high priority. Otherwise, it receives low priority. ','language independent'),(55,'RePiR (regression test prioritization using information retrieval)','The technique consists in reducing the TCP problem to an Information Retrieval one. It considers the differences between two versions of the software as the query and the test cases as the document collection. Thus, the prioritized test suite is based on ranking the similarity score between test cases and the program differences.\nProgram elements are extracted from source code and used to construct the document collection. The retrieval model is based on the TF.IDF.\n','object oriented'),(56,'Execution frequency-based technique','The technique is based on the assumption that the chance to detect a fault is greater when a code modification is executed in many different application states. For this reason, it uses the execution frequency of code modification to order the test cases. It has different variants as follows:\n1)	Global frequency-based prioritization technique (GFP)\nThis variant simply uses the sum of execution frequency of each statement executed by a test case to order all test cases for an execution\n2)	Local Frequency-based Prioritization technique (LFP)\nThis variant select the test case that execute each code change the most to be ordered first. The remaining of the test cases uses the sum of execution frequency as ordering function.\n3)	Change frequency-based prioritization technique (CFP)\nThis variant prioritize tests that execute the most changes. If two tests executes the same amount of changes, it prefers the test that has higher global frequency.\n4)	Dynamic X-frequency-based prioritization tec','object oriented'),(57,'Program structure based prioritization','The technique is based on analysis of program structure to calculate the fault proneness and importance of each module. Those two characteristics are grouped into the Testing-importance of module (TIM). Then, TIM value is summed for all covered modules by a test case.','object oriented'),(58,'Modified Cost-Cognizant Test Case Prioritization (MCCTCP)','The technique uses historical information stores in a historical information repository to feed a genetic algorithm that is responsible of generating a prioritized order for the test cases. The historical information used are: cost, detected faults, and fault severities. As fitness function, the genetic algorithm uses a cost-cognizant metric, APFDc, from the previous execution of the regression tests.','language independent'),(59,'Parallel Execution of NSGA-II','The technique is an adaptation of the NSGA-II algorithm so that it can be executed on the GPU to gain efficiency. ','language independent'),(60,'Multi-perspective prioritization approach','The technique is composed of multiple perspectives considered for the prioritization.\nBusiness perspective: the failure impact is considered. It measures the severity of defects that a test case has revealed in the past.\nPerformance perspective: test execution time is considered as a critical factor for time=constrained environments.\nTechnical perspective: two factors are considered, failure frequency, which measures how often test cases detect failures and cross-functionality, which measures how much of the functionality of the system is covered by a test case.\nThese perspectives are used together as a function to calculate the ordering of the test cases. The function seeks to maximize failure frequency, maximize failure impact, maximize test coverage and minimize the execution time, all this for each test case.\n','language independent'),(61,'JUPTA','JUPTA is a heuristic that tries to predict the testing ability of a test case statically. On each iteration, it selects the test case from the test suite with the highest testing ability and add it to the prioritized set. This is repeated until all test have been ordered. It can also be used with ‚Äúfeedback‚Äù mechanism, where the test ability of all the not yet selected test cases is recalculated at each iteration.\nThe different variation of JUPTA include: testing granularity at method or class level; testing ability calculated from granularities at statement or method level; use or non-use of feedback mechanism. Thus, 8 different techniques can be used based on JUPTA.\nThe testing ability is estimated by using coverage information gathered from static call graphs from the program under test.\n','object oriented'),(62,'Variable Coefficient prioritization (VC)','The technique incorporates historical data into the prioritization process. It uses 3 different historical information: test case execution (each session of regression test execution that a test case isn‚Äôt executed, this test case has its execution count increased by one. When the test case is executed, the count is zeroed; test case priority in previous regression test session; historical fault detection effectiveness (ratio of the number of times a test case detects a fault to the total number of its executions). Furthermore, coverage information is also used.','language independent'),(63,'String distance prioritization','The technique is based on string distance between test cases. Distance measures can be: hamming distance, Levenshtein or Edit Distance, Cartesian and Manhattan distance.\nThe fitness function used in the prioritization is based on a distance between each test case and the set of the preceding test cases in the test suite. \nA greedy algorithm is used. At each iteration, it chooses a test cases which is the most distant from the set of already ordered test cases.\n','language independent'),(64,'Ordered sequence similarity-based','In this technique, each execution profile of a test case is transformed into an ordered sequence of program entities, which is sorted by their execution frequencies. Then a distance measure (edit measure) is used to measure the distance between a pair of ordered sequences.\nFOS: farthest-first ordered sequence. It‚Äôs an algorithm similar to ART. The difference is that all not yet selected test cases are included in the candidate set for FOS.\nGOS: greed-aided-clustering ordered sequence. Test cases in each cluster are prioritized by the additional greedy algorithm and then the test cases are selected from each cluster according to the orderings.\nConsidering this, 4 variations of the technique are proposed: FOS-statement, FOS-branch, GOS statement, GOS-branch.\n','language independent'),(65,'Multi objective prioritization approach (MOTCP+)','The technique focus on maximizing the number of discovered faults that are both technical and business critical. For this end, it recovers traceability links among software artifacts and uses a metric-based approach to identify critical and fault-prone portions of software artifacts.\nThree dimensions are used as objective for the prioritization algorithm: traceability links between requirements and test cases (requirements coverage), code coverage and execution cost. The algorithm used is NSGA-II.\n','object oriented'),(66,'GeTLO (Generalizaed AT using Lexicographical Order)','The technique aims at solving the issue of coverage based techniques when the prioritization objective is achieved and a tie is found. It uses lexicographical ordering. ','language independent'),(67,'REG+OI+POI. REG: Regular statement (branches) executed by a test case, OI: Output Influencing, POI: ','A relevant slice is composed by parts of the code that can affect a test case output. Based on that, the technique orders test cases according to a weight composed by the number of requirements (e.g.: statement/branch) present in the relevant slice + total of number of requirements exercised by a test case.\nThe technique has also two variations: GRP_REG + OI + POI and MOD * (REG + OI + POI)\n','language independent'),(68,'Enhanced Additional Greedy Algorithm (EAGA)','The technique is a modified version of the additional greedy algorithm. It is based on the coverage of code elements. The difference from the additional greedy algorithm is that the proposed approach firstly selects the code elements with the smallest coverage across all test cases. Then, the additional coverage is calculated for the test cases covering these code elements and the test case with the highest additional coverage is selected and is put into the ordered sequence of test cases. This is repeated until all test cases were ordered.','language independent'),(69,'topic-based TCP','The technique is a black-box static technique that uses developer knowledge contained in identifier names, comments and string literals. A text analysis technique named topic modelling is used to compute similarity between pair of test cases. The authors hypoteshize that similar test cases detect the same faults, then this can be used to select diverse test cases.\rThe steps performed by the approch are: 1. Preprocess the test suite to extract linguistic data for each test case. 2. Apply topic modeling to the extracted data, resulting in a vector of topic memberships. 3. Define the distance between test cases based on the vector. 4. Prioritize the test cases by maximizing the average distances to already prioritized test cases.\r','language independent'),(70,'Selective test prioritization','this model-based test prioritization technique divides the test suite in two groups: high priority tests and low priority tests. The high priority tests are those tests that exercise a marked transition in the system model (extended Finite state machine). A marked transition is a transition that has been modified between two versions of the software under test.','extended finite state machine model'),(71,'Count-based test prioritization','This model-based test prioritization technique is intended to be used on models (extended finite state machine) that has multiple marked transitions. A marked transition is a transition that is affected by a modification in the system under test. The main idea behind this technique is that tests that execute a greater number of marked transitions should be given a higher priority.','extended finite state machine model'),(72,'Frequency-based test prioritization','This model-based test prioritization technique is intended to be used on models (extended finite state machine) that has multiple marked transitions. A marked transition is a transition that is affected by a modification in the system under test. The main idea behind this technique is that tests with a higher frequency of executing marked transitions should be given a highed priority.','extended finite state machine model'),(73,'Model dependence-based test prioritization','this model-based test prioritization technique divides the test suite in two groups: high priority tests and low priority tests. The high priority tests are those tests that exercise a marked transition in the system model (extended Finite state machine). A marked transition is a transition that has been modified between two versions of the software under test. The high priority tests are prioritized based on model dependence analysis to identify different ways in which marked transitions interact with the remaining parts of the model.','extended finite state machine model'),(74,'multilevel prioritization algorithm for user session-based test suites','the proposed prioritization approach is intended for user session-based test suites. It is a modification of the two-way inter-window prioritization algorithm. Starting from an unordered set of test case, the algorithm generates window interaction\rpairs, and creates an ordered list from the given test suite. The algorithm then extracts test cases that have the same scores and uses window/parameter interaction pairs to reassign scores for extracted test cases. At the third level, the algorithm repeats the process: extracts test cases with the same scores and uses window/parameter/value interaction pairs to recalculate the scores. If there are any test cases left with the same scores, random tie-breaking is applied.\r','web-based applications'),(75,'Adjusted Requirement properties prioritization','The technique use the importance value of properties contained in requirements to prioritize test cases. This importance value is measured according to different dimensions, like customer-assigned priority and developer-assigned priority. Theese importance values are recorded along the history of execution, and related faults detected by test cases that are attributed to these properties of requirements are also recorded. This value can be dynamically adjusted according to historical results. If the number of faults detected in the current version is less than the previous one, the importance value of the assigned requirement property is lowered  by their difference, otherwise, the difference is added. This approach can be used with total or additional strategy. Where in the additional, the requirement properties values of each test case is recalculated. ','language independent'),(76,'PORT ‚Äì Priorization of Requirements for Test','This technique is based on the requirements of a software. It considers 4 factor regarding the requirements: Customer-assigned priority, developer perceived implementation complexity, fault proneness of requirements and requirement volatility. These four factors are summed, together with a given customizable weight, to find the prioritization factor value for every requirement. Then, the weighted priority for each test case is calculated using the prioritization factor value of each requirement that a test case exercise over the total prioritization factor value for all test cases.','language independent'),(77,'CP-ARS-all','This technique tries to select test cases to be ordered such that they are the most diverse possible from the already ordered test cases. It uses the FSCS-ART algorithm to decide the next test case to be ordered. It constructs a candidate set by randomly selecting k test cases from the not yet prioritized list and the test case with maximum distance from its nearest neighboor from the already prioritized list. ','language independent'),(78,'CP-ARS-pass','This technique is similar to ARS-all. It uses online information, that is, the next prioritized test case depends on the previous execution results of the current version. Then, the technique is executed along the tests. Different from ARS-all, this technique uses selective memory. That is, the already prioritized set will include only non-failed tests. In this way, it ensures that each new test case is far away from already failed tests. The distance measure used is based on the category partition CP method.','language independent'),(79,'Requirements based prioritization','The technique is based on requirements and uses 6 different factors: 1. Customer assigned priority of requirements; 2. Developer-perceived code implementation complexity; 3. Changes in requirement; 4. Fault impact of requirements; 5. Usability and 6. Application flow. Where the factors 1 to 3 are for new test cases and 4 to 6 are for regression test cases. The factors are summed together with a factor weight (calculated by the mean value of that factor over the sum of the mean value of all six factors) to give the requirement factor value. This value is used in the computation of the test case weight for the prioritization process.','language independent'),(80,'RL-based HMM','The technique is based on a digraph model. The model is realized using a reinforcement-learning and hidden-Markov-model technique.  It assumes that an automatic test case generator exists. The reinforcement learning is used to learn a model which is able to generate test cases and generate a rank of the based on their forward probabilities.','GUI applications');
/*!40000 ALTER TABLE `proposed_technique` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `publication_vehicle`
--

DROP TABLE IF EXISTS `publication_vehicle`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `publication_vehicle` (
  `idpublication_vehicle` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(200) DEFAULT NULL,
  PRIMARY KEY (`idpublication_vehicle`)
) ENGINE=InnoDB AUTO_INCREMENT=56 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `publication_vehicle`
--

LOCK TABLES `publication_vehicle` WRITE;
/*!40000 ALTER TABLE `publication_vehicle` DISABLE KEYS */;
INSERT INTO `publication_vehicle` VALUES (1,'International Arab Journal of Information Technology'),(2,'Software Testing, Verification and Reliability'),(3,'International Journal of Software Engineering and Knowledge Engineering'),(4,'Students Conference on Engineering and Systems'),(5,'International Conference on Software Engineering'),(6,'IEEE Transactions on Software Engineering'),(7,'International Workshop on Automation of Software Test'),(8,'ACM Symposium on Applied Computing'),(9,'International Computer Software and Applications Conference'),(10,'International Conference on Contemporary Computing and Informatics'),(11,'International Conference on Machine Intelligence Research and Advancement'),(12,'International Conference on Computational Intelligence and Computing Research'),(13,'Indian Journal of Science and Technology'),(14,'Communications in Computer and Information Science'),(15,'European Conference on Software Maintenance and Reengineering'),(16,'International Conference on Intelligent Systems Design and Applications'),(17,'International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing'),(18,'International Conference on Software Engineering and Knowledge Engineering'),(19,'International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology'),(20,'International Conference on Software Engineering and Service Sciences'),(21,'Asia-Pacific Software Engineering Conference'),(22,'International Symposium on Software Testing and Analysis'),(23,'International Conference on Computer Science and Information Technology'),(24,'International Conference on Recent Advances in Information Technology'),(25,'International Conference on Software Maintenance'),(26,'International Journal of Soft Computing'),(27,'ARPN Journal of Engineering and Applied Sciences'),(28,'International Conference on High Performance Computing and Applications'),(29,'Advances in Intelligent Systems and Computing'),(30,'International Symposium on Software Reliability Engineering'),(31,'Lecture Notes in Computer Science'),(32,'Empirical Software Engineering'),(33,'International Symposium on Foundations of Software Engineering'),(34,'International Conference on Automated Software Engineering'),(35,'International Conference on Software Testing, Verification and Validation'),(36,'ACM Transactions on Software Engineering and Methodology'),(37,'International Conference on Software Quality, Reliability and Security'),(38,'International Symposium on Empirical Software Engineering and Measurement'),(39,'International Symposium on Search Based Software Engineering'),(40,'Mathematical Problems in Engineering'),(41,'Software Quality Journal'),(42,'International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools'),(43,'Annual Genetic and Evolutionary Computation Conference'),(44,'Annual Hawaii International Conference on System Sciences'),(45,'Journal of Systems and Software'),(46,'Science of Computer Programming'),(47,'Automated Software Engineering'),(48,'Annual Computer Software and Applications Conference'),(49,'Science China Information Sciences'),(50,'International Conference on Industrial Engineering and Engineering Management'),(51,'International Workshop on Automating Test Case Design, Selection, and Evaluation'),(52,'International Workshop on Continuous Software Evolution and Delivery'),(53,'International Symposium on Empirical Software Engineering'),(54,'Information and Software Technology'),(55,'International Workshop Advances in Model Based Testing');
/*!40000 ALTER TABLE `publication_vehicle` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Temporary view structure for view `publication_vehicle_ranking`
--

DROP TABLE IF EXISTS `publication_vehicle_ranking`;
/*!50001 DROP VIEW IF EXISTS `publication_vehicle_ranking`*/;
SET @saved_cs_client     = @@character_set_client;
SET character_set_client = utf8;
/*!50001 CREATE VIEW `publication_vehicle_ranking` AS SELECT 
 1 AS `name`,
 1 AS `amount`*/;
SET character_set_client = @saved_cs_client;

--
-- Table structure for table `qa_answer`
--

DROP TABLE IF EXISTS `qa_answer`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `qa_answer` (
  `question_idquestion` int(11) NOT NULL AUTO_INCREMENT,
  `study_idStudy` int(11) NOT NULL,
  `study_publication_vehicle_idpublication_vehicle` int(11) NOT NULL,
  `value` float DEFAULT NULL,
  PRIMARY KEY (`question_idquestion`,`study_idStudy`,`study_publication_vehicle_idpublication_vehicle`),
  KEY `fk_question_has_study_study1_idx` (`study_idStudy`,`study_publication_vehicle_idpublication_vehicle`),
  KEY `fk_question_has_study_question1_idx` (`question_idquestion`),
  CONSTRAINT `fk_question_has_study_question1` FOREIGN KEY (`question_idquestion`) REFERENCES `question` (`idquestion`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `fk_question_has_study_study1` FOREIGN KEY (`study_idStudy`, `study_publication_vehicle_idpublication_vehicle`) REFERENCES `study` (`idStudy`, `publication_vehicle_idpublication_vehicle`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `qa_answer`
--

LOCK TABLES `qa_answer` WRITE;
/*!40000 ALTER TABLE `qa_answer` DISABLE KEYS */;
INSERT INTO `qa_answer` VALUES (1,1,1,1),(1,2,2,1),(1,3,3,1),(1,4,4,1),(1,5,3,0.5),(1,6,5,1),(1,7,6,1),(1,8,7,1),(1,9,8,1),(1,10,9,0.5),(1,11,10,0.5),(1,12,11,0.5),(1,13,12,0.5),(1,14,12,0.5),(1,15,13,1),(1,16,14,0),(1,17,15,1),(1,18,16,0.5),(1,19,17,0),(1,20,18,0.5),(1,21,19,0),(1,22,9,1),(1,23,20,0),(1,24,21,1),(1,25,22,0.5),(1,26,8,1),(1,27,23,0.5),(1,28,7,1),(1,29,24,0),(1,30,25,1),(1,31,26,0.5),(1,32,27,1),(1,33,28,0),(1,34,20,0),(1,35,13,0),(1,36,29,1),(1,37,13,1),(1,38,25,1),(1,39,22,1),(1,40,6,1),(1,41,6,1),(1,42,30,1),(1,43,25,1),(1,45,22,1),(1,46,31,1),(1,47,6,1),(1,48,32,1),(1,49,6,1),(1,50,33,1),(1,51,34,1),(1,52,30,0.5),(1,53,34,1),(1,54,9,1),(1,55,35,1),(1,56,5,1),(1,57,36,1),(1,58,22,1),(1,59,2,1),(1,60,26,0.5),(1,61,37,0.5),(1,62,33,1),(1,63,35,1),(1,64,38,1),(1,65,30,1),(1,66,39,0.5),(1,67,31,1),(1,68,40,1),(1,69,31,1),(1,70,9,1),(1,71,31,0.5),(1,72,41,1),(1,73,31,1),(1,74,25,1),(1,75,33,1),(1,76,5,1),(1,77,42,1),(1,78,21,1),(1,79,43,0.5),(1,80,44,1),(1,81,45,1),(1,82,31,1),(1,83,37,1),(1,84,6,1),(1,85,6,1),(1,86,46,1),(1,87,47,1),(1,88,41,1),(1,89,6,1),(1,90,6,1),(1,91,45,0.5),(1,92,48,0.5),(1,93,49,1),(1,94,50,1),(1,95,32,1),(1,96,41,1),(1,97,5,1),(1,98,51,1),(1,99,52,1),(1,100,45,0.5),(1,101,37,1),(1,102,53,1),(1,103,3,1),(1,104,54,1),(1,105,55,1),(1,106,25,1),(1,107,35,1),(1,108,2,1),(1,109,36,0.5),(2,1,1,0),(2,2,2,1),(2,3,3,1),(2,4,4,1),(2,5,3,1),(2,6,5,0.5),(2,7,6,1),(2,8,7,1),(2,9,8,1),(2,10,9,0.5),(2,11,10,0.5),(2,12,11,1),(2,13,12,1),(2,14,12,1),(2,15,13,1),(2,16,14,0.5),(2,17,15,1),(2,18,16,0.5),(2,19,17,1),(2,20,18,1),(2,21,19,1),(2,22,9,0.5),(2,23,20,1),(2,24,21,1),(2,25,22,1),(2,26,8,1),(2,27,23,1),(2,28,7,1),(2,29,24,1),(2,30,25,1),(2,31,26,1),(2,32,27,1),(2,33,28,1),(2,34,20,1),(2,35,13,0),(2,36,29,1),(2,37,13,0),(2,38,25,1),(2,39,22,1),(2,40,6,1),(2,41,6,1),(2,42,30,1),(2,43,25,1),(2,45,22,1),(2,46,31,1),(2,47,6,1),(2,48,32,1),(2,49,6,1),(2,50,33,1),(2,51,34,1),(2,52,30,1),(2,53,34,1),(2,54,9,1),(2,55,35,1),(2,56,5,1),(2,57,36,1),(2,58,22,1),(2,59,2,1),(2,60,26,1),(2,61,37,1),(2,62,33,1),(2,63,35,0.5),(2,64,38,0.5),(2,65,30,0.5),(2,66,39,0.5),(2,67,31,1),(2,68,40,1),(2,69,31,1),(2,70,9,1),(2,71,31,1),(2,72,41,1),(2,73,31,1),(2,74,25,1),(2,75,33,1),(2,76,5,1),(2,77,42,1),(2,78,21,1),(2,79,43,1),(2,80,44,0.5),(2,81,45,1),(2,82,31,1),(2,83,37,0.5),(2,84,6,1),(2,85,6,1),(2,86,46,1),(2,87,47,1),(2,88,41,1),(2,89,6,1),(2,90,6,1),(2,91,45,0.5),(2,92,48,1),(2,93,49,1),(2,94,50,1),(2,95,32,1),(2,96,41,1),(2,97,5,1),(2,98,51,1),(2,99,52,1),(2,100,45,0.5),(2,101,37,1),(2,102,53,0.5),(2,103,3,0.5),(2,104,54,0.5),(2,105,55,1),(2,106,25,1),(2,107,35,1),(2,108,2,1),(2,109,36,1),(3,1,1,0.5),(3,2,2,1),(3,3,3,1),(3,4,4,0),(3,5,3,0),(3,6,5,1),(3,7,6,1),(3,8,7,1),(3,9,8,1),(3,10,9,0.5),(3,11,10,0),(3,12,11,0),(3,13,12,0),(3,14,12,0),(3,15,13,0),(3,16,14,0),(3,17,15,0),(3,18,16,0),(3,19,17,0),(3,20,18,1),(3,21,19,0),(3,22,9,1),(3,23,20,0),(3,24,21,1),(3,25,22,0),(3,26,8,0),(3,27,23,0),(3,28,7,0),(3,29,24,0),(3,30,25,0.5),(3,31,26,0),(3,32,27,1),(3,33,28,0),(3,34,20,0),(3,35,13,0),(3,36,29,0),(3,37,13,0),(3,38,25,1),(3,39,22,1),(3,40,6,1),(3,41,6,1),(3,42,30,1),(3,43,25,1),(3,45,22,1),(3,46,31,0.5),(3,47,6,1),(3,48,32,1),(3,49,6,1),(3,50,33,1),(3,51,34,1),(3,52,30,1),(3,53,34,1),(3,54,9,1),(3,55,35,1),(3,56,5,1),(3,57,36,1),(3,58,22,1),(3,59,2,1),(3,60,26,1),(3,61,37,1),(3,62,33,1),(3,63,35,1),(3,64,38,1),(3,65,30,1),(3,66,39,1),(3,67,31,1),(3,68,40,1),(3,69,31,1),(3,70,9,1),(3,71,31,1),(3,72,41,0),(3,73,31,0),(3,74,25,0),(3,75,33,0),(3,76,5,0),(3,77,42,0),(3,78,21,0.5),(3,79,43,0),(3,80,44,1),(3,81,45,1),(3,82,31,0),(3,83,37,0),(3,84,6,1),(3,85,6,1),(3,86,46,1),(3,87,47,1),(3,88,41,1),(3,89,6,1),(3,90,6,1),(3,91,45,1),(3,92,48,1),(3,93,49,1),(3,94,50,1),(3,95,32,1),(3,96,41,1),(3,97,5,1),(3,98,51,1),(3,99,52,1),(3,100,45,1),(3,101,37,1),(3,102,53,1),(3,103,3,0.5),(3,104,54,0.5),(3,105,55,0),(3,106,25,0),(3,107,35,0),(3,108,2,0.5),(3,109,36,1),(4,1,1,0),(4,2,2,1),(4,3,3,1),(4,4,4,0),(4,5,3,0),(4,6,5,1),(4,7,6,1),(4,8,7,1),(4,9,8,1),(4,10,9,0),(4,11,10,0),(4,12,11,0),(4,13,12,0),(4,14,12,0),(4,15,13,0),(4,16,14,0),(4,17,15,0),(4,18,16,0),(4,19,17,0),(4,20,18,1),(4,21,19,0),(4,22,9,1),(4,23,20,0),(4,24,21,1),(4,25,22,0),(4,26,8,0),(4,27,23,0),(4,28,7,0),(4,29,24,0),(4,30,25,0),(4,31,26,0),(4,32,27,1),(4,33,28,0),(4,34,20,0),(4,35,13,0),(4,36,29,0),(4,37,13,0),(4,38,25,1),(4,39,22,1),(4,40,6,1),(4,41,6,1),(4,42,30,1),(4,43,25,1),(4,45,22,0),(4,46,31,0),(4,47,6,1),(4,48,32,1),(4,49,6,1),(4,50,33,1),(4,51,34,1),(4,52,30,1),(4,53,34,1),(4,54,9,1),(4,55,35,1),(4,56,5,1),(4,57,36,1),(4,58,22,1),(4,59,2,1),(4,60,26,1),(4,61,37,1),(4,62,33,1),(4,63,35,1),(4,64,38,1),(4,65,30,1),(4,66,39,1),(4,67,31,1),(4,68,40,1),(4,69,31,1),(4,70,9,1),(4,71,31,0),(4,72,41,0),(4,73,31,0),(4,74,25,0),(4,75,33,0),(4,76,5,0),(4,77,42,0),(4,78,21,0),(4,79,43,0),(4,80,44,1),(4,81,45,0),(4,82,31,0),(4,83,37,0),(4,84,6,1),(4,85,6,1),(4,86,46,1),(4,87,47,1),(4,88,41,1),(4,89,6,1),(4,90,6,1),(4,91,45,1),(4,92,48,1),(4,93,49,1),(4,94,50,1),(4,95,32,1),(4,96,41,1),(4,97,5,1),(4,98,51,1),(4,99,52,1),(4,100,45,1),(4,101,37,0),(4,102,53,0.5),(4,103,3,0),(4,104,54,0),(4,105,55,0),(4,106,25,0),(4,107,35,0),(4,108,2,0),(4,109,36,1),(5,1,1,0),(5,2,2,1),(5,3,3,1),(5,4,4,0.5),(5,5,3,1),(5,6,5,1),(5,7,6,1),(5,8,7,1),(5,9,8,1),(5,10,9,1),(5,11,10,0),(5,12,11,0.5),(5,13,12,0.5),(5,14,12,0),(5,15,13,0),(5,16,14,0),(5,17,15,1),(5,18,16,1),(5,19,17,1),(5,20,18,0.5),(5,21,19,0),(5,22,9,1),(5,23,20,0),(5,24,21,1),(5,25,22,1),(5,26,8,0.5),(5,27,23,1),(5,28,7,1),(5,29,24,1),(5,30,25,1),(5,31,26,0.5),(5,32,27,1),(5,33,28,0),(5,34,20,0.5),(5,35,13,0),(5,36,29,0),(5,37,13,1),(5,38,25,1),(5,39,22,1),(5,40,6,1),(5,41,6,1),(5,42,30,1),(5,43,25,1),(5,45,22,1),(5,46,31,1),(5,47,6,1),(5,48,32,1),(5,49,6,1),(5,50,33,1),(5,51,34,1),(5,52,30,1),(5,53,34,1),(5,54,9,1),(5,55,35,1),(5,56,5,1),(5,57,36,1),(5,58,22,1),(5,59,2,1),(5,60,26,1),(5,61,37,1),(5,62,33,1),(5,63,35,1),(5,64,38,1),(5,65,30,1),(5,66,39,1),(5,67,31,0.5),(5,68,40,1),(5,69,31,1),(5,70,9,1),(5,71,31,0.5),(5,72,41,1),(5,73,31,1),(5,74,25,1),(5,75,33,1),(5,76,5,1),(5,77,42,1),(5,78,21,0.5),(5,79,43,1),(5,80,44,1),(5,81,45,1),(5,82,31,0.5),(5,83,37,1),(5,84,6,1),(5,85,6,1),(5,86,46,1),(5,87,47,1),(5,88,41,1),(5,89,6,1),(5,90,6,1),(5,91,45,1),(5,92,48,0.5),(5,93,49,1),(5,94,50,1),(5,95,32,1),(5,96,41,1),(5,97,5,1),(5,98,51,1),(5,99,52,1),(5,100,45,1),(5,101,37,0.5),(5,102,53,0.5),(5,103,3,1),(5,104,54,1),(5,105,55,0.5),(5,106,25,0.5),(5,107,35,0.5),(5,108,2,1),(5,109,36,1);
/*!40000 ALTER TABLE `qa_answer` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `question`
--

DROP TABLE IF EXISTS `question`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `question` (
  `idquestion` int(11) NOT NULL AUTO_INCREMENT,
  `question` varchar(200) DEFAULT NULL,
  PRIMARY KEY (`idquestion`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `question`
--

LOCK TABLES `question` WRITE;
/*!40000 ALTER TABLE `question` DISABLE KEYS */;
INSERT INTO `question` VALUES (1,'The authors explicitly defined the dataset used on the study'),(2,'The authors explicitly defined the measures used on the study'),(3,'Statistical significance tests were used on the results to draw the conclusions'),(4,'The authors explicitly defined which statistical tests were used'),(5,'The authors explicitly defined the procedures used during the evaluation execution');
/*!40000 ALTER TABLE `question` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `study`
--

DROP TABLE IF EXISTS `study`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `study` (
  `idStudy` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(200) DEFAULT NULL,
  `year` decimal(10,0) DEFAULT NULL,
  `publication_vehicle_idpublication_vehicle` int(11) NOT NULL,
  `conclusions` varchar(999) DEFAULT NULL,
  PRIMARY KEY (`idStudy`,`publication_vehicle_idpublication_vehicle`),
  KEY `fk_study_publication_vehicle_idx` (`publication_vehicle_idpublication_vehicle`),
  CONSTRAINT `fk_study_publication_vehicle` FOREIGN KEY (`publication_vehicle_idpublication_vehicle`) REFERENCES `publication_vehicle` (`idpublication_vehicle`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=110 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `study`
--

LOCK TABLES `study` WRITE;
/*!40000 ALTER TABLE `study` DISABLE KEYS */;
INSERT INTO `study` VALUES (1,'Test Case Prioritization for Regression Testing Using Immune Operator',2013,1,'Authors compare their proposed algorithm for test case prioritization with genetic algorithms (GA). They conclude from an experiment that the proposed algorithm (IPA) performs better than GA, requiring less iterations to converge and achieving better APSC, APDC and APBC, which are the metrics used in the experiment.'),(2,'Prioritizing test cases for early detection of refactoring faults',2016,2,'Study presents a Refactor-based approach (RBA) for test case prioritization. The approach was evaluated regarding effectiveness through a case study and experimental studies. The case study shows the applicability of RBA in real projects when dealing with real test suites. The results of experimental studies show statistical evidence that, in fact, RBA promotes early detection of refactoring problems, when compared with other prioritization techniques. Moreover, RBA tends to place fault-revealing test cases in closer\npositions, which can be very helpful for pinpointing the fault.'),(3,'ComboRT: A New Approach for Generating Regression Test Cases for Evolving Programs',2016,3,'Paper proposes a technique for selection and prioritization of regression test cases. The technique is based on impact analysis and as results show that it outperforms TMC and AMC in terms of APFD metric.'),(4,'Clustering Based Novel Test Case Prioritization Technique',2015,4,'Authors propose a clustering based test case prioritization approach. The proposed approach outperformed other approaches in a case study.'),(5,'Regression Test Cases Prioritization Using Clustering\nand Code Change Relevance',2016,3,'Authors have presented a regression test case prioritization approach based on test case clustering program changes. They evaluated the proposal by measuring its precision and inclusiveness and found positive results. '),(6,'How Does Regression Test Prioritization Perform in\nReal-World Software Evolution?',2016,5,'The authors perform a comparative studies using 8 different prioritization techniques in order to find the impact of these techniques on software evolution scenarios. They conclude that both the traditional and time-aware test prioritization techniques tested become much less effective in software evolution involving test additions and they are stable and effective in case of only source code changes.'),(7,'To Be Optimal Or Not in Test-Case Prioritization',2016,6,'The authors conducted two experimental studies in order to compare two prioritization strategies, namely additional coverage-based and optimal coverage-based. They found that the optimal strategy is slightly better than the additional one, regarding the coverage goal, but significantly less effective regarding APFD and execution time. They conclude that it may not be worthwhile to pursue optimality in test-case prioritization by taking coverage as an intermediate goal.'),(8,'Test case prioritization incorporating ordered sequence of program elements',2012,7,'The authors improve the similarity-based test case prioritization technique using the ordered sequence of program elements measured by execution frequencies. Edit distance is used to calculate pair-wise distance of elements. The empirical results show that their technique performs better than ART or random techniques in terms of APFD.'),(9,'An empirical study on the effectiveness of time-aware test case prioritization techniques',2011,8,'The authors have studied the effectiveness of some statement coverage based time-aware test case prioritization techniques. Their results show that both for the purpose of statement coverage and fault detection, the time cost of each test case is not a key factor for time-aware test case prioritization. Therefore, it is not worth considering the time cost of each test case in most cases. They suggest using the techniques that consider only coverage or assuming all the test cases have the same time cost when prioritizing test cases.'),(10,'Design and analysis of cost-cognizant test case prioritization using genetic algorithm with test history',2010,9,'Authors propose a cost-cognizant prioritization technique that orders test cases according to their historical information. There is no need to analyze source code. Only historical information are considered, such as fault severity and test cost. They found in an experiment that the proposed technique outperforms other techniques in terms of APFDc measure, that is, considering test costs.'),(11,'A hierarchical test case prioritization technique for object oriented software',2015,10,'The authors presented an approach for test case prioritization based on hierarchical properties of object-oriented classes. They evaluated the technique in a test case and found that the proposal performs better than a random prioritization in terms of APFD.'),(12,'Evolutionary search algorithms for test case prioritization',2014,11,'the authors present a prioritization approach using genetic algorithm. The approach uses information about testing time and code coverage of each test case. They found in their evaluation that the approach performs better than random, optimal (?) and FEP (?) approaches.'),(13,'A novel approach for test case prioritization\n',2013,12,'Authors propose an approach that uses mutation test to check if a test can detect a fault. Then, each test capability is described by a binary string. The ordering uses Hamming distance between the strings. That is, each time, the test that have the high Hamming distance to the already selected tests are selected to the ordered set. The approach was demonstrated with an example comparing to others approaches like random and total fault coverage and achieved better APFD in the demonstration than the others.'),(14,'Modular based multiple test case prioritization',2012,12,'A modular based test case prioritization method is proposed. Test cases are ordered in two different levels, namely at modular level and global level. The ordering is based on the amount of faults covered by each test case. It was evaluated in a case study with greedy and additional greedy approach, performing better than them.'),(15,'An Ant Colony Algorithm to Prioritize the Regression Test Cases of Object-Oriented Programs',2016,13,'The author proposes the use of an Ant Colony Algorithm to prioritize test cases. The algorithm is based on test cases execution time. In the example given, the failures which each test case could detect was previously known. The results from the experimental analysis show that the proposed algorithm performs better than the random prioritization approach.'),(16,'Pair-Wise Time-Aware Test Case Prioritization for Regression Testing',2012,14,' the authors propose a prioritization approach based on time constraints. Number of faults detected, time of execution and number of repetitive faults detected are taken into consideration when prioritizing each test case. Thus, the faults that each test case can expose need to be known before execution. The authors have conducted a study comparing the proposed technique with other two techniques (which have not been described). As results, the authors found that the proposed technique performed better than the techniques APFD (average percentage of faults detection) and OTCP (optimal test case prioritization).'),(17,'A Multi-Objective Technique to Prioritize Test Cases based on Latent Semantic Indexing',2012,15,'The authors propose a technique that considers three different dimensions when prioritizing test cases. Structural dimension, related to the source code. Functional coverage related to the requirements and cost dimension related to the execution time of the test cases. Thus, the approach is a multi-objective technique that uses traceability among requirements and source code using an information retrieval technique named Latent Semantic Indexing (LSI). The authors conducted a case study and found that the proposed technique outperforms the compared techniques (random, code coverage and additional code coverage)'),(18,'Regression test cases prioritization using Failure Pursuit Sampling',2010,16,'The authors presented an adapted approach of the failure pursuit sampling technique for prioritization of regressions test cases. The approach uses information about previous runs of test cases. After an evaluation comparing the new approach with the original one, some improvements were found for some versions of the tested software, while others did not improve.'),(19,'Dynamic test case prioritization based on multi-objective',2014,17,'Authors propose a multi-objective prioritization approach, based on five dimensions: coverage, fault exposure potential, requirement property relevance, history information and execution time. The approach calculates an order for each of the dimensions and then normalize them to achieve a final ordering. The approach was demonstrated with an example and APBC metric was calculated. For the example, the proposed approach achieved the best results together with the additional coverage method.'),(20,'Similarity-based regression test case prioritization',2015,18,'authors propose a similarity-based approach for TCP. They test different similarity measures and conclude that the Euclidian distance performs better for the TCP task. Also, they empirically compare the proposed approach with coverage-based approaches. They found that their approach outperforms some of the coverage-based approaches and performs equally well as its better one (additional branch coverage) in terms of NAPFD, a normalized APFD measure, which exclude non executed test cases.'),(21,'Total coverage based regression test case prioritization using genetic algorithm',2015,19,'Authors propose a prioritization approach that also generate test cases to achieve total conditions coverage of a program. It uses genetic algorithm to do so. They tested it and compared to a bee colony algorithm, finding that the proposed approach achieved better coverage and executed faster.'),(22,'A clustering-Bayesian network based approach for test case prioritization\n',2015,9,'the authors propose an improvement for a previous prioritization technique based on bayesian networks (BN). The improvement consists on using an additional information before the BN calculation, which is a clustering based on coverage similarity between the test cases. The approach is tested in an experiment and the results found indicate that it is significantly better than the compared approaches.'),(23,'Test case prioritization for regression testing based on ant colony optimization\n',2015,20,'The authors proposed an approach for prioritization based on ant colony optimization algorithm that uses number of faults detected by a test case, test case execution time and fault severity as input. They execute an example to demonstrate and compare their approach with other common approaches. They found that they proposed approach performed better than the common ones and equally well as the optimal order in terms of APFD'),(24,'Test Case Prioritization Based on Information Retrieval Concepts',2014,21,'The authors propose a prioritization approach based on information retrieval and coverage based concepts. They are combined in order to feed a linear regression model that gives an estimated value of how many faults each test case can detect. This value is used to order the test cases. The approach is evaluated by comparing its performance in terms of APFD against traditional coverage approaches. Also, different coverage approaches are used in the linear regression model. The results achieved indicate that the proposed approach is general more effective, and never worse than existing baseline approaches.'),(25,'Effectively prioritizing tests in development environment',2002,22,'the authors present a prioritization framework named Echelon which prioritize test cases based on coverage and source code modification between two versions of a program, using its binary representation. They presented some results from the execution of the approach using Microsoft products and found that the approach was able to help on the identification of early faults.'),(26,'Towards the prioritization of regression test suites with data flow information',2005,8,'The authors propose a prioritization approach that uses data-flow information of the software under test. This is motivated by empirical studies that have shown that tests fulfilling an all-DU (def-use) test adequacy are more likely to reveal defects, thus, this is used to order the test cases. The proposed approach was tested in a case study that showed that no significant overhead is added on executing the algorithm. As for the effectiveness of the approach, it was found that, compared to a random ordering, the approach sometimes performed better and sometimes performed worse in terms of APFD.'),(27,'Prioritizing test cases for resource constraint environments using historical test case performance data',2009,23,'The authors present a prioritization approach based on historical and coverage information. They evaluated the approach on a series of Siemens applications and found that the approach achieved considerable achievements in terms of APFD.'),(28,'A refactoring-based approach for test case selection and prioritization',2013,7,'the authors present a prioritization approach based on refactoring edits of a program. Higher priority is given to test cases that exercise methods of the software affected by refactorings. They evaluated the approach on the JMock application, finding that, for refactoring faults, the approach outperforms the compared traditional techniques in terms of F-MEASURE and APFD.'),(29,'Effectiveness of Prioritization of Test Cases Based\non Faults',2016,24,'The authors presents a prioritization approach using fault rate and fault severity as factors. The proposed approach was evaluated and compared to traditional and kavitha et al approach. It was found that the proposed approach achieved better APFD than the others approaches compared.'),(30,'Prioritizing JUnit test cases in absence of coverage information',2009,25,'the authors propose Jupta, a static approach for prioritizing JUnit test cases in absence of coverage information. Jupta prioritizes test cases according to the static call graphs of test cases. Although Jupta does not rely on any previous coverage information, Jupta is able to perform as effective as techniques based on coverage information according to their controlled experimental study.'),(31,'Efficiency of test case prioritization technique based on practical priority factors\n',2015,26,'Conclusion: the authors propose an approach based on 10 different factors of a project, based on time, defect, requirements and complexity. These factors are weighted and combined into a unique value that is used to order the test cases of a test suite. They demonstrated an example of its application and the results found indicate that they can found a larger amount of defects in less time than a random ordering.'),(32,'Branch coverage based test case prioritization',2015,27,'The authors propose to evaluate two combinations of prioritization approaches. Additional branch coverage (Add) combined with ART. The combinations execute are 1) execute the first iteration of Add and the rest with ART and 2) execute the ART but instead of using Manhattan distance as distance measurement, uses Additional branch coverage. The results from the experiment shows that for the smaller program tested, only ART outperformed the control technique, random ordering. For the larger program, all compared techniques significantly outperformed random ordering and the best one was the pure Add technique and the second one was the combination of Add and ART.'),(33,'Test case prioritization techniques \'an empirical study\'',2014,28,'The authors gives an overview of existing prioritization approaches and discusses about greedy and genetic techniques. They evaluate the approaches and find that the genetic algorithm is more effective in terms of APFD.'),(34,'An effective test case prioritization method based on fault severity\n',2015,20,'The authors propose a technique similar to additional statement coverage (add-stmt). The reasoning is that on add-stmt, a random factor is used when two test cases have the same coverage, which can lead to loss of performance. To overcome this, they propose that instead of using coverage, use the severity of the faults that a test case can achieve. They demonstrate an example and compare with the add-stmt technique. They found that the proposed approach achieved higher APFDc than the compared technique.'),(35,'Combined Genetic and simulated annealing approach for test case prioritization\n',2015,13,'The authors propose a prioritization approach that combines genetic algorithm and simulated annealing. By using information about the total time for each test case to reveal all faults, it gives an ordered list of test cases. The approach is evaluated and compared to original genetic algorithm and simulated annealing and it is found that it can execute in less time.'),(36,'Multi-deterministic prioritization of regression test suite compared: ACO and BCO\n',2016,29,' the authors evaluates two techniques empirically: ACO and BCO. They found that both techniques achieve a considerable reduction in test suite size (this is not the goal of prioritization).'),(37,'A comparative evaluation of \"m-ACO\" technique for test suite prioritization',2016,13,'The authors empirically evaluates the performance of the m-ACO prioritization technique comparing to different approaches, like BCO, GA and original ACO. They found that the m-ACO approach achieves better values of APFD and PTR than the compared techniques.'),(38,'Test Case Prioritization: An Empirical Study',1999,25,'the authors perform an empirical study aiming at compare different prioritization techniques. They also propose a prioritization metric that aims at measuring the rate of fault detection of a test suite, named APFD. The results found in the study indicate that the FEP-based heuristics performs better than the compared ones, but given its high cost of executing, it may be worth using a coverage-based approach, like total branch coverage that costs less and yet achieve a good rate of fault detection.'),(39,'Prioritzing test cases for regression testing',2000,22,'the authors perform an empirical study aiming at compare different prioritization techniques; check if the use of finer granularities results in better prioritization in terms of APFD and if the use of fault proneness information can improve the performance of prioritization techniques. They found that finer granularities techniques achieves better results than coarser ones, more specifically, statement level techniques are better than function level techniques in terms of APFD. They also found that the use of fault indexes did not improve the effectiveness of the techniques.'),(40,'Prioritizing test cases for regression testing\n',2001,6,'the authors perform an empirical study aiming at compare different prioritization techniques in terms of APFD. The results show that FEP based techniques outperforms coverage based techniques in terms of APFD, however, given its high cost, it may not be as effective.'),(41,'Test Case Prioritization: A Family of Empirical Studies',2002,6,'the authors perform an empirical study aiming at compare different prioritization techniques; check if the use of finer granularities results in better prioritization in terms of APFD and if the use of fault proneness information can improve the performance of prioritization techniques. They found that finer granularities techniques achieves better results than coarser ones, more specifically, statement level techniques are better than function level techniques in terms of APFD. They also found that the use of fault indexes improve the effectiveness of the techniques statistically significantly, however the improvement was comparatively small.'),(42,'Empirical Studies of Test Case Prioritization in a JUnit Testing Environment',2004,30,'the authors perform an empirical study aiming at comparing prioritization techniques used in object oriented projects and Junit environments. They find that there is no difference at the granularity level used for coverage information in the rate of fault detection. Also, that the use of modification information do not improve the effectiveness of the prioritization approaches. Finally, that finer test suite granularities achieve better results in terms of APFD than coarser ones (method vs. class level)'),(43,'A controlled experiment assessing test case prioritization techniques via mutation faults',2005,25,' the authors perform an empirical study aiming at comparing prioritization techniques used in object oriented projects and Junit environments. More specifically, they evaluate the use of mutation faults instead of hand-seeded faults like previous studies. They find that mutation faults can be used to proper asses prioritization techniques capabilities. Special attention must be given to the amount of mutation faults, since the results may vary depending on it.'),(45,'Time-aware test suite prioritization\n',2006,22,'The authors propose a prioritization approach based on genetic algorithm that uses time-aware information to execute. Thus, the genetic algorithm has as input the execution time of each test case, coverage information at block or method level and time constraints. The approach was evaluated and compared to random ordering. Its results indicate the proposed approach outperforms random prioritization, being a suitable prioritization method when time constraints need to be used in the regression test.'),(46,'A prioritization approach for software test cases based on bayesian networks\n',2007,31,' the authors present a prioritization approach based on Bayesian networks. They construct a Bayesian model based on information extracted from the source code, such as quality metrics, coverage data and change analysis. They found through an experiment that the proposed approach performed better than compared techniques as the number of faults in the tested software grows.'),(47,'On the use of mutation faults in empirical assessments of test case prioritization techniques',2006,6,'The authors perform an empirical study to compare the use of mutation faults to hand seeded ones in test case prioritization studies focused on java programs and different prioritization techniques. They found that non-control techniques can improve the rate of fault detection on JUnit and TSL test suites, assessed relative to mutation faults, but results may vary according to number of mutation faults and test suites‚Äô fault detection ability. Mutation faults may provide a low-cost avenue to obtaining data sets on which statistically significant conclusions can be obtained. Also, studies of prioritization techniques using small numbers of faults may lead to inappropriate assessments of those techniques.'),(48,'Prioritizing JUnit Test Cases: An Empirical Assessment\nand Cost-Benefits Analysis',2006,32,'the authors investigate the effects of fine- versus coarse-granularity choices in test design on JUnit prioritization. They also investigate the practical impact of using prioritization techniques, related to costs. They found that for 3 subjects tested, the largest saving of costs was achieved by the additional block coverage and additional method coverage techniques.'),(49,'Search algorithms for regression test case prioritization',2007,6,'the authors present different techniques that use search-based algorithm to solve prioritization problems. They also execute an empirical study to measure the effectiveness of these techniques. The results indicate that greedy algorithm performs much worse than Additional greedy, 2-optimal and genetic algorithm. Also there is no significant difference between 2optimal and additional greedy algorithm, suggesting that the cheaper to implement and execute should be used.'),(50,'An empirical study of the effect of time constraints on the cost-benefits of regression testing',2008,33,'the authors present an empirical study to measure the effect of time constraints when applied on prioritization techniques. They found that the time constraints can play a significant role in determining the cost-effectiveness of prioritization techniques and cost-benefit among them. '),(51,'Adaptive random test case prioritization\n',2009,34,'the authors propose a family of techniques based on adaptive random test case prioritization. The techniques (which are the same but considers different granularities) use coverage information to build the prioritized order based on test cases that are farthest away from the already selected test cases, using distance measure Jaccard. They evaluate the approaches and find that they are way more effective than random ordering. Also, the approach at the branch granularity, using maxmin Jaccard measure is as efficient and as effective as traditional greedy approaches based on coverage.'),(52,'Oracle-centric test case prioritization',2012,30,'The authors propose a prioritization technique based on test oracles. They use dataflow information captured from a previous version to order the test cases of the current version. They evaluated the approach in a case study and found that the proposed technique outperforms random and additional block coverage for the tested subjects.'),(53,'Prioritizing tests for fault localization through ambiguity group reduction',2011,34,'the authors present a diagnosis test prioritization algorithm, called RAPTOR. The technique select test cases by their ability to produce a refined diagnosis rather than to produce a failure as early as possible. They evaluated the approach and found that it can significantly improve the reduction of inspection cost when compared to random, ART, additional statement coverage, FEP and SEQUOA.'),(54,'Adaptive test-case prioritization guided by output inspection\n',2013,9,'the authors present an adaptive prioritization technique that prioritizes test cases as they run. The output of an executed test is used along with statement coverage information to update the priority of the remaining test cases after each one is executed. They evaluated the approach on different programs and found that the approach is usually more effective than total statement approach and competitive to additional statement approach.'),(55,'Coverage-based test case prioritisation: An industrial case study',2013,35,'the authors empirically evaluate different coverage based prioritization approaches on an industrial system with real regression faults. They found that prioritization techniques based on additional coverage using finer grained coverage criteria (block, basic block and decision) outperformed all other techniques.'),(56,'Bridging the gap between the total and additional test-case prioritization strategies',2013,5,' the authors present a strategy for unifying the strengths of both the total and additional coverage prioritization approaches. They do this by using a p-value, which is the probability that a test case can reveal a fault. This value can be set manually or it can be set by the proposed model that uses quality metrics of the program under test. They tested different p-values on different programs and found that wide ranges of strategies are more effective than either the total or the additional strategies. Also, wide ranges of the proposed strategies using method coverage can be as effective as or more effective than the additional strategy using statement coverage'),(57,'A unified test case prioritization approach',2014,36,'the authors propose an extension for the previous proposed approach that unify total and additional prioritization strategies. In this extension, they added a static way of using the approach (getting coverage information). They evaluated the different variants of the proposed approach with 40 different C programs with LOC ranging between 3000 and 10000 and 13 different java programs with LOC ranging between 1890 and 80400; classes between 50 and 650. They found that techniques based on dynamic coverage were slightly more effective than those based on static coverage, and sometimes exhibited no significant differences. Techniques using method coverage were more beneficial than those\nusing statement coverage. The proposed approach was more effective when applied to test cases at the test-method level than at the test-class level. Finally, the proposed approach was more effective when applied to Java programs with unit tests than to C programs with system tests.\n'),(58,'Empirical evaluation of Pareto efficient multi-objective regression test case prioritisation\n',2015,22,'authors empirically evaluate different techniques that focus on multi objective problem solution, in this case, test case prioritization. They compare those techniques with traditional additional greedy strategies. They found that MOEAs (Multi objective Evolutionary Algorithms), specifically, NSGA-II and TAEA and hybrid algorithms can produce solutions whose  prioritisation effectiveness, measured by APFDc metric, is either equal or superior to those of solutions produced by the additional greedy algorithms.'),(59,'Coverage-based regression test case selection, minimization and prioritization: A case study on an industrial system\n',2015,2,'authors perform a case study of coverage-based prioritization techniques on a real industrial system with real faults. They found that additional coverage using finer grained\ncoverage criteria (block, basic block and decision) outperformed all other techniques used in the\nstudy, including random ordering (APFD = 74.2 vs 59.6 to 70.8 for the other techniques). The results of this study, when looking at all reported studies on prioritization, show that industry studies yield less positive results regarding the effectiveness of coverage-based techniques than\nartificial ones based on smaller programs and seeded faults.\n'),(60,'The effects of replacement strategies of genetic algorithm in regression test case prioritizatoin of selected test cases',2015,26,'authors present a genetic algorithm that aim at prioritizing regression test cases. They provide four different strategies for the replacement phase of the algorithm and evaluate each of them. They found through an empirical evaluation that the strategies ARW and ARP are the best choice from the four in terms of APFD.'),(61,'PORA: Proportion-Oriented Randomized Algorithm for Test Case Prioritization',2015,37,'the authors present a search based algorithm to TCP. The algorithm uses test cases input data to calculate the best ordering of the tests. They found through an empirical study on four UNIX programs that the proposed PORA technique outperforms traditional coverage based techniques, like total and additional strategies at different granularities levels in terms of APFD and converge competitively in terms of time to execute with other techniques.'),(62,'A large-scale empirical comparison of static and dynamic test case prioritization techniques',2016,33,'the authors perform an empirical study to evaluate static and dynamic prioritization techniques on 30 different Java programs. They found that the static techniques tend to outperform dynamic ones at the test-class level, but at the test-method level, dynamic outperforms static.'),(63,'An empirical study on bayesian network-based approach for test case prioritization\n',2008,35,'the authors propose an extension for a previously proposed prioritization technique that uses Bayesian networks. The extension is on the use of the feedback mechanism. They evaluated the new extension against the old technique and found that the use of the feedback mechanism improved the APFD for TSL suites, but not for JUnit ones. '),(64,'An effective fault aware test case prioritization by incorporating a fault localization technique\n',2010,38,'Authors propose a prioritization technique that considers fault detection history. They use TARANTULA suspiciousness metric In order to measure the likelihood that a covered element has any faults, but in the inverse order of the original proposal, since they use the premise that regression faults from earlier versions are always fixed. They evaluated the approach through an experimental study and found that the proposed approach outperforms total coverage approach.'),(65,'A similarity-based approach for test case prioritization using historical failure data\n',2015,30,'Authors propose a similarity-based approach for test case prioritization that uses historical failure information. They present 3 different similarity metrics that can be used and empirically evaluates them. They found that the best similarity metric for prioritization purpose is the IBC (improved basic counting) compared to the other proposed and traditional metrics.'),(66,'Hypervolume-Based Search for Test Case Prioritization',2015,39,'Authors propose the use of hypervolume attributes on solving prioritization problem with multiple criteria through genetic algorithms. They empirically evaluated the approach and found that it provides an improvement in terms of APFDc over additional greedy approaches.'),(67,'Test case prioritization using online fault detection information\n',2016,31,'Authors propose a prioritization technique that uses information about the faults detected during the execution of the test to adapt itself and behave more likely the additional greedy strategy or the total greedy strategy. They evaluated the approach empirically and found that the proposed technique outperforms the compared technique in most cases.'),(68,'Empirical Study of the Effects of Different Similarity Measures on Test Case Prioritization\n',2016,40,'Authors propose a new prioritization algorithm similar to the ART-based called Global similarity-based algorithm that is based on the similarity measures between test cases of a test suite. The proposal was empirically evaluated against ART-based and traditional coverage-based techniques. Also, different similarity measures were tested. They found that Euclidian distance performed better as a similarity measure than the compared ones. They also found that the proposed technique performed better than ART-based and traditional coverage-based techniques, except for additional branch coverage, which performed equally well as the proposed technique.'),(69,'Regression test case prioritisation for Guava\n',2015,31,'the authors investigate the use of multi-objective test case prioritization NSGA-II on a Java project with different fitness objectives. They found that it is possible to cover 100% of changed code and statement coverage with 0.2% of test cases requiring only 0.45% of total execution time of the entire suite. This highlights that developers for that project can use TCP to optimize their work.'),(70,'Bypassing code coverage approximation limitations via effective input-based randomized test case prioritization\n',2013,9,'Authors propose a randomized local beam search technique which doesn‚Äôt need coverage information to execute. It is based on test input data. It tries to evenly spread the input domain of test cases to find an effective solution. They found through an experiment that the proposed approach performs better than total coverage approaches and equally well with additional branch and additional statement coverage.'),(71,'Epistatic genetic algorithm for test case prioritization\n',2015,31,'the authors introduce the concept of epistasis into genetic algorithm to solve test case prioritization problem. They propose a new way to represent test case genes with Epistatic Test Case Segment and two new crossover operators. They found through an empirical study that the proposed crossover operators are better in terms of APSC (statement coverage) than the compared ones.'),(72,'Selecting a cost-effective test case prioritization technique\n',2004,41,'authors perform an empirical study to evaluate some prioritization techniques (traditional total and additional coverage and with binary difference) in terms of cost-effectiveness. They provided empirical evidences about the cost-effectiveness of each technique compared to the other, which can help practitioners on selecting the more appropriate one for his needs.'),(73,'Incorporating historical test case performance data and resource constraints into test case prioritization\n',2009,31,NULL),(74,'A clustering approach to improving test case prioritization: An industrial case study\n',2011,25,' authors propose the use of clustering for existing prioritization approaches that uses code coverage, code complexity, fault history and a combination of them. They show through an empirical study over a real industrial system that the use of clustering can improve the rate of fault detection even when time constraints need to be applied to the prioritization.'),(75,'Techniques for improving regression testing in continuous integration development environments\n',2014,33,'authors propose a prioritization technique to be used at continuous integration environments, where test suites arrive with new versions of the software and are executed as resources become available. Given this scenario, they want to prioritize those test suites. The technique that they use is to establish a time window and prioritize all test cases within this windows that failed recently, hasn‚Äôt been executed recently or is a new test suite. They performed an empirical study to evaluate different values for this time window and found that a window of 0.1 hours worked better for the dataset of test executions used.'),(76,'An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes',2015,5,'the authors present a prioritization technique that is based on information retrieval concepts. It requires the analysis of the source code to extract its elements and then those elements can be used to prioritize the test cases. The approach is evaluated in an empirical study which shows that the proposed approach performs better than traditional coverage approaches (total and additional) and JUPTA, a static analysis based approach that also doesn‚Äôt require coverage information.'),(77,'Prioritizing Regression Tests for Desktop and Web-Applications based on the Execution Frequency of Modified Code',2016,42,'authors present a family of techniques that uses execution frequency of code modification to order the test cases. They evaluated the approaches and compared to available studies on literature. They found that the proposed approach achieves better APFD than the compared techniques. They also found that their dynamic variants are better than the static‚Äôs at the cost of adding overhead to the process.'),(78,'Test Case Prioritization based on Analysis of Program Structure',2008,21,'authors present a prioritization technique that rely on the analysis of the program structure to prioritize test cases. They extract program structure characteristics and test coverage from the program‚Äôs call graph. They evaluated the approach on 2 Java programs and found that it performs better than traditional approaches on two out of four cases. '),(79,'Empirically studying the role of selection operators during search-based test suite prioritization\n',2010,43,'authors present an empirical study that evaluates genetic algorithms with different operators for test case prioritization. They found that the operator TRU40 gives the CE score across all application tested. '),(80,'On the fault-detection capabilities of adaptive random test case prioritization: Case studies with large test suites\n',2011,44,'authors conduct two experimental studies in order to investigate the use of two different distance measures on ART techniques. They also evaluated the use of measures that uses coverage versus measures that uses frequency information. They found that the coverage based distance measure was superior to frequency one on revealing failures. They also found that FSCS-ART using coverage distance measure performed the best when compared to other techniques, including traditional ones, like additional coverage at branch level. '),(81,'A history-based cost-cognizant test case prioritization technique in regression testing',2012,45,'authors propose a prioritization approach that makes use of historical information about test case executions to feed a genetic algorithm that calculates the prioritized order. They compared they approach with other genetic and traditional techniques and found that the proposed technique can significantly improve the effectiveness of test case execution when compared to the traditional coverage-based techniques and other history-based approaches.'),(82,'A fine-grained parallel multi-objective test case prioritization on GPU\n',2013,31,'authors present an adaptation of the NSGA-II algorithm in order to make it be calculated on the GPU and speed-up the execution time. They evaluated the approach and found improvement of up to 100x compared to a normal CPU version.'),(83,'Multi-Perspective Regression Test Prioritization for\nTime-constrained Environments',2015,37,'Authors present a framework for prioritization that includes three different perspectives. Business, performance and technical. Those are used to calculate the final ordering of the test cases. They evaluated the approach and compared to traditional (manual) ordering. They found that it is capable of efficiently prioritize the test cases, achieving better performance than manual regression testing.'),(84,'The effects of time constraints on test case prioritization: A series of controlled experiments\n',2010,6,'Authors perform an empirical study to evaluate the cost-effectiveness of the use of prioritization techniques, using an economic model named EVOMO. The results shows that time constraints can indeed play a significant role in determining both the cost-effectiveness of prioritization techniques and the relative cost-benefit trade-offs among techniques. Also, when the SUT has a large number of faults, heuristics techniques can be beneficial.'),(85,'A static approach to prioritizing JUnit test cases\n',2012,6,'Authors present a static prioritization approach with 8 different variations. The technique doesn‚Äôt require dynamic coverage information. They evaluated the approach and compared to traditional dynamic techniques. The results show that dynamic technique achieve better effectiveness, however, JUPTA attain results close to them and may be more cost-effective, since there is no need to dynamically gather coverage information. Among the JUPTA variations, those using feedback and operating at method and statement coverage level exhibit the greatest effectiveness.'),(86,'An improved method for test case prioritization by incorporating historical test case data\n',2012,46,'Authors present a history-based prioritization technique that incorporates three measures into the prioritization process: number of sessions that a test case hasn‚Äôt been executed, test case priority in previous executions and historical fault detection effectiveness. They evaluated the approach, comparing to other history-based techniques. The proposed approach outperformed the compared ones.'),(87,'Prioritizing test cases with string distances\n',2012,47,' authors present a prioritization approach that uses string distances. An empirical evaluation showed that the best distance measure to be used with the approach is Manhattan distance and that the approach is better than random ordering.'),(88,'Similarity-based test case prioritization using ordered sequences of program entities\n',2014,41,'authors present a new similarity-based prioritization technique based on ordered sequence of program entities collected from execution profiles of test cases. This technique can be applied to prioritization algorithms like FOS and GOS. An empirical study showed that the new techniques can increase the fault detection rate and detect bugs in loops more quickly than existing approaches.'),(89,'A Multi-objective technique to prioritize test cases\n',2015,6,'author propose a multi-objective prioritization technique based on three dimensions: requirements coverage, code coverage and execution cost, using the algorithm NSGA-II. The technique was evaluated in an empirical study with 21 java programs. The results show that the approach is competitive with existing approaches, outperforming them in most of the cases.'),(90,'Test Case Prioritization Using Lexicographical Ordering\n',2016,6,'authors present a technique that aims at resolving limitation from existing greedy techniques, specifically, when a tie occurs in the process of coverage based prioritization. The limitation is solved by using lexicographical ordering. They evaluated the approach and compared with the traditional coverage approaches and Bayesian networks. The results show that the proposed technique outperforms all but one technique (additional coverage) in all cases. Additional coverage is outperformed by the proposed technique in 4 out of 6 cases and is similar in the others.'),(91,'Experiments with test case prioritization using relevant slices\n',2008,45,'authors present a technique that use relevant slices of test cases to prioritize them. They evaluated the approach and found that the proposed heuristics outperforms the traditional total approach.'),(92,'Input-based adaptive randomized test case prioritization: A local beam search approach',2013,48,'the authors extend a previously proposed technique based on the input of test cases for randomized prioritization. They evaluate the approach with different variants and compare to other search-based techniques. They found that the proposed techniques can achieve higher mean effectiveness than compared techniques, but the results are not statistically significant. This means that the proposed technique can be as effective as the best techniques using code coverage information.'),(93,'Comparing logic coverage criteria on test case prioritization\n',2012,49,'authors present an empirical study to compare prioritization techniques that used logic criteria coverage. The results show that criteria with fine-grained coverage information, MC/DC and fault based logic coverage criteria, have the best fault detection capability.'),(94,'A study of applying severity-weighted greedy algorithm to software test case prioritization during testing',2014,50,'The authors present an enhanced version of the Additional Greedy Algorithm. They evaluate the approach and compare it agaisnt the traditional Additional Greedy Algorithm and Greedy Algorithm. They found that the proposed approach can be more suitable for prioiritization than AGA and GA, according to results collected in terms of APFD, APFDc and APDC.'),(95,'Static test case prioritization using topic models',2014,32,'Authors present a prioritization technique based on topic modeling of test cases text. The topic modelling is used to maximize the diversity of test cases when ordering the test cases. They evaluated the proposed approach and found that it is more effective than a string-based and a call-graph-based technique on the studied systems.'),(96,'State-based models in regression test suite prioritization',2016,41,'authors present prioritization techniques based on system models, specifically, extended finite state machines. In those techniques, information about the system model and its behavior is used in the prioritzation. They empirically evaluate the approaches. The results indicate that all of the proposed approaches outperforms random ordering. Also, the approach model-dependence-based test prioritization performs better than all other compared techniques.'),(97,'Comparing White-box and Black-box Test Prioritization',2016,5,'Authors perform an empirical study to compare white box against black box prioritization techniques. They evaluate the effectiveness of 20 different techniques on 5 different programs. The results indicate that little difference exists between the effectiveness of white and black box techniques and that black box techniques usually requires more time to execute. '),(98,'Multilevel coarse-to-fine-grained prioritization for GUI and web applications',2016,51,'authors propose a prioritization approach intended to user session-based test suites on web application. They aim at descreasing variability of results that other approaches offer. They evaluate the approach and found that it in fact lower the variability of results and produce better results compared to other techniques.'),(99,'History-Based Dynamic Test Case Prioritization for Requirement Properties in Regression Testing',2016,52,'authors present a prioritization technique that is based on the importance value of requirements properties and is dinamically adjusted according to history of executions. They evaluated the approach, comparing to static requirement based techniques. They found that the proposed approach achieves better results in terms of APFD than the compared techniques.'),(100,'Improving test efficiency through system test prioritization',2012,45,'authors present a prioritization method that is based on the requirements. It uses four different factors to calculate the ordering for system test cases. They evaluated the approach against random ordering and found that the proposed approach achieved better results. They also found that among the prioritzation factors used, the customer priority was the most significant.'),(101,'Test Case Prioritization using Adaptive Random Sequence with Category-Partition-based Distance',2016,37,'Authors present a prioritization approach based on the adaptive random sequence algorithm. They propose two variants of the approach, one static and one dynamic. They evaluated the proposal and found that the compared approach only has better APFD values in some cases, whereas the proposed approaches always use much less time to execute than the compared. Regarding distance measure used, CP distance is more cost-effective than Manhattan distance.'),(102,'System Test Case Prioritization of New and Regression Test Cases',2005,53,'Authors present a prioritization approach based on four factors of the requirements. They evaluated the approach through an empirical study on four applications and found that the proposed apporach improves the ratio of fault detection over random prioritization.'),(103,'Requirement based system test case prioritization of new and regression test cases',2009,3,'authors propose a prioritization technique that is based on the requirements. It uses 6 different factors to calculate the execution order of the test cases. They evaluted the approach and compared to random ordering. The results indicate that the proposed approach is better than random order in terms of rate of fault detection. '),(104,'Factor oriented requirement coverage based system test case prioritization of new and regression test cases',2009,54,'authors propose a prioritization technique that is based on the requirements. It uses 6 different factors to calculate the execution order of the test cases. They evaluted the approach and compared to random ordering. The results indicate that the proposed approach is better than random order and total coverage prioritization at method and statement granularities in terms of rate of fault detection. '),(105,'Model-Based Test Prioritization Heuristic Methods and Their Evaluation',2007,55,'authors propose some model-based techniques to be used for test prioritization. They evaluted the approaches and found that some of them may improve the rate of fault detection compared to random ordering.'),(106,'Application of system models in regression test suite prioritization',2008,25,'authors propose some model-based techniques to be used for test prioritization. They evaluted the approaches and found that model dependence-based and count-based test prioritization techniques achieved the best performance among the compared techniques.'),(107,'Experimental Comparison of Code-Based and Model-Based Test Prioritization',2009,35,'authors execute a small experimental study to evaluate a model-based prioritization technique agaisnt a code-based one. They found that the model-based technique, namely, count-based test priorization, achieved better effectiveness than the code-based technique.'),(108,'Regression test suite prioritization using system models',2012,2,'authors propose two model-based prioritization techniques. They execute an experimental study to evaluate the proposed techniques agaisnt random ordering. They found that the use of the proposed techniques can improve the rate of fault detection.'),(109,'Test case prioritization using extended digraphs',2015,36,'authors propose a mode-based prioritization technique aimed to be used with GUI applications. It uses reinforcement learning and hidden markov model. They evaluated the approach and found that it achieves better results than the compared approaches. ');
/*!40000 ALTER TABLE `study` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `study_technique`
--

DROP TABLE IF EXISTS `study_technique`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `study_technique` (
  `study_idStudy` int(11) NOT NULL AUTO_INCREMENT,
  `study_publication_vehicle_idpublication_vehicle` int(11) NOT NULL,
  `technique_idtechnique` int(11) NOT NULL,
  PRIMARY KEY (`study_idStudy`,`study_publication_vehicle_idpublication_vehicle`,`technique_idtechnique`),
  KEY `fk_study_has_technique_technique1_idx` (`technique_idtechnique`),
  KEY `fk_study_has_technique_study1_idx` (`study_idStudy`,`study_publication_vehicle_idpublication_vehicle`),
  CONSTRAINT `fk_study_has_technique_study1` FOREIGN KEY (`study_idStudy`, `study_publication_vehicle_idpublication_vehicle`) REFERENCES `study` (`idStudy`, `publication_vehicle_idpublication_vehicle`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `fk_study_has_technique_technique1` FOREIGN KEY (`technique_idtechnique`) REFERENCES `proposed_technique` (`idtechnique`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=110 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `study_technique`
--

LOCK TABLES `study_technique` WRITE;
/*!40000 ALTER TABLE `study_technique` DISABLE KEYS */;
INSERT INTO `study_technique` VALUES (1,1,1),(2,2,2),(28,7,2),(3,3,3),(4,4,4),(5,3,5),(7,6,6),(8,7,7),(10,9,8),(11,10,9),(12,11,10),(13,12,11),(14,12,12),(15,13,13),(16,14,14),(17,15,15),(18,16,16),(19,17,17),(20,18,18),(21,19,19),(22,9,20),(24,21,22),(25,22,23),(27,23,25),(29,24,26),(30,25,27),(85,6,27),(30,25,28),(85,6,28),(31,26,29),(32,27,30),(32,27,31),(34,20,32),(35,13,33),(45,22,34),(46,31,35),(63,35,35),(49,6,36),(49,6,37),(51,34,38),(52,30,39),(53,34,40),(54,9,41),(56,5,42),(57,36,42),(60,26,43),(61,37,44),(64,38,45),(65,30,46),(66,39,47),(67,31,48),(68,40,49),(70,9,50),(92,48,50),(71,31,51),(73,31,52),(74,25,53),(75,33,54),(76,5,55),(77,42,56),(78,21,57),(81,45,58),(82,31,59),(83,37,60),(85,6,61),(86,46,62),(87,47,63),(88,41,64),(89,6,65),(90,6,66),(91,45,67),(94,50,68),(95,32,69),(96,41,70),(105,55,70),(106,25,70),(108,2,70),(96,41,71),(105,55,71),(106,25,71),(107,35,71),(96,41,72),(105,55,72),(96,41,73),(105,55,73),(106,25,73),(108,2,73),(98,51,74),(99,52,75),(100,45,76),(102,53,76),(101,37,77),(101,37,78),(103,3,79),(104,54,79),(109,36,80);
/*!40000 ALTER TABLE `study_technique` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `subjects`
--

DROP TABLE IF EXISTS `subjects`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `subjects` (
  `id_subjects` int(11) NOT NULL AUTO_INCREMENT,
  `amount_programs` int(11) DEFAULT NULL,
  `language` varchar(45) DEFAULT NULL,
  `evaluation_id` int(11) DEFAULT NULL,
  PRIMARY KEY (`id_subjects`),
  KEY `evaluation_id_idx` (`evaluation_id`),
  CONSTRAINT `evaluation_id` FOREIGN KEY (`evaluation_id`) REFERENCES `evaluation` (`idevaluation`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=115 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `subjects`
--

LOCK TABLES `subjects` WRITE;
/*!40000 ALTER TABLE `subjects` DISABLE KEYS */;
INSERT INTO `subjects` VALUES (1,40,'C',59),(2,13,'Java',59),(3,6,'C/C++',60),(4,1,'C/C++',61),(5,3,NULL,62),(6,4,NULL,63),(7,30,'Java',64),(8,5,'Java',65),(9,8,NULL,66),(10,5,'Java',67),(11,6,NULL,68),(12,7,'C',69),(13,8,'C',70),(14,1,'Java',71),(15,4,NULL,72),(16,4,NULL,73),(17,8,'C',74),(18,8,'C',75),(19,1,'C++/X++',76),(20,1,'dataset of test results',77),(21,8,'Java',78),(22,3,'Java',79),(23,2,'Google Web toolkit',79),(24,2,'Java',80),(25,8,NULL,81),(26,2,'C',82),(27,2,'C',83),(28,2,'C',84),(29,8,NULL,85),(30,1,'C++',85),(31,3,NULL,86),(32,5,'Java',87),(33,4,'Java',88),(34,8,'C',89),(35,7,'C',90),(36,5,'Java',91),(37,31,'Java',92),(38,6,'Java',93),(39,7,NULL,94),(40,4,NULL,95),(41,1,'PL_SQL',1),(42,3,'Java',2),(43,26,'Java',3),(44,4,'Java',4),(45,2,'Java',5),(46,1,'Flow Graph',6),(47,8,'Java',7),(48,8,'C',8),(49,2,'Java',8),(50,2,'C',9),(51,3,'Java',9),(52,3,'Java',10),(53,8,'C',11),(54,1,'C',12),(55,1,'',13),(56,3,'C',14),(57,3,'',16),(58,3,'Java',17),(59,2,'Java',19),(60,1,'C',20),(61,4,'C',22),(62,1,'Java',24),(63,2,'Java',24),(64,4,'Java',26),(65,1,'',27),(66,3,'Java',28),(67,7,'C',29),(68,1,'C',29),(69,1,'Java',30),(70,2,'Java',32),(71,2,'C',34),(72,7,'C',39),(73,7,'C',40),(74,1,'C',40),(75,7,'C',41),(76,1,'C',41),(77,7,'C',42),(78,1,'C',42),(79,2,'C',43),(80,1,'C',43),(81,4,'Java',44),(82,4,'Java',45),(83,2,'Java',46),(84,1,'Java',47),(85,4,'Java',48),(86,2,'Java',49),(87,4,'Java',50),(88,6,'C',51),(89,5,'Java',52),(90,11,'C',53),(91,3,'Simulink',54),(92,12,'C',55),(93,8,'C',56),(94,4,'Java',56),(95,1,'C/C++',57),(96,4,'Java',58),(97,3,'Java',96),(98,7,'C',97),(99,1,'C',97),(100,2,'Java',98),(101,5,'C',99),(102,5,'C',100),(103,3,'Java',101),(104,1,'PHP',101),(105,1,'Java',102),(106,2,'',103),(107,11,'C',104),(108,1,'Java',105),(109,1,'PHP',105),(110,5,'C',107),(111,6,'C',108),(112,7,'C/C++',109),(113,5,'C',110),(114,4,NULL,111);
/*!40000 ALTER TABLE `subjects` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `subjects_metric`
--

DROP TABLE IF EXISTS `subjects_metric`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `subjects_metric` (
  `subjects_id` int(11) NOT NULL,
  `metric_id` int(11) NOT NULL,
  `range_start` int(11) DEFAULT NULL,
  `range_final` int(11) DEFAULT NULL,
  PRIMARY KEY (`subjects_id`,`metric_id`),
  KEY `subject_id_idx` (`subjects_id`),
  KEY `metric_id_idx` (`metric_id`),
  CONSTRAINT `metric_id` FOREIGN KEY (`metric_id`) REFERENCES `artifact_metric` (`idartifact_metric`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `subject_id` FOREIGN KEY (`subjects_id`) REFERENCES `subjects` (`id_subjects`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `subjects_metric`
--

LOCK TABLES `subjects_metric` WRITE;
/*!40000 ALTER TABLE `subjects_metric` DISABLE KEYS */;
INSERT INTO `subjects_metric` VALUES (1,1,3000,10000),(2,1,1890,80400),(2,3,50,650),(3,1,4528,14437),(3,2,567,2005),(4,1,59132,72925),(5,1,283,670),(5,2,10,23),(5,3,3,12),(5,4,25,42),(6,1,4081,10124),(6,2,217,809),(7,1,3234,82998),(7,7,6,122),(7,10,16,4026),(8,1,7600,80400),(8,2,78,912),(8,3,26,627),(9,1,138,6218),(9,2,1052,13585),(10,2,2205,4130),(11,1,520,59846),(11,2,360,4130),(12,1,173,726),(12,2,1052,5542),(13,1,261,17153),(13,2,217,4130),(14,2,26815,26815),(14,3,62,62),(15,1,4081,10124),(15,2,217,809),(16,1,3016,59412),(16,2,764,6159),(17,1,5000,68000),(17,2,19,1985),(18,1,148,6218),(18,2,1052,13585),(19,1,650000,705000),(19,2,600,827),(19,3,500,908),(21,1,5700,96900),(22,1,5400,43400),(22,3,50,389),(23,1,40000,170000),(23,3,979,2300),(24,1,5400,16800),(24,3,19,192),(24,7,10,18),(24,10,84,209),(25,2,14,66),(26,1,512,6199),(26,2,5542,13551),(26,12,20,136),(27,1,512,6199),(27,2,5542,13551),(27,12,20,136),(28,1,6671,14424),(28,2,525,360),(28,12,120,285),(29,1,73,3813),(29,2,1472,13550),(30,1,11876,11876),(30,2,9287,9287),(31,1,2985,3327),(31,2,501,700),(32,1,7600,80400),(32,2,78,912),(32,3,26,627),(33,1,1894,80444),(33,3,19,650),(33,4,284,7524),(33,7,10,149),(33,10,78,878),(34,1,148,6218),(34,2,1052,13585),(35,1,173,565),(35,2,1052,5542),(36,1,2459,75429),(36,2,47,164),(37,1,2000,138000),(37,2,33,2307),(37,9,10,31),(38,1,5400,80400),(38,10,78,912),(39,1,138,516),(39,2,1052,5542),(40,1,4081,10124),(40,2,217,809),(41,1,693,693),(41,2,520,520),(42,1,2000,17000),(42,2,65,504),(43,2,3568,3568),(44,2,41,97),(44,3,19,393),(44,4,106,3313),(45,1,389,627),(45,7,28,150),(47,1,1362,67282),(47,2,7,5269),(48,1,67,6218),(48,2,1026,13585),(48,4,9,136),(49,1,1831,2255),(49,2,95,567),(49,4,187,274),(50,1,7050,13128),(50,2,214,470),(50,4,88,155),(51,1,1589,35622),(51,2,48,146),(51,4,212,1234),(52,1,2459,43407),(52,2,47,164),(53,1,173,9564),(53,2,1052,13585),(54,1,6671,11990),(54,2,360,360),(54,12,120,285),(55,4,4,4),(56,1,138,299),(57,1,525,1750),(57,2,60,161),(57,8,5,9),(58,1,1864,3095),(58,2,169,397),(59,1,1121,1827),(59,2,33,47),(59,3,8,18),(59,9,10,10),(60,1,299,299),(60,2,2650,2650),(60,12,18,18),(61,1,341,17153),(61,2,370,4130),(62,1,25800,80400),(62,3,229,627),(62,4,2511,7520),(62,7,28,105),(63,1,1890,19000),(63,3,19,180),(63,4,284,1629),(63,10,84,209),(64,1,875,18200),(64,7,15,200),(64,10,94,3586),(65,1,1800000,1800000),(65,2,3128,3128),(65,12,31020,31026),(65,13,668068,668274),(66,2,7,21),(66,3,1,3),(66,4,5,33),(68,1,10000,10000),(69,1,5000,5000),(69,2,504,504),(70,1,5400,80400),(70,3,19,650),(70,10,126,878),(71,1,564,6199),(71,2,5542,13585),(71,11,180,1190),(71,12,21,136),(72,1,138,516),(72,2,1608,5542),(73,1,138,516),(73,2,1608,5542),(74,1,6218,6218),(74,2,13585,13585),(75,1,138,516),(75,2,1608,5542),(76,1,6218,6218),(76,2,13585,13585),(77,1,138,516),(77,2,1608,5542),(78,1,6218,6218),(78,2,13585,13585),(79,1,5451,9153),(79,2,525,613),(79,12,133,140),(80,1,300000,300000),(80,2,135,135),(80,12,2875,2875),(81,3,50,627),(81,7,11,150),(81,10,78,877),(82,3,5,627),(82,7,11,150),(82,10,78,877),(83,2,28,53),(83,3,5,22),(83,12,73,305),(84,1,23000,124000),(84,2,0,105),(84,3,143,650),(85,3,50,627),(85,7,11,150),(85,10,78,877),(86,2,216,1533),(86,3,26,87),(87,1,5400,80400),(87,3,50,627),(87,7,11,150),(87,10,78,877),(88,1,412,11148),(88,2,1293,13585),(88,13,46,951),(88,14,56,2331),(89,1,7600,80400),(89,2,78,912),(89,3,26,627),(90,1,133,10124),(90,2,219,5542),(91,1,336,1237),(91,15,14,4510),(92,1,174,14194),(92,2,107,5542),(93,1,173,9564),(94,1,1894,80444),(95,1,59132,72925),(95,13,15085,18762),(95,14,8023,12134),(96,1,1890,80400),(96,3,19,650),(96,4,284,7524),(96,7,10,149),(96,10,78,878),(97,1,138,75429),(97,2,105,1608),(97,14,10,1080),(97,16,28,2422),(98,1,152,514),(98,2,1052,5542),(98,11,16,140),(99,1,9127,9127),(99,2,13585,13585),(99,11,1068,1068),(100,1,90000,571000),(100,2,53,120),(101,1,609,1416),(101,2,834,1439),(101,17,5,20),(101,18,20,89),(102,1,4324,58344),(102,2,111,500),(103,1,4893,18376),(103,2,105,274),(103,3,104,219),(103,4,236,644),(104,1,364290,364290),(104,2,109,109),(104,3,1557,1557),(104,4,13905,13905),(105,1,440000,440000),(105,2,500,500),(106,1,100000,2000000),(106,2,87,115),(107,1,135,516),(107,2,162,5542),(108,1,300000,300000),(108,2,100,100),(109,1,100000,100000),(109,2,100,100),(110,1,609,1416),(110,2,834,1000),(110,17,5,20),(110,18,20,87),(111,1,609,1416),(111,2,834,1439),(111,17,5,20),(111,18,20,89),(112,1,609,1416),(112,2,834,1658),(112,17,5,20),(112,18,20,89),(113,1,609,1416),(113,2,834,1000),(113,17,5,20),(113,18,20,87),(114,1,2515000,10580000);
/*!40000 ALTER TABLE `subjects_metric` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `technique_approach`
--

DROP TABLE IF EXISTS `technique_approach`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `technique_approach` (
  `id_technique` int(11) NOT NULL,
  `id_approach` int(11) NOT NULL,
  KEY `id_technique_idx` (`id_technique`),
  KEY `id_approach_idx` (`id_approach`),
  CONSTRAINT `approach_id_fk` FOREIGN KEY (`id_approach`) REFERENCES `approach` (`idapproach`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `technique_id_fk` FOREIGN KEY (`id_technique`) REFERENCES `proposed_technique` (`idtechnique`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `technique_approach`
--

LOCK TABLES `technique_approach` WRITE;
/*!40000 ALTER TABLE `technique_approach` DISABLE KEYS */;
INSERT INTO `technique_approach` VALUES (1,1),(19,1),(34,1),(43,1),(47,1),(51,1),(58,1),(59,1),(65,1),(2,2),(3,2),(5,2),(20,2),(23,2),(35,2),(55,2),(56,2),(2,3),(4,3),(5,3),(6,3),(9,3),(10,3),(11,3),(15,3),(17,3),(18,3),(19,3),(20,3),(22,3),(23,3),(24,3),(25,3),(27,3),(28,3),(30,3),(31,3),(35,3),(36,3),(37,3),(38,3),(40,3),(41,3),(42,3),(45,3),(48,3),(49,3),(52,3),(53,3),(60,3),(61,3),(62,3),(66,3),(4,4),(8,4),(13,4),(14,4),(16,4),(17,4),(25,4),(45,4),(46,4),(52,4),(53,4),(54,4),(58,4),(60,4),(62,4),(4,5),(11,5),(12,5),(21,5),(26,5),(32,5),(33,5),(45,5),(7,6),(18,6),(49,6),(63,6),(64,6),(5,7),(10,7),(15,7),(17,7),(29,7),(39,8),(40,9),(44,10),(50,10),(57,11),(67,11),(68,3),(68,10),(69,6),(70,12),(71,12),(72,12),(73,12),(74,10),(75,4),(76,7),(77,6),(78,6),(79,7),(80,12);
/*!40000 ALTER TABLE `technique_approach` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `technique_evaluation`
--

DROP TABLE IF EXISTS `technique_evaluation`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `technique_evaluation` (
  `technique_idtechnique` int(11) NOT NULL,
  `evaluation_idevaluation` int(11) NOT NULL,
  PRIMARY KEY (`technique_idtechnique`,`evaluation_idevaluation`),
  KEY `fk_technique_has_evaluation_evaluation1_idx` (`evaluation_idevaluation`),
  KEY `fk_technique_has_evaluation_technique1_idx` (`technique_idtechnique`),
  CONSTRAINT `fk_technique_has_evaluation_evaluation1` FOREIGN KEY (`evaluation_idevaluation`) REFERENCES `evaluation` (`idevaluation`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `fk_technique_has_evaluation_technique1` FOREIGN KEY (`technique_idtechnique`) REFERENCES `proposed_technique` (`idtechnique`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `technique_evaluation`
--

LOCK TABLES `technique_evaluation` WRITE;
/*!40000 ALTER TABLE `technique_evaluation` DISABLE KEYS */;
INSERT INTO `technique_evaluation` VALUES (1,1),(2,2),(2,3),(3,4),(4,5),(5,6),(6,8),(6,9),(7,10),(8,12),(9,13),(10,14),(11,15),(12,16),(13,17),(14,18),(15,19),(16,20),(17,21),(18,22),(19,23),(20,24),(21,25),(22,26),(23,27),(24,28),(25,29),(2,30),(26,31),(27,32),(28,32),(29,33),(30,34),(31,34),(32,36),(33,37),(34,46),(35,47),(36,51),(37,51),(38,53),(39,54),(40,55),(41,56),(42,58),(42,59),(43,62),(44,63),(35,65),(45,66),(46,67),(47,68),(48,69),(49,70),(50,72),(51,73),(52,75),(53,76),(54,77),(55,78),(56,79),(57,80),(58,84),(59,85),(60,86),(27,88),(28,88),(61,88),(62,89),(63,90),(64,91),(65,92),(66,93),(67,94),(50,95),(68,97),(69,98),(70,99),(71,99),(72,99),(73,99),(74,101),(75,102),(76,103),(77,104),(78,104),(79,105),(79,106),(70,107),(71,107),(72,107),(73,107),(70,108),(71,108),(73,108),(71,109),(70,110),(73,110),(80,111);
/*!40000 ALTER TABLE `technique_evaluation` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `technique_granularity`
--

DROP TABLE IF EXISTS `technique_granularity`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `technique_granularity` (
  `id_technique` int(11) NOT NULL,
  `id_granularity` int(11) NOT NULL,
  KEY `id_technique_fk1_idx` (`id_technique`),
  KEY `id_granularity_fk_idx` (`id_granularity`),
  CONSTRAINT `id_granularity_fk` FOREIGN KEY (`id_granularity`) REFERENCES `granularity` (`idgranularity`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `id_technique_fk1` FOREIGN KEY (`id_technique`) REFERENCES `proposed_technique` (`idtechnique`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `technique_granularity`
--

LOCK TABLES `technique_granularity` WRITE;
/*!40000 ALTER TABLE `technique_granularity` DISABLE KEYS */;
INSERT INTO `technique_granularity` VALUES (2,1),(3,1),(5,1),(6,1),(20,1),(27,1),(28,1),(34,1),(42,1),(44,1),(46,1),(53,1),(57,1),(61,1),(66,1),(6,2),(7,2),(15,2),(17,2),(19,2),(22,2),(38,2),(40,2),(41,2),(42,2),(45,2),(47,2),(48,2),(55,2),(56,2),(61,2),(64,2),(67,2),(8,3),(16,3),(22,3),(38,3),(44,3),(9,4),(20,4),(35,4),(43,4),(53,4),(13,5),(14,5),(18,6),(18,7),(22,7),(30,7),(31,7),(38,7),(45,7),(48,7),(49,7),(62,7),(64,7),(67,7),(34,8),(39,8),(66,8),(23,9),(68,5),(69,6),(75,12),(76,13),(77,6),(78,6),(79,13);
/*!40000 ALTER TABLE `technique_granularity` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `technique_input_method`
--

DROP TABLE IF EXISTS `technique_input_method`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `technique_input_method` (
  `id_technique` int(11) NOT NULL,
  `id_input_method` int(11) NOT NULL,
  KEY `id_technique_idx` (`id_technique`),
  KEY `id_input_method_idx` (`id_input_method`),
  CONSTRAINT `id_input_method` FOREIGN KEY (`id_input_method`) REFERENCES `input_method` (`idinput_method`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `id_technique` FOREIGN KEY (`id_technique`) REFERENCES `proposed_technique` (`idtechnique`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `technique_input_method`
--

LOCK TABLES `technique_input_method` WRITE;
/*!40000 ALTER TABLE `technique_input_method` DISABLE KEYS */;
INSERT INTO `technique_input_method` VALUES (4,1),(6,1),(7,1),(9,1),(10,1),(17,1),(19,1),(20,1),(22,1),(24,1),(25,1),(27,1),(28,1),(29,1),(34,1),(35,1),(36,1),(37,1),(39,1),(40,1),(42,1),(48,1),(55,1),(65,1),(67,1),(3,2),(6,2),(10,2),(18,2),(23,2),(24,2),(25,2),(5,3),(18,3),(19,3),(43,3),(57,3),(61,3),(5,4),(10,4),(15,4),(17,4),(26,4),(29,4),(32,4),(65,4),(6,5),(7,5),(11,5),(15,5),(16,5),(17,5),(13,6),(14,6),(21,6),(26,6),(33,6),(14,6),(14,7),(14,8),(41,8),(43,8),(44,8),(45,8),(48,8),(49,8),(50,8),(51,8),(57,8),(58,8),(63,8),(65,8),(66,8),(67,8),(20,9),(22,9),(23,9),(38,9),(40,9),(41,9),(42,9),(43,9),(45,9),(47,9),(52,9),(53,9),(60,9),(62,9),(66,9),(20,10),(44,11),(50,11),(8,12),(45,12),(46,12),(47,12),(52,12),(53,12),(54,12),(58,12),(60,12),(62,12),(21,4),(12,8),(20,14),(53,14),(46,13),(47,13),(49,13),(56,13),(64,13),(67,13),(2,1),(3,1),(2,2),(68,1),(68,8),(68,9),(69,8),(70,8),(70,15),(71,8),(71,15),(72,8),(72,15),(73,8),(73,15),(74,8),(75,12),(75,4),(75,8),(76,4),(76,8),(77,8),(77,11),(78,8),(78,11),(79,4),(79,8),(80,8),(80,15);
/*!40000 ALTER TABLE `technique_input_method` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `technique_type`
--

DROP TABLE IF EXISTS `technique_type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `technique_type` (
  `id_technique` int(11) NOT NULL,
  `id_type` int(11) NOT NULL,
  KEY `id_technique_fk_idx` (`id_technique`),
  KEY `id_type_fk_idx` (`id_type`),
  CONSTRAINT `id_technique_fk` FOREIGN KEY (`id_technique`) REFERENCES `proposed_technique` (`idtechnique`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `id_type_fk` FOREIGN KEY (`id_type`) REFERENCES `type` (`idtype`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `technique_type`
--

LOCK TABLES `technique_type` WRITE;
/*!40000 ALTER TABLE `technique_type` DISABLE KEYS */;
INSERT INTO `technique_type` VALUES (2,1),(3,1),(10,1),(11,1),(13,1),(14,1),(15,1),(16,1),(17,1),(18,1),(23,1),(24,1),(25,1),(34,1),(39,1),(41,1),(42,1),(46,1),(47,1),(48,1),(49,1),(51,1),(52,1),(56,1),(60,1),(62,1),(64,1),(65,1),(66,1),(67,1),(8,2),(9,2),(12,2),(15,2),(19,2),(20,2),(21,2),(22,2),(25,2),(26,2),(27,2),(28,2),(29,2),(32,2),(33,2),(35,2),(40,2),(43,2),(44,2),(45,2),(50,2),(53,2),(54,2),(55,2),(56,2),(57,2),(58,2),(61,2),(63,2),(68,1),(69,2),(70,2),(71,1),(72,1),(73,2),(74,2),(75,2),(76,2),(77,2),(78,1),(79,2),(80,1);
/*!40000 ALTER TABLE `technique_type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `type`
--

DROP TABLE IF EXISTS `type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `type` (
  `idtype` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`idtype`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `type`
--

LOCK TABLES `type` WRITE;
/*!40000 ALTER TABLE `type` DISABLE KEYS */;
INSERT INTO `type` VALUES (1,'dynamic'),(2,'static');
/*!40000 ALTER TABLE `type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Final view structure for view `publication_vehicle_ranking`
--

/*!50001 DROP VIEW IF EXISTS `publication_vehicle_ranking`*/;
/*!50001 SET @saved_cs_client          = @@character_set_client */;
/*!50001 SET @saved_cs_results         = @@character_set_results */;
/*!50001 SET @saved_col_connection     = @@collation_connection */;
/*!50001 SET character_set_client      = utf8 */;
/*!50001 SET character_set_results     = utf8 */;
/*!50001 SET collation_connection      = utf8_general_ci */;
/*!50001 CREATE ALGORITHM=UNDEFINED */
/*!50013 DEFINER=`root`@`localhost` SQL SECURITY DEFINER */
/*!50001 VIEW `publication_vehicle_ranking` AS select `publication_vehicle`.`name` AS `name`,count(`study`.`publication_vehicle_idpublication_vehicle`) AS `amount` from (`study` join `publication_vehicle`) where (`study`.`publication_vehicle_idpublication_vehicle` = `publication_vehicle`.`idpublication_vehicle`) group by `study`.`publication_vehicle_idpublication_vehicle` order by count(`study`.`publication_vehicle_idpublication_vehicle`) desc */;
/*!50001 SET character_set_client      = @saved_cs_client */;
/*!50001 SET character_set_results     = @saved_cs_results */;
/*!50001 SET collation_connection      = @saved_col_connection */;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2017-04-28 16:24:37
